"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[8581],{5610(i){i.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Introduction to ROS 2 for Humanoid Robotics","href":"/docs/intro","docId":"intro","unlisted":false},{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","items":[{"type":"link","label":"ROS 2 Fundamentals for Humanoids","href":"/docs/module1/ros2-fundamentals","docId":"module1/ros2-fundamentals","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: The Digital Twin (Gazebo & Unity)","items":[{"type":"link","label":"Gazebo Physics Simulation","href":"/docs/module2/gazebo-physics","docId":"module2/gazebo-physics","unlisted":false},{"type":"link","label":"Simulated Sensors","href":"/docs/module2/simulated-sensors","docId":"module2/simulated-sensors","unlisted":false},{"type":"link","label":"Unity Integration","href":"/docs/module2/unity-integration","docId":"module2/unity-integration","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","items":[{"type":"link","label":"Isaac Ecosystem Overview","href":"/docs/isaac-ecosystem-overview","docId":"isaac-ecosystem-overview","unlisted":false},{"type":"category","label":"Isaac Sim & Synthetic Data","items":[{"type":"link","label":"Isaac Sim & Synthetic Data","href":"/docs/isaac-sim/","docId":"isaac-sim/index","unlisted":false},{"type":"link","label":"Photorealistic Simulation with Isaac Sim","href":"/docs/isaac-sim/photorealistic-simulation","docId":"isaac-sim/photorealistic-simulation","unlisted":false},{"type":"link","label":"RTX Rendering Configuration","href":"/docs/isaac-sim/rtx-rendering-configuration","docId":"isaac-sim/rtx-rendering-configuration","unlisted":false},{"type":"link","label":"USD Scene Composition","href":"/docs/isaac-sim/usd-scene-composition","docId":"isaac-sim/usd-scene-composition","unlisted":false},{"type":"link","label":"USD Scene Composition Examples","href":"/docs/isaac-sim/usd-scene-composition-examples","docId":"isaac-sim/usd-scene-composition-examples","unlisted":false},{"type":"link","label":"Synthetic Data Generation","href":"/docs/isaac-sim/synthetic-data-generation","docId":"isaac-sim/synthetic-data-generation","unlisted":false},{"type":"link","label":"Synthetic Dataset Creation Workflows","href":"/docs/isaac-sim/synthetic-dataset-workflows","docId":"isaac-sim/synthetic-dataset-workflows","unlisted":false},{"type":"link","label":"Domain Randomization","href":"/docs/isaac-sim/domain-randomization","docId":"isaac-sim/domain-randomization","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Isaac ROS Perception","items":[{"type":"link","label":"Isaac ROS Perception","href":"/docs/isaac-ros/","docId":"isaac-ros/index","unlisted":false},{"type":"link","label":"Hardware-Accelerated Perception","href":"/docs/isaac-ros/hardware-accelerated-perception","docId":"isaac-ros/hardware-accelerated-perception","unlisted":false},{"type":"link","label":"GPU Optimization Techniques","href":"/docs/isaac-ros/gpu-optimization-techniques","docId":"isaac-ros/gpu-optimization-techniques","unlisted":false},{"type":"link","label":"VSLAM Pipelines","href":"/docs/isaac-ros/vslam-pipelines","docId":"isaac-ros/vslam-pipelines","unlisted":false},{"type":"link","label":"Perception Pipeline Configurations","href":"/docs/isaac-ros/perception-pipeline-configurations","docId":"isaac-ros/perception-pipeline-configurations","unlisted":false},{"type":"link","label":"ROS 2 Integration","href":"/docs/isaac-ros/ros2-integration","docId":"isaac-ros/ros2-integration","unlisted":false},{"type":"link","label":"Sensor Processing with GPU Acceleration","href":"/docs/isaac-ros/sensor-processing-gpu-acceleration","docId":"isaac-ros/sensor-processing-gpu-acceleration","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Nav2 for Humanoid Navigation","items":[{"type":"link","label":"Nav2 for Humanoid Navigation","href":"/docs/nav2-humanoid/","docId":"nav2-humanoid/index","unlisted":false},{"type":"link","label":"Path Planning for Bipedal Robots","href":"/docs/nav2-humanoid/path-planning","docId":"nav2-humanoid/path-planning","unlisted":false},{"type":"link","label":"Localization for Humanoid Robots","href":"/docs/nav2-humanoid/localization","docId":"nav2-humanoid/localization","unlisted":false},{"type":"link","label":"Bipedal Navigation Concepts","href":"/docs/nav2-humanoid/bipedal-navigation","docId":"nav2-humanoid/bipedal-navigation","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Integration Examples","items":[{"type":"link","label":"Isaac Sim to Isaac ROS Integration","href":"/docs/isaac-sim-to-ros-integration","docId":"isaac-sim-to-ros-integration","unlisted":false},{"type":"link","label":"Isaac ROS to Nav2 Integration","href":"/docs/isaac-ros-to-nav2-integration","docId":"isaac-ros-to-nav2-integration","unlisted":false},{"type":"link","label":"Isaac Sim to Nav2 Integration","href":"/docs/isaac-sim-to-nav2-integration","docId":"isaac-sim-to-nav2-integration","unlisted":false},{"type":"link","label":"Cross-Module References","href":"/docs/cross-module-references","docId":"cross-module-references","unlisted":false},{"type":"link","label":"Isaac Ecosystem Integration","href":"/docs/cross-references","docId":"cross-references","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"link","label":"Quickstart Guide","href":"/docs/quickstart","docId":"quickstart","unlisted":false},{"type":"link","label":"Documentation Standards and Validation","href":"/docs/documentation-standards","docId":"documentation-standards","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action (VLA) & Autonomous Humanoid","items":[{"type":"link","label":"Vision-Language-Action (VLA) Systems for Autonomous Humanoids","href":"/docs/vla-overview/","docId":"vla-overview/index","unlisted":false},{"type":"category","label":"Voice-to-Action Pipeline","items":[{"type":"link","label":"Voice Command Processing Overview","href":"/docs/voice-to-action/","docId":"voice-to-action/index","unlisted":false},{"type":"link","label":"Speech Recognition with OpenAI Whisper","href":"/docs/voice-to-action/speech-recognition-whisper","docId":"voice-to-action/speech-recognition-whisper","unlisted":false},{"type":"link","label":"Intent Parsing and Natural Language Understanding","href":"/docs/voice-to-action/intent-parsing","docId":"voice-to-action/intent-parsing","unlisted":false},{"type":"link","label":"Command Translation to ROS 2","href":"/docs/voice-to-action/command-translation","docId":"voice-to-action/command-translation","unlisted":false},{"type":"link","label":"Confidence Scoring and Validation in Voice Processing","href":"/docs/voice-to-action/confidence-scoring","docId":"voice-to-action/confidence-scoring","unlisted":false},{"type":"link","label":"Voice Command Data Model and Validation","href":"/docs/voice-to-action/voice-command-data-model","docId":"voice-to-action/voice-command-data-model","unlisted":false},{"type":"link","label":"Speech-to-Text Processing Workflows and Validation","href":"/docs/voice-to-action/stt-processing-workflows","docId":"voice-to-action/stt-processing-workflows","unlisted":false},{"type":"link","label":"Simulated Voice Command Examples with ROS 2 Outputs","href":"/docs/voice-to-action/simulated-voice-examples","docId":"voice-to-action/simulated-voice-examples","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Cognitive Planning","items":[{"type":"link","label":"Cognitive Planning Overview","href":"/docs/cognitive-planning/","docId":"cognitive-planning/index","unlisted":false},{"type":"link","label":"LLM Integration for Cognitive Planning","href":"/docs/cognitive-planning/llm-integration","docId":"cognitive-planning/llm-integration","unlisted":false},{"type":"link","label":"Task Decomposition in Cognitive Planning","href":"/docs/cognitive-planning/task-decomposition","docId":"cognitive-planning/task-decomposition","unlisted":false},{"type":"link","label":"Action Sequencing in Cognitive Planning","href":"/docs/cognitive-planning/action-sequencing","docId":"cognitive-planning/action-sequencing","unlisted":false},{"type":"link","label":"Context Awareness and Environmental Integration","href":"/docs/cognitive-planning/context-awareness","docId":"cognitive-planning/context-awareness","unlisted":false},{"type":"link","label":"Cognitive Planning Data Model","href":"/docs/cognitive-planning/data-model","docId":"cognitive-planning/data-model","unlisted":false},{"type":"link","label":"Planning Validation with LLMs and Action Feasibility Checks","href":"/docs/cognitive-planning/validation","docId":"cognitive-planning/validation","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Capstone System","items":[{"type":"link","label":"Overview","href":"/docs/capstone-system/","docId":"capstone-system/index","unlisted":false},{"type":"link","label":"Pipeline Integration","href":"/docs/capstone-system/pipeline-integration","docId":"capstone-system/pipeline-integration","unlisted":false},{"type":"link","label":"Simulation Setup","href":"/docs/capstone-system/simulation-setup","docId":"capstone-system/simulation-setup","unlisted":false},{"type":"link","label":"Complete Workflow","href":"/docs/capstone-system/complete-workflow","docId":"capstone-system/complete-workflow","unlisted":false}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true}]},"docs":{"capstone-system/complete-workflow":{"id":"capstone-system/complete-workflow","title":"Complete Example Workflow","description":"Complete example workflow demonstrating the full VLA pipeline from voice command to robotic action execution","sidebar":"tutorialSidebar"},"capstone-system/error-handling":{"id":"capstone-system/error-handling","title":"Error Handling and Fallback Strategies","description":"Documentation on error handling and fallback strategies for the complete VLA pipeline"},"capstone-system/index":{"id":"capstone-system/index","title":"End-to-End Pipeline Overview","description":"Complete pipeline from voice recognition through cognitive planning to navigation, perception, and manipulation in simulated humanoid environments","sidebar":"tutorialSidebar"},"capstone-system/pipeline-integration":{"id":"capstone-system/pipeline-integration","title":"Complete Pipeline Integration","description":"Documentation on how all VLA components work together in an integrated pipeline for humanoid robotics","sidebar":"tutorialSidebar"},"capstone-system/simulation-setup":{"id":"capstone-system/simulation-setup","title":"Simulation Environment Setup","description":"Guide to setting up simulation environment for end-to-end VLA pipeline testing in Isaac Sim","sidebar":"tutorialSidebar"},"cognitive-planning/action-sequencing":{"id":"cognitive-planning/action-sequencing","title":"Action Sequencing in Cognitive Planning","description":"Documentation on action sequencing techniques using LLMs for humanoid robot planning in VLA systems","sidebar":"tutorialSidebar"},"cognitive-planning/context-awareness":{"id":"cognitive-planning/context-awareness","title":"Context Awareness and Environmental Integration","description":"Documentation on context awareness and environmental integration for cognitive planning in VLA systems","sidebar":"tutorialSidebar"},"cognitive-planning/data-model":{"id":"cognitive-planning/data-model","title":"Cognitive Planning Data Model","description":"Documentation on the data model for cognitive planning in VLA systems with validation steps","sidebar":"tutorialSidebar"},"cognitive-planning/index":{"id":"cognitive-planning/index","title":"Cognitive Planning Overview","description":"Overview of cognitive planning using LLMs for translating natural language tasks into action sequences in VLA systems","sidebar":"tutorialSidebar"},"cognitive-planning/llm-integration":{"id":"cognitive-planning/llm-integration","title":"LLM Integration for Cognitive Planning","description":"Documentation on integrating Large Language Models for cognitive planning in VLA systems","sidebar":"tutorialSidebar"},"cognitive-planning/task-decomposition":{"id":"cognitive-planning/task-decomposition","title":"Task Decomposition in Cognitive Planning","description":"Documentation on task decomposition techniques using LLMs for humanoid robot planning in VLA systems","sidebar":"tutorialSidebar"},"cognitive-planning/validation":{"id":"cognitive-planning/validation","title":"Planning Validation with LLMs and Action Feasibility Checks","description":"Documentation on planning validation using LLMs and action feasibility checks in VLA systems","sidebar":"tutorialSidebar"},"cross-module-references":{"id":"cross-module-references","title":"Cross-Module References","description":"Cross-references and integration points between Isaac Sim, Isaac ROS, and Nav2 for humanoid robotics","sidebar":"tutorialSidebar"},"cross-references":{"id":"cross-references","title":"Isaac Ecosystem Integration","description":"Cross-references and integration points between Isaac Sim, Isaac ROS, and Nav2","sidebar":"tutorialSidebar"},"documentation-standards":{"id":"documentation-standards","title":"Documentation Standards and Validation","description":"Standards and validation procedures for Isaac ecosystem documentation following Docusaurus formatting guidelines","sidebar":"tutorialSidebar"},"features":{"id":"features","title":"Project Features: Digital Twin Simulation System","description":"This document provides an overview of the key features of the digital twin simulation system implemented in Module 2: The Digital Twin (Gazebo & Unity)."},"image-guidelines":{"id":"image-guidelines","title":"Image Guidelines","description":"Guidelines for using images in the ROS 2 for Humanoid Robotics documentation"},"intro":{"id":"intro","title":"Introduction to ROS 2 for Humanoid Robotics","description":"Welcome to the comprehensive guide on ROS 2 (Robot Operating System 2) for humanoid robotics. This educational book is designed for students and developers with basic AI/programming knowledge who are entering the field of Physical AI and humanoid robotics.","sidebar":"tutorialSidebar"},"isaac-ecosystem-overview":{"id":"isaac-ecosystem-overview","title":"Isaac Ecosystem Overview","description":"Comprehensive overview of the Isaac ecosystem integration for humanoid robotics, covering Isaac Sim, Isaac ROS, and Nav2","sidebar":"tutorialSidebar"},"isaac-ros-to-nav2-integration":{"id":"isaac-ros-to-nav2-integration","title":"Isaac ROS to Nav2 Integration","description":"Integration examples showing how Isaac ROS perception data feeds into Nav2 navigation decisions for humanoid robots","sidebar":"tutorialSidebar"},"isaac-ros/gpu-optimization-techniques":{"id":"isaac-ros/gpu-optimization-techniques","title":"GPU Optimization Techniques","description":"Performance optimization techniques for Isaac ROS using GPU acceleration for humanoid robotics applications","sidebar":"tutorialSidebar"},"isaac-ros/hardware-accelerated-perception":{"id":"isaac-ros/hardware-accelerated-perception","title":"Hardware-Accelerated Perception","description":"Implementing perception pipelines with hardware acceleration using Isaac ROS for humanoid robotics applications","sidebar":"tutorialSidebar"},"isaac-ros/index":{"id":"isaac-ros/index","title":"Isaac ROS Perception","description":"Guide to implementing hardware-accelerated perception pipelines using Isaac ROS, including VSLAM techniques and ROS 2 integration","sidebar":"tutorialSidebar"},"isaac-ros/perception-pipeline-configurations":{"id":"isaac-ros/perception-pipeline-configurations","title":"Perception Pipeline Configurations","description":"Configuration and validation steps for Isaac ROS perception pipelines in humanoid robotics applications","sidebar":"tutorialSidebar"},"isaac-ros/ros2-integration":{"id":"isaac-ros/ros2-integration","title":"ROS 2 Integration","description":"Integrating Isaac ROS perception pipelines with ROS 2 for humanoid robotics applications","sidebar":"tutorialSidebar"},"isaac-ros/sensor-processing-gpu-acceleration":{"id":"isaac-ros/sensor-processing-gpu-acceleration","title":"Sensor Processing with GPU Acceleration","description":"GPU-accelerated sensor processing in Isaac ROS with ROS 2 compatibility considerations for humanoid robotics","sidebar":"tutorialSidebar"},"isaac-ros/vslam-pipelines":{"id":"isaac-ros/vslam-pipelines","title":"VSLAM Pipelines","description":"Visual Simultaneous Localization and Mapping pipelines using Isaac ROS with GPU acceleration for humanoid robotics","sidebar":"tutorialSidebar"},"isaac-sim-to-nav2-integration":{"id":"isaac-sim-to-nav2-integration","title":"Isaac Sim to Nav2 Integration","description":"Integration examples showing how Isaac Sim environments are used for Nav2 navigation testing and validation","sidebar":"tutorialSidebar"},"isaac-sim-to-ros-integration":{"id":"isaac-sim-to-ros-integration","title":"Isaac Sim to Isaac ROS Integration","description":"Integration examples showing how Isaac Sim environments are used for Isaac ROS perception pipeline testing","sidebar":"tutorialSidebar"},"isaac-sim/domain-randomization":{"id":"isaac-sim/domain-randomization","title":"Domain Randomization","description":"Implementing domain randomization techniques in Isaac Sim to improve synthetic-to-real transfer for humanoid robotics","sidebar":"tutorialSidebar"},"isaac-sim/index":{"id":"isaac-sim/index","title":"Isaac Sim & Synthetic Data","description":"Comprehensive guide to NVIDIA Isaac Sim for photorealistic simulations and synthetic datasets for humanoid robots","sidebar":"tutorialSidebar"},"isaac-sim/photorealistic-simulation":{"id":"isaac-sim/photorealistic-simulation","title":"Photorealistic Simulation with Isaac Sim","description":"Creating photorealistic simulation environments using NVIDIA Isaac Sim and RTX rendering for humanoid robotics","sidebar":"tutorialSidebar"},"isaac-sim/rtx-rendering-configuration":{"id":"isaac-sim/rtx-rendering-configuration","title":"RTX Rendering Configuration","description":"Configuration examples for NVIDIA RTX rendering in Isaac Sim for photorealistic humanoid robotics simulation","sidebar":"tutorialSidebar"},"isaac-sim/synthetic-data-generation":{"id":"isaac-sim/synthetic-data-generation","title":"Synthetic Data Generation","description":"Creating synthetic datasets using Isaac Sim for training perception systems in humanoid robotics","sidebar":"tutorialSidebar"},"isaac-sim/synthetic-dataset-workflows":{"id":"isaac-sim/synthetic-dataset-workflows","title":"Synthetic Dataset Creation Workflows","description":"Complete workflows for creating synthetic datasets with domain randomization for humanoid robotics applications","sidebar":"tutorialSidebar"},"isaac-sim/usd-scene-composition":{"id":"isaac-sim/usd-scene-composition","title":"USD Scene Composition","description":"Creating and composing scenes using Universal Scene Description (USD) in Isaac Sim for humanoid robotics applications","sidebar":"tutorialSidebar"},"isaac-sim/usd-scene-composition-examples":{"id":"isaac-sim/usd-scene-composition-examples","title":"USD Scene Composition Examples","description":"Practical examples and validation steps for USD-based scene composition in Isaac Sim for humanoid robotics","sidebar":"tutorialSidebar"},"module1/ros2-fundamentals":{"id":"module1/ros2-fundamentals","title":"ROS 2 Fundamentals for Humanoids","description":"Learn core ROS 2 concepts for humanoid robots","sidebar":"tutorialSidebar"},"module2/api-reference":{"id":"module2/api-reference","title":"API Reference: Module 2 - The Digital Twin (Gazebo & Unity)","description":"This document provides a comprehensive reference for the REST APIs available in Module 2, which focuses on digital twin simulation using Gazebo for physics and Unity for visualization."},"module2/exercises":{"id":"module2/exercises","title":"Exercises and Challenges: Module 2 - Digital Twin Simulation","description":"This collection of exercises and challenges is designed to reinforce your understanding of digital twin simulation concepts using Gazebo for physics and Unity for visualization, integrated with ROS 2."},"module2/gazebo-physics":{"id":"module2/gazebo-physics","title":"Gazebo Physics Simulation","description":"Learn how to create physics-based simulations in Gazebo for humanoid robots with ROS 2 integration","sidebar":"tutorialSidebar"},"module2/index":{"id":"module2/index","title":"Module 2: The Digital Twin (Gazebo & Unity)","description":"Welcome to Module 2 of the Robotics Simulation Guide, where we explore the concept of digital twins using Gazebo for physics simulation and Unity for visualization. This module will teach you how to create realistic simulation environments that mirror real-world robotic systems."},"module2/performance-optimization":{"id":"module2/performance-optimization","title":"Performance Optimization Guide: Module 2 - Digital Twin Simulation","description":"This guide provides strategies and techniques for optimizing the performance of your Gazebo-Unity digital twin simulation environment."},"module2/quickstart":{"id":"module2/quickstart","title":"Module 2 Quickstart Guide: Digital Twin Simulation","description":"Get started quickly with Gazebo-Unity digital twin simulation for humanoid robots. This guide will help you set up, configure, and run your first simulation in under 30 minutes."},"module2/simulated-sensors":{"id":"module2/simulated-sensors","title":"Simulated Sensors","description":"Learn how to simulate various sensors in Gazebo and connect them to ROS 2 topics for humanoid robotics applications","sidebar":"tutorialSidebar"},"module2/troubleshooting":{"id":"module2/troubleshooting","title":"Troubleshooting Guide: Module 2 - Digital Twin Simulation","description":"This guide provides solutions to common issues encountered when setting up and running the Gazebo-Unity digital twin simulation environment."},"module2/unity-integration":{"id":"module2/unity-integration","title":"Unity Integration","description":"Learn how to integrate Unity with Gazebo simulation for visualization and human-robot interaction","sidebar":"tutorialSidebar"},"nav2-humanoid/balance-step-dynamics":{"id":"nav2-humanoid/balance-step-dynamics","title":"Balance and Step Dynamics for Navigation","description":"Balance and step dynamics considerations for humanoid robot navigation using Nav2, addressing unique challenges of bipedal locomotion"},"nav2-humanoid/behavior-tree-configurations":{"id":"nav2-humanoid/behavior-tree-configurations","title":"Behavior Tree Configurations for Humanoid Navigation","description":"Behavior tree configurations adapted for humanoid robot navigation using Nav2, including custom nodes for bipedal locomotion"},"nav2-humanoid/bipedal-navigation":{"id":"nav2-humanoid/bipedal-navigation","title":"Bipedal Navigation Concepts","description":"Navigation concepts specifically adapted for bipedal robots using Nav2, addressing unique challenges of humanoid locomotion","sidebar":"tutorialSidebar"},"nav2-humanoid/index":{"id":"nav2-humanoid/index","title":"Nav2 for Humanoid Navigation","description":"Guide to implementing navigation systems specifically for humanoid robots using Nav2, accounting for bipedal locomotion challenges","sidebar":"tutorialSidebar"},"nav2-humanoid/localization":{"id":"nav2-humanoid/localization","title":"Localization for Humanoid Robots","description":"Localization techniques adapted for bipedal robots using Nav2, considering sensor fusion and Z-axis movement","sidebar":"tutorialSidebar"},"nav2-humanoid/path-planning":{"id":"nav2-humanoid/path-planning","title":"Path Planning for Bipedal Robots","description":"Path planning algorithms adapted for humanoid robots using Nav2, considering balance and step dynamics","sidebar":"tutorialSidebar"},"nav2-humanoid/z-axis-movement-navigation":{"id":"nav2-humanoid/z-axis-movement-navigation","title":"Z-Axis Movement Navigation Examples","description":"Navigation examples and validation steps for humanoid robots handling Z-axis movement, stairs, and elevation changes using Nav2"},"python-agents":{"id":"python-agents","title":"Python Agents for ROS 2","description":"Learning Objectives"},"quickstart":{"id":"quickstart","title":"Quickstart Guide","description":"Quickstart guide for implementing NVIDIA Isaac ecosystem for humanoid robotics","sidebar":"tutorialSidebar"},"test-modern-ui":{"id":"test-modern-ui","title":"Test Modern UI Features","description":"Testing the new modern UI features"},"urdf-humanoids":{"id":"urdf-humanoids","title":"URDF for Humanoid Robots","description":"Learning Objectives"},"using-icons":{"id":"using-icons","title":"Using Icons in Documentation","description":"How to use Docusaurus icons in the ROS 2 for Humanoid Robotics documentation"},"vla-cross-references":{"id":"vla-cross-references","title":"VLA Cross-Module References","description":"Cross-references between Vision-Language-Action documentation modules"},"vla-documentation-standards":{"id":"vla-documentation-standards","title":"VLA Documentation Standards","description":"Standards and formatting guidelines for Vision-Language-Action documentation following Docusaurus best practices"},"vla-ecosystem-overview":{"id":"vla-ecosystem-overview","title":"VLA Ecosystem Integration Overview","description":"Overview of the Vision-Language-Action ecosystem and how components integrate for humanoid autonomy"},"vla-overview/index":{"id":"vla-overview/index","title":"Vision-Language-Action (VLA) Systems for Autonomous Humanoids","description":"Comprehensive guide to integrating language, perception, and action for humanoid robot autonomy","sidebar":"tutorialSidebar"},"voice-to-action/command-translation":{"id":"voice-to-action/command-translation","title":"Command Translation to ROS 2","description":"Documentation on translating parsed voice commands to ROS 2 actions and messages in VLA systems","sidebar":"tutorialSidebar"},"voice-to-action/confidence-scoring":{"id":"voice-to-action/confidence-scoring","title":"Confidence Scoring and Validation in Voice Processing","description":"Documentation on confidence scoring mechanisms and validation approaches for voice command processing in VLA systems","sidebar":"tutorialSidebar"},"voice-to-action/index":{"id":"voice-to-action/index","title":"Voice Command Processing Overview","description":"Overview of how voice commands are processed and converted to ROS 2 actions in the VLA system","sidebar":"tutorialSidebar"},"voice-to-action/intent-parsing":{"id":"voice-to-action/intent-parsing","title":"Intent Parsing and Natural Language Understanding","description":"Documentation on parsing natural language voice commands to extract actionable intents in VLA systems","sidebar":"tutorialSidebar"},"voice-to-action/simulated-voice-examples":{"id":"voice-to-action/simulated-voice-examples","title":"Simulated Voice Command Examples with ROS 2 Outputs","description":"Comprehensive examples of voice commands and their expected ROS 2 outputs in VLA systems","sidebar":"tutorialSidebar"},"voice-to-action/speech-recognition-whisper":{"id":"voice-to-action/speech-recognition-whisper","title":"Speech Recognition with OpenAI Whisper","description":"Detailed documentation on implementing speech recognition using OpenAI Whisper for VLA systems","sidebar":"tutorialSidebar"},"voice-to-action/stt-processing-workflows":{"id":"voice-to-action/stt-processing-workflows","title":"Speech-to-Text Processing Workflows and Validation","description":"Documentation on speech-to-text processing workflows and validation parameters in VLA systems","sidebar":"tutorialSidebar"},"voice-to-action/voice-command-data-model":{"id":"voice-to-action/voice-command-data-model","title":"Voice Command Data Model and Validation","description":"Documentation on the voice command data model and validation processes in VLA systems","sidebar":"tutorialSidebar"}}}')}}]);