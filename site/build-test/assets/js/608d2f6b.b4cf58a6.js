"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[2573],{1327(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var t=i(2540),a=i(3023);const r={title:"Perception Pipeline Configurations",description:"Configuration and validation steps for Isaac ROS perception pipelines in humanoid robotics applications",sidebar_position:6,tags:["perception","pipeline","configuration","validation","isaac-ros"]},s="Perception Pipeline Configurations",o={id:"isaac-ros/perception-pipeline-configurations",title:"Perception Pipeline Configurations",description:"Configuration and validation steps for Isaac ROS perception pipelines in humanoid robotics applications",source:"@site/docs/isaac-ros/perception-pipeline-configurations.md",sourceDirName:"isaac-ros",slug:"/isaac-ros/perception-pipeline-configurations",permalink:"/docs/isaac-ros/perception-pipeline-configurations",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/isaac-ros/perception-pipeline-configurations.md",tags:[{label:"perception",permalink:"/docs/tags/perception"},{label:"pipeline",permalink:"/docs/tags/pipeline"},{label:"configuration",permalink:"/docs/tags/configuration"},{label:"validation",permalink:"/docs/tags/validation"},{label:"isaac-ros",permalink:"/docs/tags/isaac-ros"}],version:"current",sidebarPosition:6,frontMatter:{title:"Perception Pipeline Configurations",description:"Configuration and validation steps for Isaac ROS perception pipelines in humanoid robotics applications",sidebar_position:6,tags:["perception","pipeline","configuration","validation","isaac-ros"]},sidebar:"tutorialSidebar",previous:{title:"VSLAM Pipelines",permalink:"/docs/isaac-ros/vslam-pipelines"},next:{title:"ROS 2 Integration",permalink:"/docs/isaac-ros/ros2-integration"}},l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Perception Pipeline Architecture",id:"perception-pipeline-architecture",level:2},{value:"Modular Pipeline Design",id:"modular-pipeline-design",level:3},{value:"Node Configuration Structure",id:"node-configuration-structure",level:3},{value:"Configuration Examples",id:"configuration-examples",level:2},{value:"VSLAM Pipeline Configuration",id:"vslam-pipeline-configuration",level:3},{value:"Object Detection Pipeline Configuration",id:"object-detection-pipeline-configuration",level:3},{value:"Depth Processing Pipeline Configuration",id:"depth-processing-pipeline-configuration",level:3},{value:"Launch File Configurations",id:"launch-file-configurations",level:2},{value:"Complete Perception Pipeline Launch",id:"complete-perception-pipeline-launch",level:3},{value:"Validation Procedures",id:"validation-procedures",level:2},{value:"Pipeline Validation Framework",id:"pipeline-validation-framework",level:3},{value:"Automated Configuration Validation",id:"automated-configuration-validation",level:3},{value:"Performance Validation",id:"performance-validation",level:2},{value:"Benchmarking Tools",id:"benchmarking-tools",level:3},{value:"Troubleshooting and Diagnostics",id:"troubleshooting-and-diagnostics",level:2},{value:"Common Configuration Issues",id:"common-configuration-issues",level:3},{value:"Diagnostic Tools Configuration",id:"diagnostic-tools-configuration",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Configuration Management",id:"configuration-management",level:3},{value:"Validation Strategies",id:"validation-strategies",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Integration Testing",id:"integration-testing",level:2},{value:"End-to-End Validation",id:"end-to-end-validation",level:3}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"perception-pipeline-configurations",children:"Perception Pipeline Configurations"}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Isaac ROS perception pipelines are configurable systems that process sensor data to extract meaningful information for humanoid robots. This document covers the configuration of perception pipelines, validation procedures, and best practices for ensuring reliable operation."}),"\n",(0,t.jsx)(n.h2,{id:"perception-pipeline-architecture",children:"Perception Pipeline Architecture"}),"\n",(0,t.jsx)(n.h3,{id:"modular-pipeline-design",children:"Modular Pipeline Design"}),"\n",(0,t.jsx)(n.p,{children:"Isaac ROS perception pipelines follow a modular design:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# Example modular perception pipeline configuration\nperception_pipeline:\n  modules:\n    # Image preprocessing module\n    image_preprocessing:\n      enabled: true\n      nodes:\n        - image_rectification\n        - image_normalization\n        - noise_reduction\n\n    # Feature extraction module\n    feature_extraction:\n      enabled: true\n      nodes:\n        - edge_detection\n        - corner_detection\n        - feature_matching\n\n    # Object detection module\n    object_detection:\n      enabled: true\n      nodes:\n        - neural_network_inference\n        - object_classification\n        - bounding_box_refinement\n\n    # Tracking module\n    tracking:\n      enabled: true\n      nodes:\n        - object_tracking\n        - trajectory_prediction\n        - motion_analysis\n\n    # Output processing module\n    output_processing:\n      enabled: true\n      nodes:\n        - result_fusion\n        - confidence_scoring\n        - message_formatting\n"})}),"\n",(0,t.jsx)(n.h3,{id:"node-configuration-structure",children:"Node Configuration Structure"}),"\n",(0,t.jsx)(n.p,{children:"Each node in the perception pipeline has a standard configuration structure:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# Standard node configuration template\nnode_name:\n  ros__parameters:\n    # Processing parameters\n    processing_frequency: 30.0  # Hz\n    queue_size: 10\n    timeout: 1.0  # seconds\n\n    # Hardware acceleration\n    use_gpu: true\n    gpu_id: 0\n\n    # Performance parameters\n    max_processing_time: 0.033  # 30 FPS = 33ms per frame\n    enable_performance_monitoring: true\n\n    # Input/output configuration\n    input_topic: "/camera/image_raw"\n    output_topic: "/perception/result"\n    input_qos:\n      reliability: 2  # reliable\n      durability: 2   # volatile\n      history: 1      # keep_last\n      depth: 1\n'})}),"\n",(0,t.jsx)(n.h2,{id:"configuration-examples",children:"Configuration Examples"}),"\n",(0,t.jsx)(n.h3,{id:"vslam-pipeline-configuration",children:"VSLAM Pipeline Configuration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# VSLAM pipeline configuration for humanoid robot\nvslam_pipeline:\n  ros__parameters:\n    # Camera parameters\n    image_width: 1280\n    image_height: 720\n    camera_matrix: [615.0, 0.0, 640.0, 0.0, 615.0, 360.0, 0.0, 0.0, 1.0]\n    distortion_coefficients: [0.0, 0.0, 0.0, 0.0, 0.0]\n\n    # Feature parameters\n    max_num_features: 2000\n    min_feature_distance: 20\n    quality_level: 0.01\n    harris_k: 0.04\n\n    # Tracking parameters\n    max_features: 500\n    tracking_quality_threshold: 20\n    pyramid_levels: 3\n    window_size: 21\n\n    # Mapping parameters\n    map_size: 100\n    enable_localization: true\n    enable_mapping: true\n    keyframe_threshold_translation: 0.5  # meters\n    keyframe_threshold_rotation: 0.1     # radians\n\n    # Optimization parameters\n    local_bundle_adjustment: true\n    global_bundle_adjustment: false\n    max_local_kfs: 10\n\n    # GPU acceleration\n    use_gpu: true\n    gpu_id: 0\n\n    # Humanoid-specific parameters\n    max_height_change: 0.3  # meters (for bipedal gait)\n    motion_model: "humanoid"\n    balance_aware: true\n\n    # Performance monitoring\n    enable_performance_monitoring: true\n    publish_diagnostics: true\n'})}),"\n",(0,t.jsx)(n.h3,{id:"object-detection-pipeline-configuration",children:"Object Detection Pipeline Configuration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# Object detection pipeline configuration\nobject_detection_pipeline:\n  ros__parameters:\n    # Neural network parameters\n    model_path: "/path/to/tensorrt/model.plan"\n    input_tensor_name: "input"\n    output_tensor_names: ["detection_boxes", "detection_classes", "detection_scores"]\n    max_batch_size: 1\n\n    # Detection parameters\n    confidence_threshold: 0.5\n    nms_threshold: 0.4\n    max_objects: 100\n\n    # Preprocessing parameters\n    input_width: 640\n    input_height: 480\n    normalization_mean: [0.485, 0.456, 0.406]\n    normalization_std: [0.229, 0.224, 0.225]\n\n    # Postprocessing parameters\n    anchor_sizes: [32, 64, 128, 256, 512]\n    aspect_ratios: [0.5, 1.0, 2.0]\n\n    # Performance parameters\n    processing_frequency: 15.0  # Lower frequency for complex detection\n    max_processing_time: 0.066  # 15 FPS = 66ms per frame\n\n    # GPU acceleration\n    use_gpu: true\n    gpu_id: 0\n    use_int8: false\n    use_fp16: true\n\n    # Humanoid-specific parameters\n    detection_classes_of_interest: ["person", "chair", "table", "door"]\n    min_detection_distance: 0.5  # meters\n    max_detection_distance: 10.0 # meters\n'})}),"\n",(0,t.jsx)(n.h3,{id:"depth-processing-pipeline-configuration",children:"Depth Processing Pipeline Configuration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# Depth processing pipeline configuration\ndepth_processing_pipeline:\n  ros__parameters:\n    # Input parameters\n    depth_image_topic: "/depth_camera/depth/image_rect_raw"\n    camera_info_topic: "/depth_camera/camera_info"\n\n    # Processing parameters\n    depth_unit_scaling_factor: 0.001  # Convert mm to meters\n    min_depth: 0.1  # meters\n    max_depth: 10.0 # meters\n\n    # Filtering parameters\n    enable_hole_filling: true\n    hole_filling_kernel_size: 3\n    enable_noise_filtering: true\n    noise_threshold: 0.1  # meters\n\n    # Point cloud generation\n    enable_point_cloud: true\n    point_cloud_decimation_factor: 2\n    point_cloud_output_topic: "/depth_processing/points"\n\n    # Obstacle detection\n    enable_obstacle_detection: true\n    obstacle_height_threshold: 0.3  # meters above ground\n    obstacle_distance_threshold: 2.0 # meters\n\n    # Performance parameters\n    processing_frequency: 30.0\n    max_processing_time: 0.033\n\n    # GPU acceleration\n    use_gpu: true\n    gpu_id: 0\n'})}),"\n",(0,t.jsx)(n.h2,{id:"launch-file-configurations",children:"Launch File Configurations"}),"\n",(0,t.jsx)(n.h3,{id:"complete-perception-pipeline-launch",children:"Complete Perception Pipeline Launch"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Complete perception pipeline launch file --\x3e\n<launch>\n  \x3c!-- Arguments --\x3e\n  <arg name="use_sim_time" default="false"/>\n  <arg name="camera_namespace" default="/camera"/>\n  <arg name="perception_namespace" default="/perception"/>\n\n  \x3c!-- Image preprocessing nodes --\x3e\n  <node pkg="isaac_ros_image_proc" exec="isaac_ros_image_rectification" name="image_rectification">\n    <param name="use_sim_time" value="$(var use_sim_time)"/>\n    <param name="input_width" value="1280"/>\n    <param name="input_height" value="720"/>\n    <param name="output_width" value="1280"/>\n    <param name="output_height" value="720"/>\n    <remap from="image_raw" to="$(var camera_namespace)/image_raw"/>\n    <remap from="camera_info" to="$(var camera_namespace)/camera_info"/>\n    <remap from="image_rect" to="$(var perception_namespace)/image_rect"/>\n  </node>\n\n  \x3c!-- Feature detection node --\x3e\n  <node pkg="isaac_ros_visual_slam" exec="isaac_ros_visual_slam_node" name="visual_slam">\n    <param name="use_sim_time" value="$(var use_sim_time)"/>\n    <param name="enable_slam" value="true"/>\n    <param name="enable_localization" value="true"/>\n    <param name="map_frame" value="map"/>\n    <param name="odom_frame" value="odom"/>\n    <param name="base_frame" value="base_link"/>\n    <remap from="visual_slam/image" to="$(var perception_namespace)/image_rect"/>\n    <remap from="visual_slam/camera_info" to="$(var camera_namespace)/camera_info"/>\n    <remap from="visual_slam/pose" to="$(var perception_namespace)/pose"/>\n  </node>\n\n  \x3c!-- Object detection node --\x3e\n  <node pkg="isaac_ros_detectnet" exec="isaac_ros_detectnet" name="object_detection">\n    <param name="use_sim_time" value="$(var use_sim_time)"/>\n    <param name="model_name" value="ssd_mobilenet_v2_coco"/>\n    <param name="input_tensor" value="input"/>\n    <param name="output_tensor_detections" value="output"/>\n    <param name="confidence_threshold" value="0.7"/>\n    <param name="enable_padding" value="true"/>\n    <remap from="image" to="$(var perception_namespace)/image_rect"/>\n    <remap from="detections" to="$(var perception_namespace)/detections"/>\n  </node>\n\n  \x3c!-- Perception results aggregator --\x3e\n  <node pkg="perception_aggregator" exec="aggregator_node" name="perception_aggregator">\n    <param name="use_sim_time" value="$(var use_sim_time)"/>\n    <remap from="slam_pose" to="$(var perception_namespace)/pose"/>\n    <remap from="object_detections" to="$(var perception_namespace)/detections"/>\n    <remap from="aggregated_perception" to="$(var perception_namespace)/results"/>\n  </node>\n</launch>\n'})}),"\n",(0,t.jsx)(n.h2,{id:"validation-procedures",children:"Validation Procedures"}),"\n",(0,t.jsx)(n.h3,{id:"pipeline-validation-framework",children:"Pipeline Validation Framework"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Perception pipeline validation framework\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import Float32\nimport numpy as np\nimport time\n\nclass PerceptionPipelineValidator(Node):\n    def __init__(self):\n        super().__init__(\'perception_pipeline_validator\')\n\n        # Validation parameters\n        self.declare_parameter(\'validation_duration\', 60.0)  # seconds\n        self.declare_parameter(\'min_fps\', 15.0)\n        self.declare_parameter(\'max_latency\', 0.1)  # seconds\n        self.declare_parameter(\'min_detection_rate\', 0.8)  # fraction\n\n        self.validation_duration = self.get_parameter(\'validation_duration\').value\n        self.min_fps = self.get_parameter(\'min_fps\').value\n        self.max_latency = self.get_parameter(\'max_latency\').value\n        self.min_detection_rate = self.get_parameter(\'min_detection_rate\').value\n\n        # Validation state\n        self.start_time = None\n        self.frame_count = 0\n        self.latency_samples = []\n        self.detection_count = 0\n        self.total_frames = 0\n\n        # Subscribe to pipeline outputs\n        self.perception_sub = self.create_subscription(\n            PoseStamped,\n            \'/perception/pose\',\n            self.pose_callback,\n            10\n        )\n\n        self.detection_sub = self.create_subscription(\n            Float32,\n            \'/perception/detection_rate\',\n            self.detection_rate_callback,\n            10\n        )\n\n        # Timer for validation\n        self.timer = self.create_timer(0.1, self.validation_timer_callback)\n\n    def start_validation(self):\n        """\n        Start the validation process\n        """\n        self.get_logger().info("Starting perception pipeline validation...")\n        self.start_time = time.time()\n        self.frame_count = 0\n        self.latency_samples = []\n        self.detection_count = 0\n        self.total_frames = 0\n\n    def pose_callback(self, msg):\n        """\n        Handle pose messages for latency validation\n        """\n        if self.start_time is None:\n            return\n\n        # Calculate latency\n        current_time = self.get_clock().now()\n        msg_time = rclpy.time.Time.from_msg(msg.header.stamp)\n        latency = (current_time.nanoseconds - msg_time.nanoseconds) / 1e9\n\n        self.latency_samples.append(latency)\n        self.frame_count += 1\n        self.total_frames += 1\n\n    def detection_rate_callback(self, msg):\n        """\n        Handle detection rate messages\n        """\n        if msg.data > 0:\n            self.detection_count += 1\n\n    def validation_timer_callback(self):\n        """\n        Timer callback for validation checks\n        """\n        if self.start_time is None:\n            return\n\n        elapsed_time = time.time() - self.start_time\n\n        if elapsed_time >= self.validation_duration:\n            self.complete_validation()\n            return\n\n        # Perform intermediate validation checks\n        self.check_intermediate_metrics()\n\n    def check_intermediate_metrics(self):\n        """\n        Check intermediate validation metrics\n        """\n        current_time = time.time()\n        elapsed_time = current_time - self.start_time\n\n        if elapsed_time > 0:\n            current_fps = self.frame_count / elapsed_time\n\n            if current_fps < self.min_fps * 0.8:  # 80% of minimum\n                self.get_logger().warn(f"Current FPS ({current_fps:.2f}) is low")\n\n            if self.latency_samples:\n                avg_latency = np.mean(self.latency_samples)\n                if avg_latency > self.max_latency * 0.8:  # 80% of maximum\n                    self.get_logger().warn(f"Current latency ({avg_latency:.3f}s) is high")\n\n    def complete_validation(self):\n        """\n        Complete validation and generate report\n        """\n        elapsed_time = time.time() - self.start_time\n        avg_fps = self.frame_count / elapsed_time if elapsed_time > 0 else 0\n        avg_latency = np.mean(self.latency_samples) if self.latency_samples else float(\'inf\')\n        detection_rate = self.detection_count / self.total_frames if self.total_frames > 0 else 0\n\n        # Generate validation report\n        report = {\n            "duration": elapsed_time,\n            "avg_fps": avg_fps,\n            "avg_latency": avg_latency,\n            "detection_rate": detection_rate,\n            "total_frames": self.frame_count,\n            "passed": True\n        }\n\n        # Check if validation passed\n        if avg_fps < self.min_fps:\n            report["passed"] = False\n            report["fps_failure"] = f"Expected {self.min_fps}, got {avg_fps:.2f}"\n\n        if avg_latency > self.max_latency:\n            report["passed"] = False\n            report["latency_failure"] = f"Expected < {self.max_latency}, got {avg_latency:.3f}"\n\n        if detection_rate < self.min_detection_rate:\n            report["passed"] = False\n            report["detection_failure"] = f"Expected > {self.min_detection_rate}, got {detection_rate:.2f}"\n\n        # Log validation results\n        self.log_validation_results(report)\n\n        # Shutdown if validation is complete\n        self.get_logger().info(f"Validation completed. Passed: {report[\'passed\']}")\n        rclpy.shutdown()\n\n    def log_validation_results(self, report):\n        """\n        Log validation results\n        """\n        self.get_logger().info("PERCEPTION PIPELINE VALIDATION RESULTS")\n        self.get_logger().info(f"Duration: {report[\'duration\']:.2f}s")\n        self.get_logger().info(f"Average FPS: {report[\'avg_fps\']:.2f}")\n        self.get_logger().info(f"Average Latency: {report[\'avg_latency\']:.3f}s")\n        self.get_logger().info(f"Detection Rate: {report[\'detection_rate\']:.2f}")\n        self.get_logger().info(f"Total Frames Processed: {report[\'total_frames\']}")\n        self.get_logger().info(f"Validation Passed: {report[\'passed\']}")\n\n        if not report[\'passed\']:\n            self.get_logger().error("VALIDATION FAILED")\n            for key, value in report.items():\n                if key.endswith(\'_failure\'):\n                    self.get_logger().error(f"  {value}")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"automated-configuration-validation",children:"Automated Configuration Validation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Automated configuration validation\nimport yaml\nimport json\nfrom jsonschema import validate, ValidationError\nimport os\n\nclass ConfigValidator:\n    def __init__(self):\n        self.schemas = self.load_schemas()\n\n    def load_schemas(self):\n        """\n        Load JSON schemas for different configuration types\n        """\n        schemas = {}\n\n        # Schema for VSLAM configuration\n        schemas[\'vslam\'] = {\n            "type": "object",\n            "properties": {\n                "ros__parameters": {\n                    "type": "object",\n                    "properties": {\n                        "image_width": {"type": "number"},\n                        "image_height": {"type": "number"},\n                        "max_num_features": {"type": "number"},\n                        "use_gpu": {"type": "boolean"},\n                        "gpu_id": {"type": "number"}\n                    },\n                    "required": ["image_width", "image_height", "max_num_features"]\n                }\n            },\n            "required": ["ros__parameters"]\n        }\n\n        # Schema for object detection configuration\n        schemas[\'object_detection\'] = {\n            "type": "object",\n            "properties": {\n                "ros__parameters": {\n                    "type": "object",\n                    "properties": {\n                        "model_path": {"type": "string"},\n                        "confidence_threshold": {"type": "number"},\n                        "use_gpu": {"type": "boolean"},\n                        "gpu_id": {"type": "number"}\n                    },\n                    "required": ["model_path", "confidence_threshold"]\n                }\n            },\n            "required": ["ros__parameters"]\n        }\n\n        return schemas\n\n    def validate_config(self, config_path, config_type):\n        """\n        Validate configuration file against schema\n        """\n        try:\n            with open(config_path, \'r\') as f:\n                config = yaml.safe_load(f)\n\n            schema = self.schemas.get(config_type)\n            if not schema:\n                raise ValueError(f"Unknown config type: {config_type}")\n\n            validate(instance=config, schema=schema)\n            return True, "Configuration is valid"\n\n        except ValidationError as e:\n            return False, f"Configuration validation error: {e.message}"\n        except Exception as e:\n            return False, f"Error validating configuration: {str(e)}"\n\n    def validate_all_configs(self, config_dir):\n        """\n        Validate all configuration files in a directory\n        """\n        results = {}\n\n        for filename in os.listdir(config_dir):\n            if filename.endswith(\'.yaml\') or filename.endswith(\'.yml\'):\n                config_path = os.path.join(config_dir, filename)\n\n                # Determine config type from filename\n                if \'vslam\' in filename or \'visual_slam\' in filename:\n                    config_type = \'vslam\'\n                elif \'detection\' in filename or \'detectnet\' in filename:\n                    config_type = \'object_detection\'\n                else:\n                    continue  # Skip unknown config types\n\n                is_valid, message = self.validate_config(config_path, config_type)\n                results[filename] = {"valid": is_valid, "message": message}\n\n        return results\n\n# Example usage\nvalidator = ConfigValidator()\nresults = validator.validate_all_configs("/path/to/configs")\nfor filename, result in results.items():\n    print(f"{filename}: {result[\'message\']}")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"performance-validation",children:"Performance Validation"}),"\n",(0,t.jsx)(n.h3,{id:"benchmarking-tools",children:"Benchmarking Tools"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Perception pipeline benchmarking\nimport time\nimport statistics\nimport threading\nfrom collections import deque\n\nclass PerceptionBenchmark:\n    def __init__(self, node):\n        self.node = node\n        self.metrics = {\n            'processing_times': deque(maxlen=1000),\n            'frame_rates': deque(maxlen=100),\n            'memory_usage': deque(maxlen=1000),\n            'cpu_usage': deque(maxlen=1000)\n        }\n        self.start_times = {}\n        self.is_benchmarking = False\n\n    def start_benchmark(self):\n        \"\"\"\n        Start benchmarking\n        \"\"\"\n        self.is_benchmarking = True\n        self.benchmark_thread = threading.Thread(target=self._benchmark_loop)\n        self.benchmark_thread.start()\n\n    def stop_benchmark(self):\n        \"\"\"\n        Stop benchmarking\n        \"\"\"\n        self.is_benchmarking = False\n        if hasattr(self, 'benchmark_thread'):\n            self.benchmark_thread.join()\n\n    def measure_processing_time(self, operation_name, func, *args, **kwargs):\n        \"\"\"\n        Measure processing time for a specific operation\n        \"\"\"\n        start_time = time.perf_counter()\n        result = func(*args, **kwargs)\n        end_time = time.perf_counter()\n\n        processing_time = end_time - start_time\n        self.metrics['processing_times'].append(processing_time)\n\n        self.node.get_logger().debug(f\"{operation_name} took {processing_time:.4f}s\")\n\n        return result\n\n    def _benchmark_loop(self):\n        \"\"\"\n        Background benchmarking loop\n        \"\"\"\n        import psutil\n\n        while self.is_benchmarking:\n            # Measure system metrics\n            cpu_percent = psutil.cpu_percent()\n            memory_percent = psutil.virtual_memory().percent\n\n            self.metrics['cpu_usage'].append(cpu_percent)\n            self.metrics['memory_usage'].append(memory_percent)\n\n            time.sleep(0.1)  # Sample every 100ms\n\n    def get_benchmark_report(self):\n        \"\"\"\n        Generate benchmark report\n        \"\"\"\n        report = {\n            'processing_time_stats': {\n                'mean': statistics.mean(self.metrics['processing_times']) if self.metrics['processing_times'] else 0,\n                'median': statistics.median(self.metrics['processing_times']) if self.metrics['processing_times'] else 0,\n                'std_dev': statistics.stdev(self.metrics['processing_times']) if len(self.metrics['processing_times']) > 1 else 0,\n                'min': min(self.metrics['processing_times']) if self.metrics['processing_times'] else 0,\n                'max': max(self.metrics['processing_times']) if self.metrics['processing_times'] else 0,\n                'count': len(self.metrics['processing_times'])\n            },\n            'system_stats': {\n                'avg_cpu': statistics.mean(self.metrics['cpu_usage']) if self.metrics['cpu_usage'] else 0,\n                'avg_memory': statistics.mean(self.metrics['memory_usage']) if self.metrics['memory_usage'] else 0,\n                'peak_memory': max(self.metrics['memory_usage']) if self.metrics['memory_usage'] else 0\n            }\n        }\n\n        return report\n\n    def print_benchmark_report(self):\n        \"\"\"\n        Print benchmark report\n        \"\"\"\n        report = self.get_benchmark_report()\n\n        print(\"PERCEPTION PIPELINE BENCHMARK REPORT\")\n        print(\"=\" * 50)\n        print(f\"Processing Time (ms):\")\n        print(f\"  Mean: {report['processing_time_stats']['mean'] * 1000:.2f}\")\n        print(f\"  Median: {report['processing_time_stats']['median'] * 1000:.2f}\")\n        print(f\"  Std Dev: {report['processing_time_stats']['std_dev'] * 1000:.2f}\")\n        print(f\"  Min: {report['processing_time_stats']['min'] * 1000:.2f}\")\n        print(f\"  Max: {report['processing_time_stats']['max'] * 1000:.2f}\")\n        print(f\"  Count: {report['processing_time_stats']['count']}\")\n        print(f\"\\nSystem Usage:\")\n        print(f\"  Avg CPU: {report['system_stats']['avg_cpu']:.1f}%\")\n        print(f\"  Avg Memory: {report['system_stats']['avg_memory']:.1f}%\")\n        print(f\"  Peak Memory: {report['system_stats']['peak_memory']:.1f}%\")\n"})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-and-diagnostics",children:"Troubleshooting and Diagnostics"}),"\n",(0,t.jsx)(n.h3,{id:"common-configuration-issues",children:"Common Configuration Issues"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# Troubleshooting checklist configuration\ntroubleshooting:\n  common_issues:\n    - issue: "High latency in perception pipeline"\n      causes:\n        - "GPU not properly utilized"\n        - "CPU bottleneck"\n        - "Memory allocation issues"\n      solutions:\n        - "Verify GPU acceleration is enabled"\n        - "Check for CPU-intensive operations"\n        - "Optimize memory allocation patterns"\n\n    - issue: "Low detection accuracy"\n      causes:\n        - "Incorrect model configuration"\n        - "Poor lighting conditions"\n        - "Inadequate training data"\n      solutions:\n        - "Verify model path and parameters"\n        - "Improve lighting or use preprocessing"\n        - "Retrain model with better data"\n\n    - issue: "Pipeline crashes or hangs"\n      causes:\n        - "Memory leaks"\n        - "Resource conflicts"\n        - "Invalid configuration parameters"\n      solutions:\n        - "Check memory usage patterns"\n        - "Verify resource allocation"\n        - "Validate all configuration parameters"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"diagnostic-tools-configuration",children:"Diagnostic Tools Configuration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# Diagnostic tools configuration\ndiagnostics:\n  enabled: true\n  publishers:\n    - name: "performance_monitor"\n      type: "PerformanceMonitor"\n      parameters:\n        update_rate: 1.0  # Hz\n        topics_to_monitor: ["/camera/image_raw", "/perception/pose", "/perception/detections"]\n\n    - name: "resource_monitor"\n      type: "ResourceMonitor"\n      parameters:\n        update_rate: 5.0  # Hz\n        resources_to_monitor: ["cpu", "memory", "gpu"]\n\n    - name: "health_monitor"\n      type: "HealthMonitor"\n      parameters:\n        update_rate: 0.2  # Hz (every 5 seconds)\n        nodes_to_monitor: ["visual_slam", "object_detection", "image_rectification"]\n'})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsx)(n.h3,{id:"configuration-management",children:"Configuration Management"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Modular Configuration"}),": Separate configurations by function (VSLAM, detection, tracking)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Parameter Validation"}),": Validate all parameters before launching nodes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Environment Adaptation"}),": Adjust parameters based on deployment environment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Version Control"}),": Track configuration changes with version control"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"validation-strategies",children:"Validation Strategies"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Automated Testing"}),": Implement automated validation for all configurations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance Baselines"}),": Establish performance baselines for comparison"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Incremental Testing"}),": Test pipeline components individually before integration"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Continuous Monitoring"}),": Monitor performance during operation"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Profiling First"}),": Profile before optimizing to identify bottlenecks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPU Utilization"}),": Maximize GPU utilization for compute-intensive tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory Management"}),": Optimize memory allocation and transfers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pipeline Parallelism"}),": Use pipeline parallelism to improve throughput"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"integration-testing",children:"Integration Testing"}),"\n",(0,t.jsx)(n.h3,{id:"end-to-end-validation",children:"End-to-End Validation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# End-to-end perception pipeline test\nimport unittest\nimport rclpy\nfrom rclpy.action import ActionClient\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PoseStamped\nimport time\n\nclass TestPerceptionPipeline(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        rclpy.init()\n\n    @classmethod\n    def tearDownClass(cls):\n        rclpy.shutdown()\n\n    def setUp(self):\n        self.node = rclpy.create_node(\'test_perception_pipeline\')\n\n        # Publishers for test data\n        self.image_pub = self.node.create_publisher(Image, \'/test_camera/image_raw\', 10)\n\n        # Subscribers for validation\n        self.pose_sub = self.node.create_subscription(\n            PoseStamped, \'/perception/pose\', self.pose_callback, 10\n        )\n\n        self.received_poses = []\n\n    def pose_callback(self, msg):\n        self.received_poses.append(msg)\n\n    def test_pipeline_latency(self):\n        """\n        Test that pipeline maintains acceptable latency\n        """\n        # Send test image\n        test_image = self.create_test_image()\n        start_time = time.time()\n        self.image_pub.publish(test_image)\n\n        # Wait for response with timeout\n        timeout = 5.0  # seconds\n        start_wait = time.time()\n\n        while len(self.received_poses) == 0 and (time.time() - start_wait) < timeout:\n            rclpy.spin_once(self.node, timeout_sec=0.1)\n\n        end_time = time.time()\n        latency = end_time - start_time\n\n        # Assert acceptable latency (adjust based on requirements)\n        self.assertLess(latency, 0.1, f"Pipeline latency too high: {latency}s")\n\n    def create_test_image(self):\n        """\n        Create a test image for validation\n        """\n        from cv_bridge import CvBridge\n        import cv2\n        import numpy as np\n\n        # Create a simple test image\n        img = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n\n        bridge = CvBridge()\n        msg = bridge.cv2_to_imgmsg(img, encoding="bgr8")\n        msg.header.stamp = self.node.get_clock().now().to_msg()\n\n        return msg\n\nif __name__ == \'__main__\':\n    unittest.main()\n'})}),"\n",(0,t.jsx)(n.p,{children:"This comprehensive guide provides configuration examples and validation procedures for Isaac ROS perception pipelines in humanoid robotics applications, ensuring reliable and performant operation."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},3023(e,n,i){i.d(n,{R:()=>s,x:()=>o});var t=i(3696);const a={},r=t.createContext(a);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);