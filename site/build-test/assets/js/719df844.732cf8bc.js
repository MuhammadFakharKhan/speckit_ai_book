"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[1082],{3023(e,n,i){i.d(n,{R:()=>a,x:()=>r});var o=i(3696);const t={},s=o.createContext(t);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(s.Provider,{value:n},e.children)}},8972(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>r,toc:()=>l});var o=i(2540),t=i(3023);const s={title:"Vision-Language-Action (VLA) Systems for Autonomous Humanoids",description:"Comprehensive guide to integrating language, perception, and action for humanoid robot autonomy",sidebar_position:1,tags:["vla","autonomous-humanoid","robotics","vision-language-action"]},a="Vision-Language-Action (VLA) Systems for Autonomous Humanoids",r={id:"vla-overview/index",title:"Vision-Language-Action (VLA) Systems for Autonomous Humanoids",description:"Comprehensive guide to integrating language, perception, and action for humanoid robot autonomy",source:"@site/docs/vla-overview/index.md",sourceDirName:"vla-overview",slug:"/vla-overview/",permalink:"/docs/vla-overview/",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla-overview/index.md",tags:[{label:"vla",permalink:"/docs/tags/vla"},{label:"autonomous-humanoid",permalink:"/docs/tags/autonomous-humanoid"},{label:"robotics",permalink:"/docs/tags/robotics"},{label:"vision-language-action",permalink:"/docs/tags/vision-language-action"}],version:"current",sidebarPosition:1,frontMatter:{title:"Vision-Language-Action (VLA) Systems for Autonomous Humanoids",description:"Comprehensive guide to integrating language, perception, and action for humanoid robot autonomy",sidebar_position:1,tags:["vla","autonomous-humanoid","robotics","vision-language-action"]},sidebar:"tutorialSidebar",previous:{title:"Documentation Standards and Validation",permalink:"/docs/documentation-standards"},next:{title:"Voice Command Processing Overview",permalink:"/docs/voice-to-action/"}},c={},l=[{value:"Overview",id:"overview",level:2},{value:"What is VLA?",id:"what-is-vla",level:3},{value:"Architecture Overview",id:"architecture-overview",level:3},{value:"Key Components",id:"key-components",level:3},{value:"Getting Started",id:"getting-started",level:2},{value:"Target Audience",id:"target-audience",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"vision-language-action-vla-systems-for-autonomous-humanoids",children:"Vision-Language-Action (VLA) Systems for Autonomous Humanoids"}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Welcome to the comprehensive guide on Vision-Language-Action (VLA) systems for autonomous humanoid robots. This documentation covers the integration of language understanding, visual perception, and physical action to create intelligent, responsive humanoid systems."}),"\n",(0,o.jsx)(n.h3,{id:"what-is-vla",children:"What is VLA?"}),"\n",(0,o.jsx)(n.p,{children:"Vision-Language-Action (VLA) refers to the integration of three critical components for robot autonomy:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision"}),": Perceiving and understanding the visual environment"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Language"}),": Understanding and processing natural language commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action"}),": Executing physical actions in response to language commands and environmental perception"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This system enables humanoid robots to understand and respond to human commands in natural language while perceiving and interacting with their environment."}),"\n",(0,o.jsx)(n.h3,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,o.jsx)(n.p,{children:"The VLA system follows a pipeline architecture:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Voice Command \u2192 Speech Recognition \u2192 Intent Parsing \u2192 Cognitive Planning \u2192 Action Execution \u2192 Physical Action\n"})}),"\n",(0,o.jsx)(n.p,{children:"Each component in this pipeline is designed to work seamlessly with the others, enabling complex behaviors that respond to natural human commands."}),"\n",(0,o.jsx)(n.h3,{id:"key-components",children:"Key Components"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice-to-Action Pipeline"}),": Converts spoken language into executable robot actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cognitive Planning"}),": Translates high-level goals into specific action sequences"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"End-to-End Pipeline"}),": Complete integration of all components for autonomous behavior"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,o.jsx)(n.p,{children:"This documentation is organized into three main sections:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"/docs/voice-to-action/",children:"Voice-to-Action"})," - Understanding how voice commands are processed and converted to ROS 2 actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"/docs/cognitive-planning/",children:"Cognitive Planning"})," - How natural language tasks are translated into action sequences"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"/docs/capstone-system/",children:"Capstone System"})," - Complete end-to-end pipeline integration"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Each section builds upon the previous, allowing you to understand the complete VLA system from individual components to full integration."}),"\n",(0,o.jsx)(n.h2,{id:"target-audience",children:"Target Audience"}),"\n",(0,o.jsx)(n.p,{children:"This documentation is designed for:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Advanced robotics developers"}),"\n",(0,o.jsx)(n.li,{children:"AI researchers working on human-robot interaction"}),"\n",(0,o.jsx)(n.li,{children:"Engineers implementing autonomous humanoid systems"}),"\n",(0,o.jsx)(n.li,{children:"Students studying robotics and AI integration"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(n.p,{children:"Before diving into this documentation, you should have:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Basic understanding of robotics concepts"}),"\n",(0,o.jsx)(n.li,{children:"Familiarity with ROS 2 (Robot Operating System)"}),"\n",(0,o.jsx)(n.li,{children:"Understanding of natural language processing concepts"}),"\n",(0,o.jsx)(n.li,{children:"Access to a simulated humanoid robot environment"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(n.p,{children:["Begin with the ",(0,o.jsx)(n.a,{href:"/docs/voice-to-action/",children:"Voice-to-Action"})," section to understand the foundational component of the VLA system, or explore the ",(0,o.jsx)(n.a,{href:"/docs/capstone-system/",children:"Capstone System"})," for a complete end-to-end overview."]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);