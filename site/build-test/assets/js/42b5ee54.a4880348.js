"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[4339],{2037(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>d});var a=t(2540),i=t(3023);const s={title:"Synthetic Dataset Creation Workflows",description:"Complete workflows for creating synthetic datasets with domain randomization for humanoid robotics applications",sidebar_position:8,tags:["synthetic-data","dataset","workflow","domain-randomization","training"]},r="Synthetic Dataset Creation Workflows",o={id:"isaac-sim/synthetic-dataset-workflows",title:"Synthetic Dataset Creation Workflows",description:"Complete workflows for creating synthetic datasets with domain randomization for humanoid robotics applications",source:"@site/docs/isaac-sim/synthetic-dataset-workflows.md",sourceDirName:"isaac-sim",slug:"/isaac-sim/synthetic-dataset-workflows",permalink:"/docs/isaac-sim/synthetic-dataset-workflows",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/isaac-sim/synthetic-dataset-workflows.md",tags:[{label:"synthetic-data",permalink:"/docs/tags/synthetic-data"},{label:"dataset",permalink:"/docs/tags/dataset"},{label:"workflow",permalink:"/docs/tags/workflow"},{label:"domain-randomization",permalink:"/docs/tags/domain-randomization"},{label:"training",permalink:"/docs/tags/training"}],version:"current",sidebarPosition:8,frontMatter:{title:"Synthetic Dataset Creation Workflows",description:"Complete workflows for creating synthetic datasets with domain randomization for humanoid robotics applications",sidebar_position:8,tags:["synthetic-data","dataset","workflow","domain-randomization","training"]},sidebar:"tutorialSidebar",previous:{title:"Synthetic Data Generation",permalink:"/docs/isaac-sim/synthetic-data-generation"},next:{title:"Domain Randomization",permalink:"/docs/isaac-sim/domain-randomization"}},l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Complete Dataset Creation Pipeline",id:"complete-dataset-creation-pipeline",level:2},{value:"Basic Pipeline Structure",id:"basic-pipeline-structure",level:3},{value:"Domain Randomization Implementation",id:"domain-randomization-implementation",level:2},{value:"Domain Randomizer Class",id:"domain-randomizer-class",level:3},{value:"Scene Management",id:"scene-management",level:2},{value:"Scene Manager Class",id:"scene-manager-class",level:3},{value:"Dataset Validation and Quality Assurance",id:"dataset-validation-and-quality-assurance",level:2},{value:"Validation Pipeline",id:"validation-pipeline",level:3},{value:"Configuration Files",id:"configuration-files",level:2},{value:"Example Configuration File",id:"example-configuration-file",level:3},{value:"Best Practices and Guidelines",id:"best-practices-and-guidelines",level:2},{value:"Dataset Creation Best Practices",id:"dataset-creation-best-practices",level:3},{value:"Performance Considerations",id:"performance-considerations",level:3}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"synthetic-dataset-creation-workflows",children:"Synthetic Dataset Creation Workflows"}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"This document outlines complete workflows for creating synthetic datasets using Isaac Sim, with a focus on domain randomization techniques that improve the synthetic-to-real transfer for humanoid robotics applications. These workflows are designed to generate high-quality datasets suitable for training perception and navigation systems."}),"\n",(0,a.jsx)(n.h2,{id:"complete-dataset-creation-pipeline",children:"Complete Dataset Creation Pipeline"}),"\n",(0,a.jsx)(n.h3,{id:"basic-pipeline-structure",children:"Basic Pipeline Structure"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Complete synthetic dataset creation pipeline\nimport omni\nfrom omni.isaac.kit import SimulationApp\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nimport numpy as np\nimport os\nfrom datetime import datetime\nimport json\n\nclass SyntheticDatasetPipeline:\n    def __init__(self, config_path=None):\n        # Initialize simulation app\n        self.app = SimulationApp({"headless": False})\n\n        # Initialize synthetic data helper\n        self.sd_helper = SyntheticDataHelper()\n\n        # Load configuration\n        self.config = self.load_config(config_path) if config_path else self.default_config()\n\n        # Setup output directory\n        self.output_dir = self.setup_output_directory()\n\n        # Initialize domain randomization\n        self.domain_randomizer = DomainRandomizer(self.config)\n\n        # Initialize scene manager\n        self.scene_manager = SceneManager(self.config)\n\n    def load_config(self, config_path):\n        """\n        Load configuration from file\n        """\n        with open(config_path, \'r\') as f:\n            return json.load(f)\n\n    def default_config(self):\n        """\n        Default configuration for synthetic dataset creation\n        """\n        return {\n            "dataset": {\n                "name": "humanoid_robot_dataset",\n                "description": "Synthetic dataset for humanoid robotics",\n                "version": "1.0.0"\n            },\n            "rendering": {\n                "resolution": [1920, 1080],\n                "format": "png",\n                "pixel_samples": 16,\n                "max_surface_bounces": 4\n            },\n            "domain_randomization": {\n                "enabled": True,\n                "material_randomization": True,\n                "lighting_randomization": True,\n                "camera_randomization": False,\n                "object_placement_randomization": True\n            },\n            "data_types": {\n                "rgb": True,\n                "depth": True,\n                "segmentation": True,\n                "normals": False,\n                "flow": False\n            },\n            "collection_parameters": {\n                "num_frames": 10000,\n                "frame_interval": 1,\n                "scene_changes_per_reset": 5,\n                "reset_interval": 100\n            }\n        }\n\n    def setup_output_directory(self):\n        """\n        Setup output directory with timestamp\n        """\n        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n        output_dir = f"datasets/{self.config[\'dataset\'][\'name\']}_{timestamp}"\n\n        os.makedirs(output_dir, exist_ok=True)\n        os.makedirs(f"{output_dir}/rgb", exist_ok=True)\n        os.makedirs(f"{output_dir}/depth", exist_ok=True)\n        os.makedirs(f"{output_dir}/segmentation", exist_ok=True)\n        os.makedirs(f"{output_dir}/metadata", exist_ok=True)\n\n        return output_dir\n\n    def run_dataset_creation(self):\n        """\n        Main pipeline for dataset creation\n        """\n        print(f"Starting dataset creation: {self.config[\'dataset\'][\'name\']}")\n\n        # Initialize simulation\n        self.initialize_simulation()\n\n        # Main collection loop\n        frame_count = 0\n        scene_change_count = 0\n\n        for i in range(self.config[\'collection_parameters\'][\'num_frames\']):\n            # Randomize scene if needed\n            if i % self.config[\'collection_parameters\'][\'reset_interval\'] == 0:\n                self.randomize_scene()\n                scene_change_count += 1\n\n            # Capture frame\n            self.capture_frame(frame_count)\n            frame_count += 1\n\n            # Step simulation\n            self.app.update()\n\n            # Progress reporting\n            if i % 100 == 0:\n                print(f"Captured {i}/{self.config[\'collection_parameters\'][\'num_frames\']} frames")\n\n        # Finalize dataset\n        self.finalize_dataset()\n\n        print(f"Dataset creation completed. Output saved to: {self.output_dir}")\n\n        return self.output_dir\n\n    def initialize_simulation(self):\n        """\n        Initialize the simulation environment\n        """\n        # Setup stage\n        stage = omni.usd.get_context().get_stage()\n\n        # Configure rendering settings\n        self.configure_rendering()\n\n        # Load initial scene\n        self.scene_manager.load_initial_scene()\n\n        # Setup synthetic data capture\n        self.setup_synthetic_data_capture()\n\n    def configure_rendering(self):\n        """\n        Configure rendering settings for synthetic data\n        """\n        # Set resolution\n        resolution = self.config[\'rendering\'][\'resolution\']\n        self.sd_helper.set_resolution(resolution)\n\n        # Configure rendering quality\n        self.sd_helper.set_pixel_samples(self.config[\'rendering\'][\'pixel_samples\'])\n        self.sd_helper.set_max_surface_bounces(self.config[\'rendering\'][\'max_surface_bounces\'])\n\n    def setup_synthetic_data_capture(self):\n        """\n        Setup synthetic data capture based on configuration\n        """\n        # Enable data types as specified in config\n        if self.config[\'data_types\'][\'rgb\']:\n            self.sd_helper.enable_rgb_output(True)\n\n        if self.config[\'data_types\'][\'depth\']:\n            self.sd_helper.enable_depth_output(True)\n\n        if self.config[\'data_types\'][\'segmentation\']:\n            self.sd_helper.enable_segmentation_output(True)\n\n        if self.config[\'data_types\'][\'normals\']:\n            self.sd_helper.enable_normals_output(True)\n\n        if self.config[\'data_types\'][\'flow\']:\n            self.sd_helper.enable_flow_output(True)\n\n    def randomize_scene(self):\n        """\n        Randomize scene based on domain randomization configuration\n        """\n        if self.config[\'domain_randomization\'][\'enabled\']:\n            self.domain_randomizer.randomize_current_scene()\n\n    def capture_frame(self, frame_number):\n        """\n        Capture a single frame and save to dataset\n        """\n        # Capture RGB\n        if self.config[\'data_types\'][\'rgb\']:\n            rgb_data = self.sd_helper.get_rgb_data()\n            rgb_path = f"{self.output_dir}/rgb/frame_{frame_number:06d}.png"\n            self.sd_helper.save_rgb_image(rgb_data, rgb_path)\n\n        # Capture depth\n        if self.config[\'data_types\'][\'depth\']:\n            depth_data = self.sd_helper.get_depth_data()\n            depth_path = f"{self.output_dir}/depth/frame_{frame_number:06d}.png"\n            self.sd_helper.save_depth_image(depth_data, depth_path)\n\n        # Capture segmentation\n        if self.config[\'data_types\'][\'segmentation\']:\n            seg_data = self.sd_helper.get_segmentation_data()\n            seg_path = f"{self.output_dir}/segmentation/frame_{frame_number:06d}.png"\n            self.sd_helper.save_segmentation_image(seg_data, seg_path)\n\n        # Save metadata\n        metadata = {\n            "frame_number": frame_number,\n            "timestamp": datetime.now().isoformat(),\n            "domain_randomization_params": self.domain_randomizer.get_current_params(),\n            "camera_pose": self.get_camera_pose(),\n            "lighting_conditions": self.get_lighting_conditions()\n        }\n\n        metadata_path = f"{self.output_dir}/metadata/frame_{frame_number:06d}.json"\n        with open(metadata_path, \'w\') as f:\n            json.dump(metadata, f, indent=2)\n\n    def get_camera_pose(self):\n        """\n        Get current camera pose for metadata\n        """\n        # Implementation depends on camera setup\n        return {"position": [0, 0, 0], "rotation": [0, 0, 0, 1]}\n\n    def get_lighting_conditions(self):\n        """\n        Get current lighting conditions for metadata\n        """\n        # Implementation depends on lighting setup\n        return {"intensity": 1.0, "color": [1.0, 1.0, 1.0]}\n\n    def finalize_dataset(self):\n        """\n        Finalize dataset creation and save summary\n        """\n        # Save dataset configuration\n        config_path = f"{self.output_dir}/config.json"\n        with open(config_path, \'w\') as f:\n            json.dump(self.config, f, indent=2)\n\n        # Save dataset summary\n        summary = {\n            "dataset_name": self.config[\'dataset\'][\'name\'],\n            "total_frames": self.config[\'collection_parameters\'][\'num_frames\'],\n            "data_types": self.config[\'data_types\'],\n            "domain_randomization": self.config[\'domain_randomization\'],\n            "creation_date": datetime.now().isoformat(),\n            "output_directory": self.output_dir\n        }\n\n        summary_path = f"{self.output_dir}/summary.json"\n        with open(summary_path, \'w\') as f:\n            json.dump(summary, f, indent=2)\n\n# Example usage\nif __name__ == "__main__":\n    pipeline = SyntheticDatasetPipeline()\n    output_dir = pipeline.run_dataset_creation()\n    print(f"Dataset created at: {output_dir}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"domain-randomization-implementation",children:"Domain Randomization Implementation"}),"\n",(0,a.jsx)(n.h3,{id:"domain-randomizer-class",children:"Domain Randomizer Class"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Domain randomization implementation\nimport random\nimport numpy as np\nfrom pxr import Usd, UsdGeom, UsdShade, Gf, Sdf\n\nclass DomainRandomizer:\n    def __init__(self, config):\n        self.config = config\n        self.stage = None\n        self.current_params = {}\n\n    def set_stage(self, stage):\n        """\n        Set the USD stage for randomization\n        """\n        self.stage = stage\n\n    def randomize_current_scene(self):\n        """\n        Apply domain randomization to current scene\n        """\n        if self.config[\'domain_randomization\'][\'material_randomization\']:\n            self.randomize_materials()\n\n        if self.config[\'domain_randomization\'][\'lighting_randomization\']:\n            self.randomize_lighting()\n\n        if self.config[\'domain_randomization\'][\'object_placement_randomization\']:\n            self.randomize_object_placement()\n\n        if self.config[\'domain_randomization\'][\'camera_randomization\']:\n            self.randomize_camera_parameters()\n\n    def randomize_materials(self):\n        """\n        Randomize materials in the scene\n        """\n        # Find all materials in the scene\n        materials = self.find_materials_in_scene()\n\n        for material_path in materials:\n            material_prim = self.stage.GetPrimAtPath(material_path)\n\n            # Randomize diffuse color\n            if random.random() < 0.8:  # 80% chance to randomize\n                diffuse_color = self.randomize_color()\n                self.set_material_property(material_prim, "diffuse_color", diffuse_color)\n\n            # Randomize roughness\n            if random.random() < 0.6:  # 60% chance to randomize\n                roughness = random.uniform(0.1, 0.9)\n                self.set_material_property(material_prim, "roughness", roughness)\n\n            # Randomize metallic\n            if random.random() < 0.3:  # 30% chance to randomize\n                metallic = random.uniform(0.0, 1.0)\n                self.set_material_property(material_prim, "metallic", metallic)\n\n            # Store parameters for metadata\n            self.current_params[f"material_{material_path}"] = {\n                "diffuse_color": diffuse_color if \'diffuse_color\' in locals() else None,\n                "roughness": roughness if \'roughness\' in locals() else None,\n                "metallic": metallic if \'metallic\' in locals() else None\n            }\n\n    def randomize_lighting(self):\n        """\n        Randomize lighting conditions in the scene\n        """\n        # Find all lights in the scene\n        lights = self.find_lights_in_scene()\n\n        for light_path in lights:\n            light_prim = self.stage.GetPrimAtPath(light_path)\n\n            # Randomize light intensity\n            intensity_factor = random.uniform(0.5, 2.0)  # 0.5x to 2x original\n            original_intensity = self.get_light_property(light_prim, "intensity")\n            new_intensity = original_intensity * intensity_factor\n            self.set_light_property(light_prim, "intensity", new_intensity)\n\n            # Randomize light color\n            if random.random() < 0.7:  # 70% chance to randomize color\n                color_temperature = random.uniform(3000, 8000)  # Kelvin\n                color = self.color_temperature_to_rgb(color_temperature)\n                self.set_light_property(light_prim, "color", color)\n\n            # Store parameters for metadata\n            self.current_params[f"light_{light_path}"] = {\n                "intensity_factor": intensity_factor,\n                "color_temperature": color_temperature if \'color_temperature\' in locals() else None\n            }\n\n    def randomize_object_placement(self):\n        """\n        Randomize placement of objects in the scene\n        """\n        # Find objects that can be randomly placed\n        movable_objects = self.find_movable_objects()\n\n        for obj_path in movable_objects:\n            obj_prim = self.stage.GetPrimAtPath(obj_path)\n\n            # Get current position\n            current_pos = self.get_object_position(obj_prim)\n\n            # Apply random offset\n            random_offset = [\n                random.uniform(-0.5, 0.5),  # X offset\n                random.uniform(-0.5, 0.5),  # Y offset\n                random.uniform(-0.2, 0.2)   # Z offset\n            ]\n\n            new_pos = [\n                current_pos[0] + random_offset[0],\n                current_pos[1] + random_offset[1],\n                current_pos[2] + random_offset[2]\n            ]\n\n            # Set new position\n            self.set_object_position(obj_prim, new_pos)\n\n            # Store parameters for metadata\n            self.current_params[f"object_{obj_path}"] = {\n                "position_offset": random_offset\n            }\n\n    def find_materials_in_scene(self):\n        """\n        Find all material paths in the current scene\n        """\n        materials = []\n        for prim in self.stage.Traverse():\n            if prim.GetTypeName() == "Material":\n                materials.append(prim.GetPath())\n        return materials\n\n    def find_lights_in_scene(self):\n        """\n        Find all light paths in the current scene\n        """\n        lights = []\n        for prim in self.stage.Traverse():\n            if prim.GetTypeName() in ["DistantLight", "DomeLight", "RectLight", "SphereLight"]:\n                lights.append(prim.GetPath())\n        return lights\n\n    def find_movable_objects(self):\n        """\n        Find objects that can be randomly moved\n        """\n        movable_objects = []\n        for prim in self.stage.Traverse():\n            # Consider objects that are not part of the robot or fixed environment\n            prim_name = prim.GetName().lower()\n            if (prim.IsA(UsdGeom.Xform) and\n                "robot" not in prim_name and\n                "ground" not in prim_name and\n                "floor" not in prim_name):\n                movable_objects.append(prim.GetPath())\n        return movable_objects\n\n    def randomize_color(self):\n        """\n        Generate a random color\n        """\n        return [random.random(), random.random(), random.random()]\n\n    def color_temperature_to_rgb(self, color_temperature):\n        """\n        Convert color temperature in Kelvin to RGB\n        """\n        # Simplified algorithm for color temperature to RGB\n        temp = color_temperature / 100\n\n        if temp <= 66:\n            red = 255\n        else:\n            red = temp - 60\n            red = 329.698727446 * (red ** -0.1332047592)\n            red = max(0, min(255, red))\n\n        if temp <= 66:\n            green = temp\n            green = 99.4708025861 * np.log(green) - 161.1195681661\n        else:\n            green = temp - 60\n            green = 288.1221695283 * (green ** -0.0755148492)\n\n        green = max(0, min(255, green))\n\n        if temp >= 66:\n            blue = 255\n        elif temp <= 19:\n            blue = 0\n        else:\n            blue = temp - 10\n            blue = 138.5177312231 * np.log(blue) - 305.0447927307\n            blue = max(0, min(255, blue))\n\n        return [red/255, green/255, blue/255]\n\n    def set_material_property(self, material_prim, property_name, value):\n        """\n        Set a material property\n        """\n        # Find the shader inside the material\n        for child in material_prim.GetChildren():\n            if child.GetTypeName() == "Shader":\n                shader = child\n                # Set the property on the shader\n                input_path = f"inputs:{property_name}"\n                shader.CreateInput(input_path, Sdf.ValueTypeNames.Color3f if property_name == "diffuse_color" else Sdf.ValueTypeNames.Float)\n                shader.GetInput(input_path).Set(value)\n                break\n\n    def set_light_property(self, light_prim, property_name, value):\n        """\n        Set a light property\n        """\n        attr_name = f"inputs:{property_name}" if property_name in ["color"] else property_name\n        attr = light_prim.GetAttribute(attr_name)\n        if attr:\n            attr.Set(value)\n\n    def get_light_property(self, light_prim, property_name):\n        """\n        Get a light property\n        """\n        attr_name = f"inputs:{property_name}" if property_name in ["color"] else property_name\n        attr = light_prim.GetAttribute(attr_name)\n        if attr:\n            return attr.Get()\n        return 1.0  # Default value\n\n    def get_object_position(self, obj_prim):\n        """\n        Get object position\n        """\n        xform = UsdGeom.Xformable(obj_prim)\n        transform_ops = xform.GetOrderedXformOps()\n\n        for op in transform_ops:\n            if op.GetOpType() == UsdGeom.XformOp.TypeTranslate:\n                return op.Get()\n\n        return [0, 0, 0]  # Default position\n\n    def set_object_position(self, obj_prim, position):\n        """\n        Set object position\n        """\n        xform = UsdGeom.Xformable(obj_prim)\n        translate_op = xform.AddTranslateOp()\n        translate_op.Set(Gf.Vec3d(position))\n\n    def get_current_params(self):\n        """\n        Get current domain randomization parameters\n        """\n        return self.current_params.copy()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"scene-management",children:"Scene Management"}),"\n",(0,a.jsx)(n.h3,{id:"scene-manager-class",children:"Scene Manager Class"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Scene management for synthetic dataset creation\nimport random\nimport os\nfrom pxr import Usd, UsdGeom, Sdf\n\nclass SceneManager:\n    def __init__(self, config):\n        self.config = config\n        self.stage = None\n        self.scene_templates = self.load_scene_templates()\n\n    def load_scene_templates(self):\n        """\n        Load available scene templates\n        """\n        # In a real implementation, this would load from files\n        return [\n            "indoor_office.usd",\n            "outdoor_park.usd",\n            "laboratory.usd",\n            "warehouse.usd",\n            "apartment.usd"\n        ]\n\n    def set_stage(self, stage):\n        """\n        Set the USD stage for scene management\n        """\n        self.stage = stage\n\n    def load_initial_scene(self):\n        """\n        Load the initial scene for dataset creation\n        """\n        # For this example, we\'ll create a simple scene programmatically\n        # In practice, you\'d load from a USD file\n        self.create_basic_scene()\n\n    def create_basic_scene(self):\n        """\n        Create a basic scene for humanoid robot dataset\n        """\n        # Create world\n        world_prim = UsdGeom.Xform.Define(self.stage, "/World")\n\n        # Create ground plane\n        ground_prim = UsdGeom.Plane.Define(self.stage, "/World/Ground")\n        ground_prim.CreateSizeAttr(20.0)\n\n        # Create simple humanoid robot (simplified representation)\n        robot_prim = UsdGeom.Xform.Define(self.stage, "/World/Robot")\n\n        # Robot body parts\n        torso_prim = UsdGeom.Capsule.Define(self.stage, "/World/Robot/Torso")\n        torso_prim.CreateRadiusAttr(0.15)\n        torso_prim.CreateHeightAttr(0.6)\n\n        head_prim = UsdGeom.Sphere.Define(self.stage, "/World/Robot/Head")\n        head_prim.CreateRadiusAttr(0.12)\n        # Position head above torso\n        UsdGeom.XformCommonAPI(head_prim.GetPrim()).SetTranslate((0, 0.5, 0))\n\n        # Create simple environment objects\n        self.create_environment_objects()\n\n    def create_environment_objects(self):\n        """\n        Create environment objects for the scene\n        """\n        # Create some obstacles/objects\n        for i in range(5):\n            obj_name = f"Obstacle_{i}"\n            obj_prim = UsdGeom.Cube.Define(self.stage, f"/World/{obj_name}")\n            obj_prim.CreateSizeAttr(0.5)\n\n            # Position randomly in the scene\n            x_pos = random.uniform(-5, 5)\n            y_pos = random.uniform(-5, 5)\n            z_pos = 0.25  # Half the cube size\n\n            UsdGeom.XformCommonAPI(obj_prim.GetPrim()).SetTranslate((x_pos, y_pos, z_pos))\n\n    def change_scene(self):\n        """\n        Change to a different scene configuration\n        """\n        # This would typically load a different scene template\n        # For this example, we\'ll just randomize the current scene\n        self.randomize_current_scene_objects()\n\n    def randomize_current_scene_objects(self):\n        """\n        Randomize objects in the current scene\n        """\n        # Move existing objects to new positions\n        for prim in self.stage.Traverse():\n            if prim.GetName().startswith("Obstacle_"):\n                # Randomize position\n                x_pos = random.uniform(-8, 8)\n                y_pos = random.uniform(-8, 8)\n                z_pos = 0.25  # Keep on ground\n\n                UsdGeom.XformCommonAPI(prim).SetTranslate((x_pos, y_pos, z_pos))\n\n                # Randomize size slightly\n                size_attr = prim.GetAttribute("size")\n                if size_attr:\n                    original_size = size_attr.Get()\n                    new_size = original_size * random.uniform(0.8, 1.2)\n                    size_attr.Set(new_size)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"dataset-validation-and-quality-assurance",children:"Dataset Validation and Quality Assurance"}),"\n",(0,a.jsx)(n.h3,{id:"validation-pipeline",children:"Validation Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Dataset validation and quality assurance\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport json\nimport os\n\nclass DatasetValidator:\n    def __init__(self, dataset_path):\n        self.dataset_path = dataset_path\n        self.stats = {}\n\n    def validate_dataset(self):\n        """\n        Validate the entire dataset\n        """\n        print("Starting dataset validation...")\n\n        # Validate structure\n        structure_valid = self.validate_dataset_structure()\n\n        # Validate individual samples\n        sample_stats = self.validate_samples()\n\n        # Generate quality metrics\n        quality_metrics = self.calculate_quality_metrics()\n\n        # Generate validation report\n        report = {\n            "structure_valid": structure_valid,\n            "sample_count": sample_stats["total_samples"],\n            "missing_samples": sample_stats["missing_samples"],\n            "quality_metrics": quality_metrics,\n            "validation_passed": structure_valid and sample_stats["missing_samples"] == 0\n        }\n\n        # Save validation report\n        self.save_validation_report(report)\n\n        return report\n\n    def validate_dataset_structure(self):\n        """\n        Validate the dataset directory structure\n        """\n        required_dirs = ["rgb", "depth", "segmentation", "metadata"]\n\n        for dir_name in required_dirs:\n            dir_path = os.path.join(self.dataset_path, dir_name)\n            if not os.path.exists(dir_path):\n                print(f"Missing required directory: {dir_path}")\n                return False\n\n        return True\n\n    def validate_samples(self):\n        """\n        Validate individual samples in the dataset\n        """\n        # Get list of all frame numbers\n        rgb_files = [f for f in os.listdir(os.path.join(self.dataset_path, "rgb")) if f.endswith(".png")]\n        frame_numbers = [int(f.split("_")[1].split(".")[0]) for f in rgb_files]\n\n        total_samples = len(frame_numbers)\n        missing_samples = 0\n\n        for frame_num in frame_numbers:\n            # Check if all required files exist for this frame\n            rgb_path = os.path.join(self.dataset_path, "rgb", f"frame_{frame_num:06d}.png")\n            depth_path = os.path.join(self.dataset_path, "depth", f"frame_{frame_num:06d}.png")\n            seg_path = os.path.join(self.dataset_path, "segmentation", f"frame_{frame_num:06d}.png")\n            meta_path = os.path.join(self.dataset_path, "metadata", f"frame_{frame_num:06d}.json")\n\n            for path in [rgb_path, depth_path, seg_path, meta_path]:\n                if not os.path.exists(path):\n                    print(f"Missing file for frame {frame_num}: {path}")\n                    missing_samples += 1\n                    break\n\n        return {\n            "total_samples": total_samples,\n            "missing_samples": missing_samples\n        }\n\n    def calculate_quality_metrics(self):\n        """\n        Calculate quality metrics for the dataset\n        """\n        # Calculate metrics on a sample of frames\n        sample_size = min(100, len(os.listdir(os.path.join(self.dataset_path, "rgb"))))\n\n        rgb_quality = []\n        depth_quality = []\n        seg_quality = []\n\n        # Sample frames for quality assessment\n        frame_files = os.listdir(os.path.join(self.dataset_path, "rgb"))\n        sample_frames = random.sample(frame_files, sample_size)\n\n        for frame_file in sample_frames:\n            frame_num = int(frame_file.split("_")[1].split(".")[0])\n\n            # Load RGB image and assess quality\n            rgb_path = os.path.join(self.dataset_path, "rgb", f"frame_{frame_num:06d}.png")\n            rgb_img = cv2.imread(rgb_path)\n            if rgb_img is not None:\n                rgb_quality.append(self.assess_image_quality(rgb_img))\n\n            # Load depth image and assess quality\n            depth_path = os.path.join(self.dataset_path, "depth", f"frame_{frame_num:06d}.png")\n            depth_img = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n            if depth_img is not None:\n                depth_quality.append(self.assess_depth_quality(depth_img))\n\n            # Load segmentation image and assess quality\n            seg_path = os.path.join(self.dataset_path, "segmentation", f"frame_{frame_num:06d}.png")\n            seg_img = cv2.imread(seg_path, cv2.IMREAD_UNCHANGED)\n            if seg_img is not None:\n                seg_quality.append(self.assess_segmentation_quality(seg_img))\n\n        return {\n            "rgb_quality_avg": np.mean(rgb_quality) if rgb_quality else 0,\n            "depth_quality_avg": np.mean(depth_quality) if depth_quality else 0,\n            "seg_quality_avg": np.mean(seg_quality) if seg_quality else 0,\n            "sample_size": sample_size\n        }\n\n    def assess_image_quality(self, img):\n        """\n        Assess RGB image quality (simplified)\n        """\n        # Calculate basic quality metrics\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n        # Focus measure using Laplacian variance\n        focus_measure = cv2.Laplacian(gray, cv2.CV_64F).var()\n\n        # Brightness measure\n        brightness = np.mean(gray)\n\n        # Contrast measure\n        contrast = np.std(gray)\n\n        # Combine metrics (weights can be adjusted)\n        quality_score = (focus_measure * 0.4 + brightness * 0.3 + contrast * 0.3) / 1000\n\n        return min(quality_score, 1.0)  # Normalize to 0-1 range\n\n    def assess_depth_quality(self, depth_img):\n        """\n        Assess depth image quality\n        """\n        # Check for valid depth values (not all zeros or invalid)\n        valid_pixels = np.count_nonzero(depth_img > 0)\n        total_pixels = depth_img.size\n\n        valid_ratio = valid_pixels / total_pixels if total_pixels > 0 else 0\n\n        # Additional depth quality checks can be added here\n        return valid_ratio\n\n    def assess_segmentation_quality(self, seg_img):\n        """\n        Assess segmentation image quality\n        """\n        # Check for proper segmentation (multiple unique values)\n        unique_values = len(np.unique(seg_img))\n\n        # Should have more than just background (0) and possibly unlabeled (255)\n        seg_quality = min(unique_values / 10.0, 1.0)  # Normalize\n\n        return seg_quality\n\n    def save_validation_report(self, report):\n        """\n        Save validation report to file\n        """\n        report_path = os.path.join(self.dataset_path, "validation_report.json")\n        with open(report_path, \'w\') as f:\n            json.dump(report, f, indent=2)\n\n        print(f"Validation report saved to: {report_path}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"configuration-files",children:"Configuration Files"}),"\n",(0,a.jsx)(n.h3,{id:"example-configuration-file",children:"Example Configuration File"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "dataset": {\n    "name": "humanoid_robot_perception_dataset",\n    "description": "Synthetic dataset for training perception models for humanoid robots",\n    "version": "1.0.0",\n    "author": "Isaac Sim User",\n    "license": "MIT"\n  },\n  "rendering": {\n    "resolution": [1280, 720],\n    "format": "png",\n    "pixel_samples": 16,\n    "max_surface_bounces": 4,\n    "enable_denoiser": true\n  },\n  "domain_randomization": {\n    "enabled": true,\n    "material_randomization": {\n      "enabled": true,\n      "probability": 0.8,\n      "diffuse_color_range": [[0.1, 0.1, 0.1], [1.0, 1.0, 1.0]],\n      "roughness_range": [0.1, 0.9],\n      "metallic_range": [0.0, 1.0]\n    },\n    "lighting_randomization": {\n      "enabled": true,\n      "probability": 0.7,\n      "intensity_range": [0.5, 2.0],\n      "color_temperature_range": [3000, 8000]\n    },\n    "object_placement_randomization": {\n      "enabled": true,\n      "probability": 0.9,\n      "position_jitter": [0.5, 0.5, 0.2],\n      "rotation_jitter": [5, 5, 5]\n    },\n    "camera_randomization": {\n      "enabled": false,\n      "position_jitter": [0.1, 0.1, 0.05],\n      "rotation_jitter": [1, 1, 1]\n    }\n  },\n  "data_types": {\n    "rgb": true,\n    "depth": true,\n    "segmentation": true,\n    "normals": false,\n    "flow": false\n  },\n  "collection_parameters": {\n    "num_frames": 5000,\n    "frame_interval": 1,\n    "scene_changes_per_reset": 5,\n    "reset_interval": 100,\n    "capture_frequency": 30\n  },\n  "validation": {\n    "enabled": true,\n    "quality_thresholds": {\n      "rgb_quality": 0.5,\n      "depth_valid_ratio": 0.8,\n      "seg_diversity": 0.3\n    }\n  }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices-and-guidelines",children:"Best Practices and Guidelines"}),"\n",(0,a.jsx)(n.h3,{id:"dataset-creation-best-practices",children:"Dataset Creation Best Practices"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Planning Phase"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Define the specific perception task the dataset will address"}),"\n",(0,a.jsx)(n.li,{children:"Determine the required scene complexity and variation"}),"\n",(0,a.jsx)(n.li,{children:"Plan for adequate domain randomization coverage"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Scene Design"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Create diverse but representative environments"}),"\n",(0,a.jsx)(n.li,{children:"Include relevant objects and obstacles for humanoid navigation"}),"\n",(0,a.jsx)(n.li,{children:"Consider lighting conditions that match target deployment"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Domain Randomization"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Start with conservative randomization ranges"}),"\n",(0,a.jsx)(n.li,{children:"Gradually expand ranges based on validation results"}),"\n",(0,a.jsx)(n.li,{children:"Monitor synthetic-to-real transfer performance"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Quality Assurance"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implement automated validation checks"}),"\n",(0,a.jsx)(n.li,{children:"Sample dataset regularly during creation"}),"\n",(0,a.jsx)(n.li,{children:"Validate with target perception models"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Render Quality vs Speed"}),": Balance rendering quality with dataset creation speed"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory Management"}),": Monitor GPU and system memory usage"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Storage Requirements"}),": Plan for large storage needs (10k+ images per dataset)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Parallel Processing"}),": Consider multi-process approaches for faster generation"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"This comprehensive workflow provides a complete pipeline for creating synthetic datasets with domain randomization for humanoid robotics applications, including validation and quality assurance measures."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},3023(e,n,t){t.d(n,{R:()=>r,x:()=>o});var a=t(3696);const i={},s=a.createContext(i);function r(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);