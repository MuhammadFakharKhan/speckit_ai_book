"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[905],{3023(e,n,i){i.d(n,{R:()=>c,x:()=>r});var t=i(3696);const o={},s=t.createContext(o);function c(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:c(e.components),t.createElement(s.Provider,{value:n},e.children)}},9129(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>l});var t=i(2540),o=i(3023);const s={title:"VLA Cross-Module References",description:"Cross-references between Vision-Language-Action documentation modules",sidebar_position:15,tags:["vla","cross-reference","integration","documentation"]},c="Cross-Module References: Vision-Language-Action (VLA) System",r={id:"vla-cross-references",title:"VLA Cross-Module References",description:"Cross-references between Vision-Language-Action documentation modules",source:"@site/docs/vla-cross-references.md",sourceDirName:".",slug:"/vla-cross-references",permalink:"/docs/vla-cross-references",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla-cross-references.md",tags:[{label:"vla",permalink:"/docs/tags/vla"},{label:"cross-reference",permalink:"/docs/tags/cross-reference"},{label:"integration",permalink:"/docs/tags/integration"},{label:"documentation",permalink:"/docs/tags/documentation"}],version:"current",sidebarPosition:15,frontMatter:{title:"VLA Cross-Module References",description:"Cross-references between Vision-Language-Action documentation modules",sidebar_position:15,tags:["vla","cross-reference","integration","documentation"]}},a={},l=[{value:"Voice-to-Action \u2192 Cognitive Planning Integration",id:"voice-to-action--cognitive-planning-integration",level:2},{value:"Key Integration Points",id:"key-integration-points",level:3},{value:"Relevant Documentation",id:"relevant-documentation",level:3},{value:"Cognitive Planning \u2192 Action Execution Integration",id:"cognitive-planning--action-execution-integration",level:2},{value:"Key Integration Points",id:"key-integration-points-1",level:3},{value:"Relevant Documentation",id:"relevant-documentation-1",level:3},{value:"Voice-to-Action \u2192 Complete Pipeline Integration",id:"voice-to-action--complete-pipeline-integration",level:2},{value:"Key Integration Points",id:"key-integration-points-2",level:3},{value:"Relevant Documentation",id:"relevant-documentation-2",level:3},{value:"Complete Architecture Flow",id:"complete-architecture-flow",level:2},{value:"Integration Example: Fetch Task",id:"integration-example-fetch-task",level:3},{value:"Best Practices for Cross-Module Integration",id:"best-practices-for-cross-module-integration",level:2},{value:"1. Consistent Data Formats",id:"1-consistent-data-formats",level:3},{value:"2. Error Handling",id:"2-error-handling",level:3},{value:"3. Performance Considerations",id:"3-performance-considerations",level:3},{value:"4. Testing and Validation",id:"4-testing-and-validation",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"cross-module-references-vision-language-action-vla-system",children:"Cross-Module References: Vision-Language-Action (VLA) System"}),"\n",(0,t.jsx)(n.p,{children:"This document provides cross-references and integration points between the different modules of the Vision-Language-Action (VLA) system for autonomous humanoid robots."}),"\n",(0,t.jsx)(n.h2,{id:"voice-to-action--cognitive-planning-integration",children:"Voice-to-Action \u2192 Cognitive Planning Integration"}),"\n",(0,t.jsx)(n.h3,{id:"key-integration-points",children:"Key Integration Points"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Intent Parsing to Task Decomposition"}),": The parsed intents from voice commands feed into the cognitive planning system for task decomposition"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Command Parameters"}),": Parameters extracted during voice processing become inputs for the cognitive planning process"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context Information"}),": Voice command metadata provides context for cognitive planning decisions"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"relevant-documentation",children:"Relevant Documentation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/voice-to-action/intent-parsing",children:"Intent Parsing"})," in Voice-to-Action module"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/cognitive-planning/task-decomposition",children:"Task Decomposition"})," in Cognitive Planning module"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/voice-to-action/command-translation",children:"Command Translation"})," for converting to action sequences"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"cognitive-planning--action-execution-integration",children:"Cognitive Planning \u2192 Action Execution Integration"}),"\n",(0,t.jsx)(n.h3,{id:"key-integration-points-1",children:"Key Integration Points"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Sequences"}),": The cognitive planning system generates action sequences that are executed by the action execution layer"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context Awareness"}),": Planning decisions incorporate environmental and robot state information for action feasibility"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fallback Strategies"}),": Planning includes alternative strategies that can be executed if primary actions fail"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"relevant-documentation-1",children:"Relevant Documentation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/cognitive-planning/action-sequencing",children:"Action Sequencing"})," in Cognitive Planning module"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/voice-to-action/command-translation",children:"Command Translation"})," for ROS 2 command generation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/capstone-system/simulation-setup",children:"Simulation Setup"})," for execution environment"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"voice-to-action--complete-pipeline-integration",children:"Voice-to-Action \u2192 Complete Pipeline Integration"}),"\n",(0,t.jsx)(n.h3,{id:"key-integration-points-2",children:"Key Integration Points"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"End-to-End Flow"}),": Voice commands initiate the complete pipeline from recognition through execution"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feedback Mechanisms"}),": Voice processing results feed into the complete pipeline state management"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validation Points"}),": Confidence scores from voice recognition affect pipeline execution decisions"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"relevant-documentation-2",children:"Relevant Documentation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/voice-to-action/speech-recognition-whisper",children:"Speech Recognition"})," as pipeline entry point"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/capstone-system/complete-workflow",children:"Complete Workflow"})," for full pipeline integration"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/capstone-system/pipeline-integration",children:"Pipeline Integration"})," for system-wide coordination"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"complete-architecture-flow",children:"Complete Architecture Flow"}),"\n",(0,t.jsx)(n.p,{children:"The VLA system follows this complete flow:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice Input"})," \u2192 ",(0,t.jsx)(n.a,{href:"/docs/voice-to-action/speech-recognition-whisper",children:"Speech Recognition"})," \u2192 ",(0,t.jsx)(n.a,{href:"/docs/voice-to-action/intent-parsing",children:"Intent Parsing"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Intent Processing"})," \u2192 ",(0,t.jsx)(n.a,{href:"/docs/cognitive-planning/task-decomposition",children:"Task Decomposition"})," \u2192 ",(0,t.jsx)(n.a,{href:"/docs/cognitive-planning/action-sequencing",children:"Action Sequencing"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Execution"})," \u2192 ",(0,t.jsx)(n.a,{href:"/docs/voice-to-action/command-translation",children:"Command Translation"})," \u2192 ",(0,t.jsx)(n.a,{href:"/docs/capstone-system/complete-workflow",children:"Complete Workflow"})]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"integration-example-fetch-task",children:"Integration Example: Fetch Task"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'User: "Go to the kitchen and bring me the red cup"\n\u2502\n\u251c\u2500 Voice Processing: [Speech Recognition](./voice-to-action/speech-recognition-whisper.md) \u2192 "go to the kitchen and bring me the red cup" (confidence: 0.92)\n\u2502\n\u251c\u2500 Intent Parsing: [Intent Parsing](./voice-to-action/intent-parsing.md) \u2192 {action: "fetch_object", destination: "kitchen", object: "red cup"}\n\u2502\n\u251c\u2500 Cognitive Planning: [Task Decomposition](./cognitive-planning/task-decomposition.md) \u2192 [navigate_to("kitchen"), locate_object("red cup"), grasp_object("red cup"), return_to("starting_position")]\n\u2502\n\u251c\u2500 Action Execution: [Command Translation](./voice-to-action/command-translation.md) \u2192 Execute each step in simulation\n\u2502  \u251c\u2500 Navigation: Plan path to kitchen\n\u2502  \u251c\u2500 Perception: Locate red cup in kitchen\n\u2502  \u251c\u2500 Manipulation: Grasp the red cup\n\u2502  \u2514\u2500 Navigation: Return to starting position\n\u2502\n\u2514\u2500 Result: Task completed successfully\n'})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices-for-cross-module-integration",children:"Best Practices for Cross-Module Integration"}),"\n",(0,t.jsx)(n.h3,{id:"1-consistent-data-formats",children:"1. Consistent Data Formats"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use consistent data structures between voice processing, cognitive planning, and action execution"}),"\n",(0,t.jsx)(n.li,{children:"Maintain standardized confidence scoring across all modules"}),"\n",(0,t.jsx)(n.li,{children:"Follow consistent naming conventions for shared entities"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-error-handling",children:"2. Error Handling"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement graceful degradation when voice recognition confidence is low"}),"\n",(0,t.jsx)(n.li,{children:"Provide fallback strategies when cognitive planning fails to generate valid action sequences"}),"\n",(0,t.jsx)(n.li,{children:"Include error recovery in the complete pipeline"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-performance-considerations",children:"3. Performance Considerations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Optimize data transfer between modules"}),"\n",(0,t.jsx)(n.li,{children:"Consider computational requirements of each module"}),"\n",(0,t.jsx)(n.li,{children:"Implement appropriate buffering and queuing between stages"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"4-testing-and-validation",children:"4. Testing and Validation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Test integration points between modules"}),"\n",(0,t.jsx)(n.li,{children:"Validate data flow from voice input to action execution"}),"\n",(0,t.jsx)(n.li,{children:"Monitor performance metrics across the entire pipeline"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Review the ",(0,t.jsx)(n.a,{href:"/docs/voice-to-action/",children:"Voice-to-Action Pipeline"})," for voice processing details"]}),"\n",(0,t.jsxs)(n.li,{children:["Explore ",(0,t.jsx)(n.a,{href:"/docs/cognitive-planning/",children:"Cognitive Planning"})," for planning algorithms"]}),"\n",(0,t.jsxs)(n.li,{children:["Examine the ",(0,t.jsx)(n.a,{href:"/docs/capstone-system/",children:"Capstone System"})," for complete integration"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);