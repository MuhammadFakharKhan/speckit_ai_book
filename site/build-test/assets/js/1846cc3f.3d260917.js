"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[4031],{3023(e,n,t){t.d(n,{R:()=>s,x:()=>c});var a=t(3696);const o={},i=a.createContext(o);function s(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),a.createElement(i.Provider,{value:n},e.children)}},5991(e,n,t){t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>p,frontMatter:()=>i,metadata:()=>c,toc:()=>l});var a=t(2540),o=t(3023);const i={title:"Isaac ROS to Nav2 Integration",description:"Integration examples showing how Isaac ROS perception data feeds into Nav2 navigation decisions for humanoid robots",sidebar_position:12,tags:["integration","isaac-ros","nav2","navigation","perception"]},s="Isaac ROS to Nav2 Integration",c={id:"isaac-ros-to-nav2-integration",title:"Isaac ROS to Nav2 Integration",description:"Integration examples showing how Isaac ROS perception data feeds into Nav2 navigation decisions for humanoid robots",source:"@site/docs/isaac-ros-to-nav2-integration.md",sourceDirName:".",slug:"/isaac-ros-to-nav2-integration",permalink:"/docs/isaac-ros-to-nav2-integration",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/isaac-ros-to-nav2-integration.md",tags:[{label:"integration",permalink:"/docs/tags/integration"},{label:"isaac-ros",permalink:"/docs/tags/isaac-ros"},{label:"nav2",permalink:"/docs/tags/nav-2"},{label:"navigation",permalink:"/docs/tags/navigation"},{label:"perception",permalink:"/docs/tags/perception"}],version:"current",sidebarPosition:12,frontMatter:{title:"Isaac ROS to Nav2 Integration",description:"Integration examples showing how Isaac ROS perception data feeds into Nav2 navigation decisions for humanoid robots",sidebar_position:12,tags:["integration","isaac-ros","nav2","navigation","perception"]},sidebar:"tutorialSidebar",previous:{title:"Isaac Sim to Isaac ROS Integration",permalink:"/docs/isaac-sim-to-ros-integration"},next:{title:"Isaac Sim to Nav2 Integration",permalink:"/docs/isaac-sim-to-nav2-integration"}},r={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Integration Architecture",id:"integration-architecture",level:2},{value:"Data Flow from Isaac ROS to Nav2",id:"data-flow-from-isaac-ros-to-nav2",level:3},{value:"Key Integration Points",id:"key-integration-points",level:3},{value:"Object Detection to Navigation Integration",id:"object-detection-to-navigation-integration",level:2},{value:"Costmap Integration",id:"costmap-integration",level:3},{value:"Dynamic Obstacle Avoidance",id:"dynamic-obstacle-avoidance",level:3},{value:"Depth Processing Integration",id:"depth-processing-integration",level:2},{value:"Depth to Elevation Map",id:"depth-to-elevation-map",level:3},{value:"Behavior Tree Integration",id:"behavior-tree-integration",level:2},{value:"Perception-Enhanced Behavior Trees",id:"perception-enhanced-behavior-trees",level:3},{value:"Custom Behavior Tree Nodes",id:"custom-behavior-tree-nodes",level:3},{value:"Humanoid-Specific Integration",id:"humanoid-specific-integration",level:2},{value:"Bipedal Navigation with Perception",id:"bipedal-navigation-with-perception",level:3},{value:"Complete Integration Example",id:"complete-integration-example",level:2},{value:"Person-Aware Navigation",id:"person-aware-navigation",level:3},{value:"Multi-Sensor Fusion for Navigation",id:"multi-sensor-fusion-for-navigation",level:3},{value:"Configuration Examples",id:"configuration-examples",level:2},{value:"Isaac ROS to Nav2 Integration Configuration",id:"isaac-ros-to-nav2-integration-configuration",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Integration Best Practices",id:"integration-best-practices",level:3},{value:"Testing Best Practices",id:"testing-best-practices",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Integration Issues",id:"common-integration-issues",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"isaac-ros-to-nav2-integration",children:"Isaac ROS to Nav2 Integration"}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"The integration between Isaac ROS and Nav2 enables perception-enhanced navigation for humanoid robots. This document provides detailed examples of how Isaac ROS perception data feeds into Nav2 navigation decisions, creating intelligent navigation systems that can adapt to dynamic environments and detected objects."}),"\n",(0,a.jsx)(n.h2,{id:"integration-architecture",children:"Integration Architecture"}),"\n",(0,a.jsx)(n.h3,{id:"data-flow-from-isaac-ros-to-nav2",children:"Data Flow from Isaac ROS to Nav2"}),"\n",(0,a.jsx)(n.p,{children:"The integration follows a comprehensive data flow that moves from perception to navigation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph LR\n    A[Isaac ROS Perception] --\x3e B[Object Detection]\n    A --\x3e C[Depth Processing]\n    A --\x3e D[Feature Extraction]\n\n    B --\x3e E[Detected Objects]\n    C --\x3e F[Depth Maps]\n    D --\x3e G[Feature Points]\n\n    E --\x3e H[Nav2 Costmap Layer]\n    F --\x3e I[Nav2 Elevation Map]\n    G --\x3e J[Nav2 Feature Tracker]\n\n    H --\x3e K[Nav2 Path Planner]\n    I --\x3e L[Nav2 3D Navigation]\n    J --\x3e M[Nav2 Behavior Trees]\n\n    K --\x3e N[Navigation Execution]\n    L --\x3e N\n    M --\x3e N\n"})}),"\n",(0,a.jsx)(n.h3,{id:"key-integration-points",children:"Key Integration Points"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Object Detection Integration"}),": Isaac ROS object detections feed into Nav2 costmaps"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Depth Processing"}),": Isaac ROS depth processing enhances Nav2 navigation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Feature Tracking"}),": Isaac ROS features support Nav2 localization and navigation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Dynamic Obstacle Avoidance"}),": Real-time perception enables dynamic navigation"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"object-detection-to-navigation-integration",children:"Object Detection to Navigation Integration"}),"\n",(0,a.jsx)(n.h3,{id:"costmap-integration",children:"Costmap Integration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Isaac ROS to Nav2 costmap integration\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom visualization_msgs.msg import MarkerArray\nfrom geometry_msgs.msg import Point\nfrom nav2_msgs.msg import Costmap\nfrom std_msgs.msg import Header\nfrom builtin_interfaces.msg import Time\nimport numpy as np\n\nclass IsaacROSNav2CostmapBridge(Node):\n    def __init__(self):\n        super().__init__(\'isaac_ros_nav2_costmap_bridge\')\n\n        # Isaac ROS perception subscribers\n        self.detection_sub = self.create_subscription(\n            Detection2DArray,\n            \'/isaac_ros/detections\',\n            self.detection_callback,\n            10\n        )\n\n        self.depth_sub = self.create_subscription(\n            Image,\n            \'/isaac_ros/depth\',\n            self.depth_callback,\n            10\n        )\n\n        # Nav2 costmap publishers\n        self.dynamic_costmap_pub = self.create_publisher(\n            OccupancyGrid,\n            \'/global_costmap/costmap\',\n            10\n        )\n\n        # Object tracking\n        self.object_tracker = ObjectTracker()\n\n        # Costmap integration parameters\n        self.declare_parameter(\'max_object_height\', 2.0)\n        self.declare_parameter(\'object_cost\', 254)  # lethal cost\n        self.declare_parameter(\'clearing_distance\', 0.5)  # meters\n\n    def detection_callback(self, detections):\n        """\n        Process Isaac ROS detections for Nav2 costmap integration\n        """\n        dynamic_obstacles = []\n\n        for detection in detections.detections:\n            # Convert Isaac ROS detection to navigation obstacle\n            obstacle = self.convert_detection_to_obstacle(detection)\n            if obstacle:\n                dynamic_obstacles.append(obstacle)\n\n        # Update Nav2 costmap with dynamic obstacles\n        self.update_costmap_with_obstacles(dynamic_obstacles)\n\n    def convert_detection_to_obstacle(self, detection):\n        """\n        Convert Isaac ROS detection format to navigation obstacle\n        """\n        # Get object pose from detection\n        if detection.bbox.center.x and detection.bbox.center.y:\n            # Convert image coordinates to world coordinates\n            world_pose = self.image_to_world_coordinates(\n                detection.bbox.center.x,\n                detection.bbox.center.y,\n                detection.results[0].id if detection.results else 0\n            )\n\n            # Create obstacle representation\n            obstacle = {\n                \'position\': world_pose,\n                \'size\': self.estimate_object_size(detection),\n                \'confidence\': detection.results[0].score if detection.results else 0.0,\n                \'class_id\': detection.results[0].id if detection.results else 0,\n                \'class_name\': self.get_class_name(detection.results[0].id if detection.results else 0)\n            }\n\n            # Only include obstacles that affect navigation\n            if self.is_navigable_obstacle(obstacle):\n                return obstacle\n\n        return None\n\n    def update_costmap_with_obstacles(self, obstacles):\n        """\n        Update Nav2 costmap with detected obstacles\n        """\n        # Get current costmap\n        current_costmap = self.get_current_costmap()\n\n        # Update costmap with dynamic obstacles\n        for obstacle in obstacles:\n            if obstacle[\'confidence\'] > 0.7:  # High confidence detections only\n                # Calculate affected area in costmap\n                affected_cells = self.calculate_affected_area(\n                    obstacle[\'position\'],\n                    obstacle[\'size\']\n                )\n\n                # Update costmap cells with obstacle information\n                for cell in affected_cells:\n                    if 0 <= cell[0] < current_costmap.info.width and \\\n                       0 <= cell[1] < current_costmap.info.height:\n                        # Set high cost for obstacle location\n                        cell_index = cell[1] * current_costmap.info.width + cell[0]\n                        current_costmap.data[cell_index] = min(\n                            254,  # Max cost\n                            current_costmap.data[cell_index] + 100\n                        )\n\n        # Publish updated costmap\n        self.dynamic_costmap_pub.publish(current_costmap)\n\n    def image_to_world_coordinates(self, u, v, depth):\n        """\n        Convert image coordinates to world coordinates\n        """\n        # Use camera intrinsics to convert to world coordinates\n        # Implementation depends on camera calibration\n        return [0.0, 0.0, 0.0]  # Placeholder\n\n    def estimate_object_size(self, detection):\n        """\n        Estimate object size from detection bounding box\n        """\n        width = detection.bbox.size_x\n        height = detection.bbox.size_y\n        return [width, height]\n\n    def is_navigable_obstacle(self, obstacle):\n        """\n        Check if obstacle affects navigation\n        """\n        # Only consider obstacles within navigable height range\n        max_height = self.get_parameter(\'max_object_height\').value\n        return obstacle[\'position\'][2] <= max_height\n'})}),"\n",(0,a.jsx)(n.h3,{id:"dynamic-obstacle-avoidance",children:"Dynamic Obstacle Avoidance"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Dynamic obstacle avoidance using Isaac ROS perception\nclass DynamicObstacleAvoider:\n    def __init__(self):\n        # Isaac ROS object detection\n        self.detection_sub = self.create_subscription(\n            Detection2DArray,\n            \'/isaac_ros/detections\',\n            self.detection_callback,\n            10\n        )\n\n        # Nav2 navigation interface\n        self.navigator = BasicNavigator()\n\n        # Dynamic obstacle tracking\n        self.obstacle_tracker = ObjectTracker()\n        self.dynamic_path_planner = DynamicPathPlanner()\n\n    def detection_callback(self, detections):\n        """\n        Process Isaac ROS detections for dynamic navigation\n        """\n        # Update obstacle tracker with new detections\n        tracked_obstacles = self.obstacle_tracker.update(detections)\n\n        # Filter for moving obstacles that affect navigation\n        moving_obstacles = [\n            obs for obs in tracked_obstacles\n            if self.is_moving_obstacle(obs) and self.is_navigation_relevant(obs)\n        ]\n\n        # Update navigation with dynamic obstacle information\n        self.update_navigation_with_obstacles(moving_obstacles)\n\n    def update_navigation_with_obstacles(self, obstacles):\n        """\n        Update navigation system with dynamic obstacle information\n        """\n        # If obstacles detected, replan path avoiding them\n        if obstacles:\n            current_pose = self.navigator.getRobotPose()\n            goal_pose = self.navigator.getGoalPose()\n\n            # Plan path avoiding dynamic obstacles\n            safe_path = self.dynamic_path_planner.plan_path_with_obstacles(\n                current_pose,\n                goal_pose,\n                obstacles\n            )\n\n            # Update navigation with safe path\n            self.navigator.updateGlobalPath(safe_path)\n\n    def is_moving_obstacle(self, obstacle):\n        """\n        Check if obstacle is moving and affects navigation\n        """\n        # Check if obstacle velocity exceeds threshold\n        velocity_threshold = 0.1  # m/s\n        return np.linalg.norm(obstacle.velocity) > velocity_threshold\n\n    def is_navigation_relevant(self, obstacle):\n        """\n        Check if obstacle is relevant for navigation\n        """\n        # Check if obstacle is in navigation path\n        # Implementation depends on path prediction\n        return True  # Placeholder\n'})}),"\n",(0,a.jsx)(n.h2,{id:"depth-processing-integration",children:"Depth Processing Integration"}),"\n",(0,a.jsx)(n.h3,{id:"depth-to-elevation-map",children:"Depth to Elevation Map"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Isaac ROS depth processing for Nav2 elevation mapping\nclass DepthToElevationMapper:\n    def __init__(self):\n        # Isaac ROS depth subscription\n        self.depth_sub = self.create_subscription(\n            Image,\n            \'/isaac_ros/depth\',\n            self.depth_callback,\n            10\n        )\n\n        # Nav2 elevation map publisher\n        self.elevation_map_pub = self.create_publisher(\n            OccupancyGrid,\n            \'/elevation_map\',\n            10\n        )\n\n        # Depth processing parameters\n        self.depth_scale_factor = 0.001  # Convert mm to meters\n        self.grid_resolution = 0.1  # 10cm resolution\n        self.elevation_map = None\n\n    def depth_callback(self, depth_msg):\n        """\n        Process Isaac ROS depth data for elevation mapping\n        """\n        # Convert depth image to numpy array\n        depth_array = self.convert_depth_msg_to_array(depth_msg)\n\n        # Process depth data for elevation mapping\n        elevation_data = self.process_depth_for_elevation(depth_array)\n\n        # Update elevation map\n        self.update_elevation_map(elevation_data)\n\n        # Publish elevation map for Nav2\n        self.publish_elevation_map()\n\n    def process_depth_for_elevation(self, depth_array):\n        """\n        Process depth array for elevation mapping\n        """\n        # Apply depth scaling\n        scaled_depth = depth_array * self.depth_scale_factor\n\n        # Filter invalid depth values\n        valid_depth = np.where(scaled_depth > 0, scaled_depth, np.nan)\n\n        # Convert to elevation grid\n        elevation_grid = self.depth_to_elevation_grid(valid_depth)\n\n        return elevation_grid\n\n    def depth_to_elevation_grid(self, depth_data):\n        """\n        Convert depth data to elevation grid\n        """\n        # Convert depth measurements to elevation map\n        # Implementation depends on robot position and orientation\n        return depth_data  # Placeholder\n'})}),"\n",(0,a.jsx)(n.h2,{id:"behavior-tree-integration",children:"Behavior Tree Integration"}),"\n",(0,a.jsx)(n.h3,{id:"perception-enhanced-behavior-trees",children:"Perception-Enhanced Behavior Trees"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Perception-enhanced behavior tree for Nav2 --\x3e\n<root main_tree_to_execute="MainTree">\n    <BehaviorTree ID="MainTree">\n        <ReactiveSequence name="MainNavigationSequence">\n            <GoalUpdated/>\n            <Fallback name="NavigationFallback">\n                <Sequence name="SafeNavigation">\n                    \x3c!-- Check for dynamic obstacles using Isaac ROS perception --\x3e\n                    <IsaacROSDynamicObstacleCheck obstacles="{dynamic_obstacles}"/>\n\n                    \x3c!-- Compute path considering dynamic obstacles --\x3e\n                    <ComputePathToPose goal="{goal}" path="{path}">\n                        <with_behavior>\n                            <IsaacROSObstacleAwarePlanner obstacles="{dynamic_obstacles}"/>\n                        </with_behavior>\n                    </ComputePathToPose>\n\n                    \x3c!-- Execute path with perception feedback --\x3e\n                    <Fallback name="PathExecutionFallback">\n                        <RecoveryNode number_of_retries="3">\n                            <Sequence name="FollowPathWithPerception">\n                                <SmoothPath path="{path}" output="{smoothed_path}"/>\n                                <FollowPath path="{smoothed_path}">\n                                    <with_perception_monitoring>\n                                        <IsaacROSPerceptionMonitor/>\n                                    </with_perception_monitoring>\n                                </FollowPath>\n                            </Sequence>\n                            <ClearEntireCostmap name="ClearLocalCostmap" service_name="local_costmap/clear_entirely_local_costmap"/>\n                        </RecoveryNode>\n\n                        \x3c!-- Alternative navigation if primary fails --\x3e\n                        <ReactiveSequence name="AlternativeNavigation">\n                            <CheckGoalReaching goal="{goal}" tolerance="0.5"/>\n                            <NavigateWithRecovery goal="{goal}">\n                                <with_perception_recovery>\n                                    <IsaacROSPerceptionRecovery/>\n                                </with_perception_recovery>\n                            </NavigateWithRecovery>\n                        </ReactiveSequence>\n                    </Fallback>\n                </Sequence>\n\n                \x3c!-- Emergency navigation if planning fails --\x3e\n                <NavigateToPose goal="{goal}"/>\n            </Fallback>\n        </ReactiveSequence>\n    </BehaviorTree>\n\n    \x3c!-- Custom perception-enhanced nodes --\x3e\n    <TreeNodeModel>\n        <Action ID="IsaacROSDynamicObstacleCheck">\n            <input_port name="detection_topic" type="std::string" default="/isaac_ros/detections"/>\n            <output_port name="dynamic_obstacles" type="vector&lt;Obstacle&gt;"/>\n        </Action>\n        <Action ID="IsaacROSObstacleAwarePlanner">\n            <input_port name="obstacles" type="vector&lt;Obstacle&gt;"/>\n        </Action>\n        <Condition ID="IsaacROSPerceptionMonitor">\n            <input_port name="monitoring_enabled" type="bool" default="true"/>\n        </Condition>\n        <Action ID="IsaacROSPerceptionRecovery">\n            <input_port name="recovery_type" type="std::string" default="perception_guided"/>\n        </Action>\n    </TreeNodeModel>\n</root>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"custom-behavior-tree-nodes",children:"Custom Behavior Tree Nodes"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-cpp",children:'// Custom behavior tree node for Isaac ROS perception integration\n#include "behaviortree_cpp_v3/action_node.h"\n#include "isaac_ros_interfaces/msg/detection2_d_array.hpp"\n#include "geometry_msgs/msg/pose_stamped.hpp"\n\nclass IsaacROSDynamicObstacleCheck : public BT::ActionNodeBase\n{\npublic:\n    IsaacROSDynamicObstacleCheck(const std::string& name, const BT::NodeConfiguration& config)\n        : BT::ActionNodeBase(name, config)\n    {\n        node_ = rclcpp::Node::make_shared("isaac_ros_dynamic_obstacle_check");\n        detection_sub_ = node_->create_subscription<isaac_ros_interfaces::msg::Detection2DArray>(\n            "/isaac_ros/detections", 10,\n            std::bind(&IsaacROSDynamicObstacleCheck::detectionCallback, this, std::placeholders::_1)\n        );\n    }\n\n    static BT::PortsList providedPorts()\n    {\n        return {\n            BT::OutputPort<std::vector<Obstacle>>("dynamic_obstacles", "Detected dynamic obstacles"),\n            BT::InputPort<std::string>("detection_topic", "Topic for Isaac ROS detections")\n        };\n    }\n\nprivate:\n    BT::NodeStatus tick() override\n    {\n        // Wait for latest detections\n        if (!latest_detections_) {\n            return BT::NodeStatus::RUNNING;\n        }\n\n        // Process detections for dynamic obstacles\n        std::vector<Obstacle> dynamic_obstacles = extractDynamicObstacles(*latest_detections_);\n\n        // Output dynamic obstacles\n        if (auto obstacles_port = config().blackboard->get<std::vector<Obstacle>*>("dynamic_obstacles")) {\n            *obstacles_port = dynamic_obstacles;\n        }\n\n        // Determine status based on obstacle detection\n        if (!dynamic_obstacles.empty()) {\n            return BT::NodeStatus::SUCCESS;  // Obstacles detected, need to avoid\n        } else {\n            return BT::NodeStatus::FAILURE;  // No obstacles, can proceed normally\n        }\n    }\n\n    void halt() override\n    {\n        // Reset state\n    }\n\n    void detectionCallback(const isaac_ros_interfaces::msg::Detection2DArray::SharedPtr msg)\n    {\n        latest_detections_ = msg;\n    }\n\n    std::vector<Obstacle> extractDynamicObstacles(const isaac_ros_interfaces::msg::Detection2DArray& detections)\n    {\n        std::vector<Obstacle> dynamic_obstacles;\n\n        // Extract obstacles that are likely to move or affect navigation\n        for (const auto& detection : detections.detections) {\n            if (isDynamicObject(detection)) {\n                Obstacle obstacle = convertDetectionToObstacle(detection);\n                dynamic_obstacles.push_back(obstacle);\n            }\n        }\n\n        return dynamic_obstacles;\n    }\n\n    bool isDynamicObject(const Detection2D& detection)\n    {\n        // Check if detection is for a dynamic object (person, vehicle, etc.)\n        // Implementation depends on object classes\n        return detection.results[0].id == 0;  // Example: person class\n    }\n\n    Obstacle convertDetectionToObstacle(const Detection2D& detection)\n    {\n        Obstacle obstacle;\n        // Convert detection to obstacle format\n        return obstacle;\n    }\n\n    rclcpp::Node::SharedPtr node_;\n    rclcpp::Subscription<isaac_ros_interfaces::msg::Detection2DArray>::SharedPtr detection_sub_;\n    isaac_ros_interfaces::msg::Detection2DArray::SharedPtr latest_detections_;\n};\n'})}),"\n",(0,a.jsx)(n.h2,{id:"humanoid-specific-integration",children:"Humanoid-Specific Integration"}),"\n",(0,a.jsx)(n.h3,{id:"bipedal-navigation-with-perception",children:"Bipedal Navigation with Perception"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Humanoid-specific navigation with Isaac ROS perception\nclass HumanoidPerceptionNavigation:\n    def __init__(self):\n        # Isaac ROS perception\n        self.perception_sub = self.create_subscription(\n            Detection2DArray,\n            \'/isaac_ros/detections\',\n            self.perception_callback,\n            10\n        )\n\n        # Nav2 navigation for humanoid\n        self.navigator = HumanoidNavigator()\n\n        # Humanoid-specific parameters\n        self.step_size = 0.4  # meters\n        self.balance_margin = 0.1  # meters\n        self.z_axis_navigation = True\n\n    def perception_callback(self, detections):\n        """\n        Process Isaac ROS detections for humanoid navigation\n        """\n        # Process detections for humanoid-specific navigation\n        humanoid_obstacles = self.filter_humanoid_obstacles(detections)\n\n        # Update navigation with humanoid-specific obstacle information\n        self.update_humanoid_navigation(humanoid_obstacles)\n\n    def filter_humanoid_obstacles(self, detections):\n        """\n        Filter obstacles relevant for humanoid navigation\n        """\n        humanoid_obstacles = []\n\n        for detection in detections.detections:\n            # Consider obstacles at humanoid height\n            if self.is_humanoid_relevant_obstacle(detection):\n                humanoid_obstacles.append(detection)\n\n        return humanoid_obstacles\n\n    def is_humanoid_relevant_obstacle(self, detection):\n        """\n        Check if obstacle is relevant for humanoid navigation\n        """\n        # Check if obstacle height is within humanoid navigation range\n        # Consider obstacles that could affect bipedal locomotion\n        return True  # Placeholder\n\n    def update_humanoid_navigation(self, obstacles):\n        """\n        Update humanoid navigation with perception data\n        """\n        # Plan footstep path considering obstacles\n        footstep_path = self.plan_footstep_path_with_obstacles(obstacles)\n\n        # Execute footstep navigation\n        self.navigator.execute_footstep_navigation(footstep_path)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"complete-integration-example",children:"Complete Integration Example"}),"\n",(0,a.jsx)(n.h3,{id:"person-aware-navigation",children:"Person-Aware Navigation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Complete integration example: Person-aware navigation\nclass PersonAwareNavigation:\n    def __init__(self):\n        # Isaac ROS person detection\n        self.person_detector = IsaacROSPersonDetector()\n\n        # Nav2 navigation\n        self.navigator = BasicNavigator()\n\n        # Social navigation behaviors\n        self.social_navigator = SocialNavigationModule()\n\n    def navigate_with_person_awareness(self, goal_pose):\n        """\n        Navigate to goal while being aware of detected persons\n        """\n        # Start person detection\n        self.person_detector.start_detection()\n\n        # Plan initial path\n        initial_path = self.navigator.getPath(goal_pose)\n\n        # Monitor for persons during navigation\n        person_monitor = PersonMonitor(\n            detection_callback=self.person_detection_callback\n        )\n\n        # Navigate with person awareness\n        success = self.execute_person_aware_navigation(\n            goal_pose,\n            person_monitor\n        )\n\n        return success\n\n    def person_detection_callback(self, detections):\n        """\n        Handle person detections for navigation\n        """\n        persons = [det for det in detections.detections\n                  if det.results and det.results[0].id == 0]  # Person class\n\n        if persons:\n            # Update navigation behavior based on person locations\n            self.update_navigation_for_social_interaction(persons)\n\n    def update_navigation_for_social_interaction(self, persons):\n        """\n        Update navigation for social interaction with detected persons\n        """\n        for person in persons:\n            person_position = self.detection_to_world_coordinates(person)\n\n            # Check if person is in navigation path\n            if self.is_person_on_path(person_position):\n                # Adjust navigation for social behavior\n                self.social_navigator.adjust_navigation_for_person(\n                    person_position,\n                    preferred_distance=1.0  # meter\n                )\n\n    def execute_person_aware_navigation(self, goal_pose, person_monitor):\n        """\n        Execute navigation with continuous person awareness\n        """\n        try:\n            # Start person monitoring\n            person_monitor.start_monitoring()\n\n            # Execute navigation\n            result = self.navigator.goToPose(goal_pose)\n\n            # Wait for completion\n            while not result.done():\n                # Check for new person detections\n                person_detections = person_monitor.get_latest_detections()\n\n                if person_detections:\n                    self.person_detection_callback(person_detections)\n\n            return result.success\n\n        finally:\n            person_monitor.stop_monitoring()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"multi-sensor-fusion-for-navigation",children:"Multi-Sensor Fusion for Navigation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Multi-sensor fusion for enhanced navigation\nclass MultiSensorFusionNavigation:\n    def __init__(self):\n        # Isaac ROS sensors\n        self.object_detector = self.create_subscription(\n            Detection2DArray, \'/isaac_ros/detections\', self.detection_callback, 10\n        )\n        self.depth_processor = self.create_subscription(\n            Image, \'/isaac_ros/depth\', self.depth_callback, 10\n        )\n        self.vslam_processor = self.create_subscription(\n            PoseStamped, \'/isaac_ros/vslam/pose\', self.vslam_callback, 10\n        )\n\n        # Fusion module\n        self.sensor_fusion = SensorFusionModule()\n\n        # Nav2 navigation\n        self.navigator = BasicNavigator()\n\n    def detection_callback(self, detections):\n        """\n        Handle object detections for navigation\n        """\n        self.sensor_fusion.add_detection_data(detections)\n\n    def depth_callback(self, depth_msg):\n        """\n        Handle depth data for navigation\n        """\n        self.sensor_fusion.add_depth_data(depth_msg)\n\n    def vslam_callback(self, pose_msg):\n        """\n        Handle VSLAM pose for navigation\n        """\n        self.sensor_fusion.add_pose_data(pose_msg)\n\n    def get_fused_navigation_data(self):\n        """\n        Get fused sensor data for navigation\n        """\n        return self.sensor_fusion.get_fused_data()\n\n    def navigate_with_sensor_fusion(self, goal_pose):\n        """\n        Navigate using fused sensor data\n        """\n        # Get fused sensor data\n        fused_data = self.get_fused_navigation_data()\n\n        # Plan path considering all sensor inputs\n        path = self.plan_fused_sensor_path(goal_pose, fused_data)\n\n        # Execute navigation with sensor fusion\n        result = self.navigator.followPath(path)\n\n        return result\n\n    def plan_fused_sensor_path(self, goal_pose, fused_data):\n        """\n        Plan path using fused sensor data\n        """\n        # Consider multiple sensor inputs:\n        # - Object detections for obstacle avoidance\n        # - Depth data for terrain analysis\n        # - VSLAM pose for localization\n        # - Other sensors as needed\n\n        return self.navigator.getPath(goal_pose)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"configuration-examples",children:"Configuration Examples"}),"\n",(0,a.jsx)(n.h3,{id:"isaac-ros-to-nav2-integration-configuration",children:"Isaac ROS to Nav2 Integration Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# Isaac ROS to Nav2 integration configuration\nisaac_ros_nav2_integration:\n  ros__parameters:\n    # Perception integration\n    perception:\n      detection_topic: "/isaac_ros/detections"\n      depth_topic: "/isaac_ros/depth"\n      vslam_topic: "/isaac_ros/vslam/pose"\n      detection_confidence_threshold: 0.7\n      max_detection_distance: 10.0  # meters\n\n    # Costmap integration\n    costmap:\n      enable_dynamic_obstacles: true\n      dynamic_obstacle_topic: "/dynamic_obstacles"\n      max_obstacle_height: 2.0  # meters\n      obstacle_cost: 254  # lethal\n      clearing_distance: 0.5  # meters\n\n    # Navigation parameters\n    navigation:\n      enable_perception_guided: true\n      perception_timeout: 5.0  # seconds\n      min_perception_frequency: 10.0  # Hz\n      safety_distance: 0.8  # meters from detected obstacles\n\n    # Humanoid-specific parameters\n    humanoid:\n      enable_footstep_planning: true\n      step_size_max: 0.4\n      balance_margin: 0.15\n      z_axis_navigation: true\n\n# Nav2 costmap configuration with Isaac ROS integration\nlocal_costmap:\n  local_costmap:\n    ros__parameters:\n      update_frequency: 5.0\n      publish_frequency: 2.0\n      global_frame: odom\n      robot_base_frame: base_link\n      use_sim_time: false\n      rolling_window: true\n      width: 6\n      height: 6\n      resolution: 0.05\n      # Isaac ROS perception integration\n      plugins: [\n        "static_layer",\n        "obstacle_layer",\n        "isaac_ros_perception_layer",  # Isaac ROS integration\n        "inflation_layer"\n      ]\n      isaac_ros_perception_layer:\n        plugin: "isaac_ros_nav2_integration.IsaacROSPeceptionLayer"\n        enabled: true\n        observation_sources: ["isaac_ros_detections"]\n        isaac_ros_detections:\n          topic: "/isaac_ros/detections"\n          max_obstacle_height: 2.0\n          clearing: true\n          marking: true\n          data_type: "PointCloud2"\n          expected_update_rate: 10.0\n          observation_persistence: 0.1\n          marking_threshold: 1\n'})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsx)(n.h3,{id:"integration-best-practices",children:"Integration Best Practices"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Synchronization"}),": Ensure proper synchronization between Isaac ROS and Nav2 data streams"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Performance Optimization"}),": Optimize perception processing for real-time navigation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Safety Considerations"}),": Implement safety measures for perception-based navigation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Fallback Mechanisms"}),": Provide fallback navigation when perception fails"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"testing-best-practices",children:"Testing Best Practices"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simulation Testing"}),": Test integration in simulation before real robot deployment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Progressive Complexity"}),": Start with simple scenarios and increase complexity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validation Framework"}),": Implement comprehensive validation of perception-navigation integration"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Edge Case Handling"}),": Test edge cases and failure scenarios"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsx)(n.h3,{id:"common-integration-issues",children:"Common Integration Issues"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# Troubleshooting guide for Isaac ROS to Nav2 integration\ntroubleshooting:\n  common_issues:\n    - issue: "Perception data not affecting navigation"\n      symptoms:\n        - "Detected objects don\'t appear in costmap"\n        - "Navigation ignores dynamic obstacles"\n      causes:\n        - "Topic remapping issues"\n        - "Coordinate frame mismatches"\n        - "Costmap configuration problems"\n      solutions:\n        - "Verify topic connections with \'ros2 topic list\'"\n        - "Check TF tree for frame relationships"\n        - "Validate costmap configuration"\n\n    - issue: "High computational load during integration"\n      symptoms:\n        - "Navigation performance degradation"\n        - "High CPU/GPU usage"\n      causes:\n        - "Frequent perception updates"\n        - "Complex processing algorithms"\n        - "Inefficient data structures"\n      solutions:\n        - "Throttle perception update rate"\n        - "Optimize processing algorithms"\n        - "Use efficient data structures"\n\n    - issue: "Perception-navigation timing issues"\n      symptoms:\n        - "Navigation decisions based on outdated perception"\n        - "Synchronization problems"\n      causes:\n        - "Message timestamp issues"\n        - "Processing delays"\n        - "Buffer overflows"\n      solutions:\n        - "Implement proper timestamp handling"\n        - "Optimize processing pipeline"\n        - "Use appropriate buffer sizes"\n'})}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(n.p,{children:"The integration between Isaac ROS and Nav2 creates powerful perception-enhanced navigation capabilities for humanoid robots. By leveraging Isaac ROS perception data, Nav2 can make more intelligent navigation decisions, avoid dynamic obstacles, and adapt to changing environments. This integration enables robots to operate more safely and effectively in complex, human-populated environments."})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);