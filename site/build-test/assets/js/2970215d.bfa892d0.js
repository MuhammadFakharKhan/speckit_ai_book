"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[7789],{1112(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var t=i(2540),o=i(3023);const s={title:"End-to-End Pipeline Overview",sidebar_label:"Overview",description:"Complete pipeline from voice recognition through cognitive planning to navigation, perception, and manipulation in simulated humanoid environments"},a="End-to-End Autonomous Pipeline",r={id:"capstone-system/index",title:"End-to-End Pipeline Overview",description:"Complete pipeline from voice recognition through cognitive planning to navigation, perception, and manipulation in simulated humanoid environments",source:"@site/docs/capstone-system/index.md",sourceDirName:"capstone-system",slug:"/capstone-system/",permalink:"/docs/capstone-system/",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/capstone-system/index.md",tags:[],version:"current",frontMatter:{title:"End-to-End Pipeline Overview",sidebar_label:"Overview",description:"Complete pipeline from voice recognition through cognitive planning to navigation, perception, and manipulation in simulated humanoid environments"},sidebar:"tutorialSidebar",previous:{title:"Planning Validation with LLMs and Action Feasibility Checks",permalink:"/docs/cognitive-planning/validation"},next:{title:"Pipeline Integration",permalink:"/docs/capstone-system/pipeline-integration"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Integration Points",id:"integration-points",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"Validation",id:"validation",level:2}];function d(e){const n={h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"end-to-end-autonomous-pipeline",children:"End-to-End Autonomous Pipeline"}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"The end-to-end autonomous pipeline represents the complete integration of Vision-Language-Action (VLA) capabilities in humanoid robotics. This system demonstrates how voice commands flow through multiple processing layers to execute complex robotic behaviors in simulated environments."}),"\n",(0,t.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,t.jsx)(n.p,{children:"The complete pipeline consists of three major components working in sequence:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice Recognition Layer"}),": Processes natural language commands using OpenAI Whisper"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cognitive Planning Layer"}),": Translates high-level tasks into executable action sequences using LLMs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Execution Layer"}),": Coordinates navigation, perception, and manipulation using ROS 2 interfaces"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"integration-points",children:"Integration Points"}),"\n",(0,t.jsx)(n.p,{children:"The pipeline integrates seamlessly with the NVIDIA Isaac ecosystem:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Isaac Sim provides the simulation environment for testing and validation"}),"\n",(0,t.jsx)(n.li,{children:"Isaac ROS handles perception and sensor processing"}),"\n",(0,t.jsx)(n.li,{children:"Nav2 manages navigation and path planning for bipedal robots"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,t.jsx)(n.p,{children:"This pipeline enables complex humanoid behaviors such as:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Voice-commanded navigation through complex environments"}),"\n",(0,t.jsx)(n.li,{children:"Multi-step task execution with environmental interaction"}),"\n",(0,t.jsx)(n.li,{children:"Adaptive behavior based on real-time perception"}),"\n",(0,t.jsx)(n.li,{children:"Context-aware manipulation and navigation"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"validation",children:"Validation"}),"\n",(0,t.jsx)(n.p,{children:"All pipeline components are validated through simulated examples that demonstrate the complete flow from voice command to robotic action execution."})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},3023(e,n,i){i.d(n,{R:()=>a,x:()=>r});var t=i(3696);const o={},s=t.createContext(o);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);