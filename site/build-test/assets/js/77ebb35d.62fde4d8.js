"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[6634],{3023(n,e,t){t.d(e,{R:()=>o,x:()=>a});var i=t(3696);const s={},r=i.createContext(s);function o(n){const e=i.useContext(r);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),i.createElement(r.Provider,{value:e},n.children)}},3661(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>a,toc:()=>c});var i=t(2540),s=t(3023);const r={title:"Intent Parsing and Natural Language Understanding",description:"Documentation on parsing natural language voice commands to extract actionable intents in VLA systems",sidebar_position:3,tags:["vla","intent-parsing","nlu","nlp","natural-language-processing"]},o="Intent Parsing and Natural Language Understanding",a={id:"voice-to-action/intent-parsing",title:"Intent Parsing and Natural Language Understanding",description:"Documentation on parsing natural language voice commands to extract actionable intents in VLA systems",source:"@site/docs/voice-to-action/intent-parsing.md",sourceDirName:"voice-to-action",slug:"/voice-to-action/intent-parsing",permalink:"/docs/voice-to-action/intent-parsing",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/voice-to-action/intent-parsing.md",tags:[{label:"vla",permalink:"/docs/tags/vla"},{label:"intent-parsing",permalink:"/docs/tags/intent-parsing"},{label:"nlu",permalink:"/docs/tags/nlu"},{label:"nlp",permalink:"/docs/tags/nlp"},{label:"natural-language-processing",permalink:"/docs/tags/natural-language-processing"}],version:"current",sidebarPosition:3,frontMatter:{title:"Intent Parsing and Natural Language Understanding",description:"Documentation on parsing natural language voice commands to extract actionable intents in VLA systems",sidebar_position:3,tags:["vla","intent-parsing","nlu","nlp","natural-language-processing"]},sidebar:"tutorialSidebar",previous:{title:"Speech Recognition with OpenAI Whisper",permalink:"/docs/voice-to-action/speech-recognition-whisper"},next:{title:"Command Translation to ROS 2",permalink:"/docs/voice-to-action/command-translation"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Intent Parsing Architecture",id:"intent-parsing-architecture",level:2},{value:"Processing Pipeline",id:"processing-pipeline",level:3},{value:"Core Components",id:"core-components",level:3},{value:"Intent Classification",id:"intent-classification",level:2},{value:"Command Categories",id:"command-categories",level:3},{value:"Navigation Intents",id:"navigation-intents",level:4},{value:"Manipulation Intents",id:"manipulation-intents",level:4},{value:"Perception Intents",id:"perception-intents",level:4},{value:"Complex Intents",id:"complex-intents",level:4},{value:"Intent Recognition Implementation",id:"intent-recognition-implementation",level:3},{value:"Named Entity Recognition",id:"named-entity-recognition",level:2},{value:"Entity Types",id:"entity-types",level:3},{value:"Location Entities",id:"location-entities",level:4},{value:"Object Entities",id:"object-entities",level:4},{value:"Action Entities",id:"action-entities",level:4},{value:"Entity Extraction Implementation",id:"entity-extraction-implementation",level:3},{value:"Context Integration",id:"context-integration",level:2},{value:"Environmental Context",id:"environmental-context",level:3},{value:"Confidence Scoring and Validation",id:"confidence-scoring-and-validation",level:2},{value:"Confidence Calculation",id:"confidence-calculation",level:3},{value:"Error Handling and Disambiguation",id:"error-handling-and-disambiguation",level:2},{value:"Ambiguity Resolution",id:"ambiguity-resolution",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Message Definitions",id:"message-definitions",level:3},{value:"Publisher Example",id:"publisher-example",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching Strategies",id:"caching-strategies",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Accuracy Improvements",id:"accuracy-improvements",level:3},{value:"Error Handling",id:"error-handling",level:3},{value:"Performance Considerations",id:"performance-considerations",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Diagnostic Tools",id:"diagnostic-tools",level:3},{value:"Future Enhancements",id:"future-enhancements",level:2},{value:"Advanced NLU Features",id:"advanced-nlu-features",level:3},{value:"Integration Improvements",id:"integration-improvements",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"intent-parsing-and-natural-language-understanding",children:"Intent Parsing and Natural Language Understanding"}),"\n",(0,i.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(e.p,{children:"Intent parsing is the critical component that transforms recognized speech into structured, actionable commands within the Vision-Language-Action (VLA) system. This process extracts the semantic meaning from natural language input, identifying the user's intent and relevant parameters to enable appropriate robot action execution."}),"\n",(0,i.jsx)(e.h2,{id:"intent-parsing-architecture",children:"Intent Parsing Architecture"}),"\n",(0,i.jsx)(e.h3,{id:"processing-pipeline",children:"Processing Pipeline"}),"\n",(0,i.jsx)(e.p,{children:"The intent parsing system follows a multi-stage pipeline:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Raw Text \u2192 Tokenization \u2192 Part-of-Speech Tagging \u2192 Named Entity Recognition \u2192 Intent Classification \u2192 Parameter Extraction \u2192 Structured Command\n"})}),"\n",(0,i.jsx)(e.p,{children:"Each stage builds upon the previous one to extract increasingly structured information from the natural language input."}),"\n",(0,i.jsx)(e.h3,{id:"core-components",children:"Core Components"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Text Preprocessing"}),": Clean and normalize the input text"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Intent Classification"}),": Identify the primary action or task"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Entity Extraction"}),": Extract relevant parameters (locations, objects, quantities)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Context Integration"}),": Incorporate environmental and robot state context"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Command Validation"}),": Verify the parsed command is feasible"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"intent-classification",children:"Intent Classification"}),"\n",(0,i.jsx)(e.h3,{id:"command-categories",children:"Command Categories"}),"\n",(0,i.jsx)(e.p,{children:"The VLA system recognizes several primary intent categories:"}),"\n",(0,i.jsx)(e.h4,{id:"navigation-intents",children:"Navigation Intents"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"MoveTo"}),": Direct the robot to navigate to a specific location"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:'Example: "Go to the kitchen", "Move to the table"'}),"\n",(0,i.jsx)(e.li,{children:"Parameters: destination (location)"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"MoveDirection"}),": Direct the robot to move in a specific direction"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:'Example: "Move forward 2 meters", "Turn left and walk"'}),"\n",(0,i.jsx)(e.li,{children:"Parameters: direction, distance"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h4,{id:"manipulation-intents",children:"Manipulation Intents"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"PickUp"}),": Instruct the robot to pick up an object"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:'Example: "Pick up the red cup", "Grasp the book"'}),"\n",(0,i.jsx)(e.li,{children:"Parameters: object (type, color, description)"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Place"}),": Direct the robot to place an object at a location"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:'Example: "Put the cup on the table", "Place it there"'}),"\n",(0,i.jsx)(e.li,{children:"Parameters: object, destination"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h4,{id:"perception-intents",children:"Perception Intents"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"FindObject"}),": Request the robot to locate specific objects"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:'Example: "Find the blue ball", "Look for the door"'}),"\n",(0,i.jsx)(e.li,{children:"Parameters: object (type, color, description)"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Describe"}),": Request environmental information"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:'Example: "What do you see?", "Describe the room"'}),"\n",(0,i.jsx)(e.li,{children:"Parameters: none"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h4,{id:"complex-intents",children:"Complex Intents"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Fetch"}),": Multi-step command combining navigation and manipulation"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:'Example: "Go to the kitchen and bring me the coffee"'}),"\n",(0,i.jsx)(e.li,{children:"Parameters: destination, object"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Follow"}),": Direct the robot to follow a person or object"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:'Example: "Follow me", "Follow the person"'}),"\n",(0,i.jsx)(e.li,{children:"Parameters: target (person, object)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"intent-recognition-implementation",children:"Intent Recognition Implementation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\nclass IntentType(Enum):\n    MOVE_TO = "move_to"\n    MOVE_DIRECTION = "move_direction"\n    PICK_UP = "pick_up"\n    PLACE = "place"\n    FIND_OBJECT = "find_object"\n    DESCRIBE = "describe"\n    FETCH = "fetch"\n    FOLLOW = "follow"\n\n@dataclass\nclass IntentParameters:\n    destination: Optional[str] = None\n    direction: Optional[str] = None\n    distance: Optional[float] = None\n    object_type: Optional[str] = None\n    object_color: Optional[str] = None\n    object_description: Optional[str] = None\n    target: Optional[str] = None\n\n@dataclass\nclass ParsedIntent:\n    intent_type: IntentType\n    parameters: IntentParameters\n    confidence: float\n    original_text: str\n\nclass IntentClassifier:\n    def __init__(self):\n        self.intent_patterns = self._load_intent_patterns()\n        self.entity_extractors = self._initialize_entity_extractors()\n\n    def _load_intent_patterns(self) -> Dict[IntentType, List[str]]:\n        """\n        Load pattern templates for intent classification\n        """\n        return {\n            IntentType.MOVE_TO: [\n                r"go to the (?P<destination>\\w+)",\n                r"move to the (?P<destination>\\w+)",\n                r"navigate to (?P<destination>\\w+)",\n                r"head to (?P<destination>\\w+)"\n            ],\n            IntentType.MOVE_DIRECTION: [\n                r"move (?P<direction>\\w+) (?P<distance>\\d+) meters?",\n                r"go (?P<direction>\\w+) (?P<distance>\\d+) meters?",\n                r"turn (?P<direction>\\w+) and (?P<action>\\w+)"\n            ],\n            IntentType.PICK_UP: [\n                r"pick up the (?P<object_color>\\w+)?\\s*(?P<object_type>\\w+)",\n                r"grasp the (?P<object_type>\\w+)",\n                r"take the (?P<object_type>\\w+)"\n            ],\n            IntentType.FETCH: [\n                r"go to the (?P<destination>\\w+) and bring me the (?P<object_type>\\w+)",\n                r"get me the (?P<object_type>\\w+) from the (?P<destination>\\w+)"\n            ]\n            # Additional patterns for other intent types...\n        }\n\n    def classify_intent(self, text: str) -> Optional[ParsedIntent]:\n        """\n        Classify the intent of the given text\n        """\n        text_lower = text.lower().strip()\n\n        for intent_type, patterns in self.intent_patterns.items():\n            for pattern in patterns:\n                import re\n                match = re.search(pattern, text_lower)\n                if match:\n                    # Extract parameters based on pattern groups\n                    params = IntentParameters()\n                    for key, value in match.groupdict().items():\n                        if hasattr(params, key):\n                            setattr(params, key, value)\n\n                    # Convert distance to float if present\n                    if params.distance:\n                        try:\n                            params.distance = float(params.distance)\n                        except ValueError:\n                            pass\n\n                    return ParsedIntent(\n                        intent_type=intent_type,\n                        parameters=params,\n                        confidence=0.85,  # Placeholder confidence\n                        original_text=text\n                    )\n\n        return None\n'})}),"\n",(0,i.jsx)(e.h2,{id:"named-entity-recognition",children:"Named Entity Recognition"}),"\n",(0,i.jsx)(e.h3,{id:"entity-types",children:"Entity Types"}),"\n",(0,i.jsx)(e.p,{children:"The system recognizes several entity types critical for command execution:"}),"\n",(0,i.jsx)(e.h4,{id:"location-entities",children:"Location Entities"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Rooms"}),": kitchen, living room, bedroom, bathroom, office"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Furniture"}),": table, chair, couch, counter, shelf"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Objects"}),": door, window, light, switch"]}),"\n"]}),"\n",(0,i.jsx)(e.h4,{id:"object-entities",children:"Object Entities"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Types"}),": cup, book, ball, phone, keys, computer"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Colors"}),": red, blue, green, yellow, black, white"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Descriptors"}),": big, small, heavy, light, round, square"]}),"\n"]}),"\n",(0,i.jsx)(e.h4,{id:"action-entities",children:"Action Entities"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Directions"}),": forward, backward, left, right, up, down"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Distances"}),": meters, feet, steps, units"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Quantities"}),": one, two, several, all, some"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"entity-extraction-implementation",children:"Entity Extraction Implementation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import re\nfrom typing import List, Tuple\n\nclass EntityExtractor:\n    def __init__(self):\n        self.location_entities = [\n            "kitchen", "living room", "bedroom", "bathroom", "office",\n            "dining room", "hallway", "garage", "garden", "table",\n            "chair", "couch", "counter", "shelf", "door", "window"\n        ]\n\n        self.object_entities = [\n            "cup", "book", "ball", "phone", "keys", "computer",\n            "bottle", "plate", "glass", "box", "bag", "pen",\n            "pencil", "notebook", "laptop", "tablet", "watch"\n        ]\n\n        self.color_entities = [\n            "red", "blue", "green", "yellow", "purple", "orange",\n            "pink", "brown", "black", "white", "gray", "silver",\n            "gold", "cyan", "magenta", "lime", "navy", "maroon"\n        ]\n\n        self.direction_entities = [\n            "forward", "backward", "back", "left", "right",\n            "up", "down", "north", "south", "east", "west"\n        ]\n\n    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n        """\n        Extract named entities from the given text\n        """\n        entities = {\n            "location": [],\n            "object": [],\n            "color": [],\n            "direction": []\n        }\n\n        text_lower = text.lower()\n\n        # Extract location entities\n        for location in self.location_entities:\n            if location in text_lower:\n                entities["location"].append(location)\n\n        # Extract object entities\n        for obj in self.object_entities:\n            if obj in text_lower:\n                entities["object"].append(obj)\n\n        # Extract color entities\n        for color in self.color_entities:\n            if color in text_lower:\n                entities["color"].append(color)\n\n        # Extract direction entities\n        for direction in self.direction_entities:\n            if direction in text_lower:\n                entities["direction"].append(direction)\n\n        return entities\n\n    def extract_quantities(self, text: str) -> List[float]:\n        """\n        Extract numerical quantities from text\n        """\n        # Pattern for numbers (integers and floats)\n        number_pattern = r\'\\b\\d+(?:\\.\\d+)?\\b\'\n        numbers = re.findall(number_pattern, text)\n        return [float(num) for num in numbers]\n'})}),"\n",(0,i.jsx)(e.h2,{id:"context-integration",children:"Context Integration"}),"\n",(0,i.jsx)(e.h3,{id:"environmental-context",children:"Environmental Context"}),"\n",(0,i.jsx)(e.p,{children:"The intent parsing system incorporates environmental context to improve accuracy:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'@dataclass\nclass EnvironmentalContext:\n    known_locations: List[str]\n    visible_objects: List[Dict[str, str]]  # Objects with type, color, location\n    robot_capabilities: List[str]\n    robot_position: Dict[str, float]  # x, y, z coordinates\n    current_task: Optional[str]\n\nclass ContextAwareIntentParser:\n    def __init__(self):\n        self.intent_classifier = IntentClassifier()\n        self.entity_extractor = EntityExtractor()\n        self.current_context = EnvironmentalContext(\n            known_locations=[],\n            visible_objects=[],\n            robot_capabilities=[],\n            robot_position={"x": 0.0, "y": 0.0, "z": 0.0},\n            current_task=None\n        )\n\n    def parse_intent_with_context(self, text: str, context: EnvironmentalContext = None) -> Optional[ParsedIntent]:\n        """\n        Parse intent considering environmental context\n        """\n        if context:\n            self.current_context = context\n\n        # First, do basic intent parsing\n        basic_intent = self.intent_classifier.classify_intent(text)\n        if not basic_intent:\n            return None\n\n        # Enhance with context\n        enhanced_intent = self._enhance_with_context(basic_intent)\n\n        return enhanced_intent\n\n    def _enhance_with_context(self, intent: ParsedIntent) -> ParsedIntent:\n        """\n        Enhance parsed intent with environmental context\n        """\n        # Resolve ambiguous locations based on known locations\n        if intent.parameters.destination:\n            intent.parameters.destination = self._resolve_ambiguous_location(\n                intent.parameters.destination\n            )\n\n        # Resolve ambiguous objects based on visible objects\n        if intent.parameters.object_type:\n            intent.parameters.object_type = self._resolve_ambiguous_object(\n                intent.parameters.object_type,\n                intent.parameters.object_color\n            )\n\n        return intent\n\n    def _resolve_ambiguous_location(self, location: str) -> str:\n        """\n        Resolve potentially ambiguous location based on known locations\n        """\n        # If the location is already known, return as-is\n        if location in self.current_context.known_locations:\n            return location\n\n        # Try to find a match among known locations\n        for known_location in self.current_context.known_locations:\n            if location.lower() in known_location.lower() or known_location.lower() in location.lower():\n                return known_location\n\n        # If no match found, return original (will need clarification)\n        return location\n\n    def _resolve_ambiguous_object(self, obj_type: str, obj_color: str = None) -> str:\n        """\n        Resolve potentially ambiguous object based on visible objects\n        """\n        # Filter visible objects by type and color\n        matching_objects = [\n            obj for obj in self.current_context.visible_objects\n            if obj.get(\'type\', \'\').lower() == obj_type.lower() and\n            (not obj_color or obj.get(\'color\', \'\').lower() == obj_color.lower())\n        ]\n\n        if len(matching_objects) == 1:\n            # Single match found, return the specific object\n            return matching_objects[0].get(\'type\', obj_type)\n        elif len(matching_objects) > 1:\n            # Multiple matches, need disambiguation\n            # Return original and let higher level handle disambiguation\n            pass\n\n        return obj_type\n'})}),"\n",(0,i.jsx)(e.h2,{id:"confidence-scoring-and-validation",children:"Confidence Scoring and Validation"}),"\n",(0,i.jsx)(e.h3,{id:"confidence-calculation",children:"Confidence Calculation"}),"\n",(0,i.jsx)(e.p,{children:"The system calculates confidence scores for parsed intents:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class IntentConfidenceCalculator:\n    def calculate_confidence(self, parsed_intent: ParsedIntent, text: str) -> float:\n        """\n        Calculate confidence score for the parsed intent\n        """\n        confidence_factors = {\n            \'pattern_match_strength\': self._evaluate_pattern_match(parsed_intent),\n            \'entity_resolution\': self._evaluate_entity_resolution(parsed_intent),\n            \'context_consistency\': self._evaluate_context_consistency(parsed_intent),\n            \'command_feasibility\': self._evaluate_command_feasibility(parsed_intent)\n        }\n\n        # Weighted average of confidence factors\n        weights = {\n            \'pattern_match_strength\': 0.4,\n            \'entity_resolution\': 0.3,\n            \'context_consistency\': 0.2,\n            \'command_feasibility\': 0.1\n        }\n\n        total_confidence = sum(\n            confidence_factors[key] * weights[key]\n            for key in confidence_factors\n        )\n\n        return min(total_confidence, 1.0)  # Cap at 1.0\n\n    def _evaluate_pattern_match(self, parsed_intent: ParsedIntent) -> float:\n        """\n        Evaluate confidence based on pattern matching strength\n        """\n        # Higher confidence for more specific pattern matches\n        if parsed_intent.parameters.destination and parsed_intent.parameters.object_type:\n            return 0.95\n        elif parsed_intent.parameters.destination or parsed_intent.parameters.object_type:\n            return 0.85\n        else:\n            return 0.70\n\n    def _evaluate_entity_resolution(self, parsed_intent: ParsedIntent) -> float:\n        """\n        Evaluate confidence based on entity resolution success\n        """\n        # Placeholder implementation\n        return 0.8 if parsed_intent.parameters.destination else 0.6\n\n    def _evaluate_context_consistency(self, parsed_intent: ParsedIntent) -> float:\n        """\n        Evaluate confidence based on consistency with environmental context\n        """\n        # Placeholder implementation\n        return 0.9 if parsed_intent.parameters.destination else 0.7\n\n    def _evaluate_command_feasibility(self, parsed_intent: ParsedIntent) -> float:\n        """\n        Evaluate confidence based on robot capability to execute command\n        """\n        # Placeholder implementation\n        return 0.85\n'})}),"\n",(0,i.jsx)(e.h2,{id:"error-handling-and-disambiguation",children:"Error Handling and Disambiguation"}),"\n",(0,i.jsx)(e.h3,{id:"ambiguity-resolution",children:"Ambiguity Resolution"}),"\n",(0,i.jsx)(e.p,{children:"The system handles ambiguous commands through disambiguation:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class IntentDisambiguator:\n    def __init__(self):\n        self.ambiguity_patterns = [\n            r"the (\\w+)",  # Ambiguous object references\n            r"it",         # Pronoun references\n            r"there",      # Vague location references\n            r"this"        # Vague object references\n        ]\n\n    def detect_ambiguity(self, text: str, parsed_intent: ParsedIntent) -> List[str]:\n        """\n        Detect potential ambiguities in the parsed intent\n        """\n        ambiguities = []\n\n        # Check for ambiguous pronouns\n        if "it" in text.lower() or "there" in text.lower():\n            ambiguities.append("pronoun_or_location_reference")\n\n        # Check for vague object references\n        if parsed_intent.parameters.object_type == "it" or parsed_intent.parameters.destination == "there":\n            ambiguities.append("vague_reference")\n\n        # Check for multiple possible interpretations\n        possible_interpretations = self._find_possible_interpretations(text)\n        if len(possible_interpretations) > 1:\n            ambiguities.append("multiple_interpretations")\n\n        return ambiguities\n\n    def _find_possible_interpretations(self, text: str) -> List[ParsedIntent]:\n        """\n        Find multiple possible interpretations of ambiguous text\n        """\n        # Implementation would use multiple parsing strategies\n        # to find different possible interpretations\n        return []\n\n    def generate_disambiguation_queries(self, text: str, ambiguities: List[str]) -> List[str]:\n        """\n        Generate queries to resolve ambiguities\n        """\n        queries = []\n\n        if "pronoun_or_location_reference" in ambiguities:\n            queries.append("Could you clarify what you mean by \'it\' or \'there\'?")\n\n        if "vague_reference" in ambiguities:\n            queries.append("Could you be more specific about the object or location?")\n\n        if "multiple_interpretations" in ambiguities:\n            queries.append(f"I heard \'{text}\'. Could you clarify what you mean?")\n\n        return queries\n'})}),"\n",(0,i.jsx)(e.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,i.jsx)(e.h3,{id:"message-definitions",children:"Message Definitions"}),"\n",(0,i.jsx)(e.p,{children:"Intent parsing results are communicated through ROS 2 messages:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Custom message: ParsedIntent.msg\nstring intent_type\nstring parameters_json  # Serialized parameters\nfloat32 confidence\nstring original_text\ntime timestamp\nstring[] detected_entities\n"})}),"\n",(0,i.jsx)(e.h3,{id:"publisher-example",children:"Publisher Example"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom vla_msgs.msg import ParsedIntent as ParsedIntentMsg\n\nclass IntentParserROS2Bridge(Node):\n    def __init__(self):\n        super().__init__('intent_parser_bridge')\n        self.publisher = self.create_publisher(\n            ParsedIntentMsg,\n            'intent_parser/result',\n            10\n        )\n        self.intent_parser = ContextAwareIntentParser()\n\n    def parse_and_publish(self, text: str):\n        \"\"\"\n        Parse text and publish result via ROS 2\n        \"\"\"\n        parsed_intent = self.intent_parser.parse_intent_with_context(text)\n\n        if parsed_intent:\n            msg = ParsedIntentMsg()\n            msg.intent_type = parsed_intent.intent_type.value\n            msg.parameters_json = self._serialize_parameters(parsed_intent.parameters)\n            msg.confidence = parsed_intent.confidence\n            msg.original_text = parsed_intent.original_text\n            msg.timestamp = self.get_clock().now().to_msg()\n\n            # Extract detected entities\n            entity_extractor = EntityExtractor()\n            entities = entity_extractor.extract_entities(text)\n            msg.detected_entities = [f\"{k}:{v}\" for k, v_list in entities.items() for v in v_list]\n\n            self.publisher.publish(msg)\n\n    def _serialize_parameters(self, params: IntentParameters) -> str:\n        \"\"\"\n        Serialize intent parameters to JSON\n        \"\"\"\n        import json\n        return json.dumps({\n            'destination': params.destination,\n            'direction': params.direction,\n            'distance': params.distance,\n            'object_type': params.object_type,\n            'object_color': params.object_color,\n            'object_description': params.object_description,\n            'target': params.target\n        })\n"})}),"\n",(0,i.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(e.h3,{id:"caching-strategies",children:"Caching Strategies"}),"\n",(0,i.jsx)(e.p,{children:"For improved performance, the system implements caching:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from functools import lru_cache\nimport hashlib\n\nclass CachedIntentParser:\n    def __init__(self, max_cache_size: int = 1000):\n        self.intent_parser = ContextAwareIntentParser()\n        self.cache = {}\n        self.max_cache_size = max_cache_size\n\n    @lru_cache(maxsize=1000)\n    def parse_cached(self, text: str, context_hash: str = "") -> Optional[ParsedIntent]:\n        """\n        Parse text with caching based on content\n        """\n        return self.intent_parser.parse_intent_with_context(text)\n\n    def get_context_hash(self, context: EnvironmentalContext) -> str:\n        """\n        Generate hash for environmental context\n        """\n        context_str = f"{context.known_locations}{context.visible_objects}{context.robot_capabilities}"\n        return hashlib.md5(context_str.encode()).hexdigest()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(e.h3,{id:"accuracy-improvements",children:"Accuracy Improvements"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Domain-Specific Training"}),": Fine-tune models for specific robot environments"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Context Integration"}),": Always consider environmental context when parsing"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multi-Modal Input"}),": Combine voice with visual context for better accuracy"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Continuous Learning"}),": Update models based on successful interactions"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"error-handling",children:"Error Handling"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Graceful Degradation"}),": Provide useful responses even with low confidence"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Disambiguation"}),": Prompt for clarification when uncertain"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Fallback Strategies"}),": Have backup plans for unrecognized commands"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"User Feedback"}),": Learn from user corrections to improve accuracy"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Real-Time Processing"}),": Optimize for real-time response"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Resource Efficiency"}),": Minimize computational requirements"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Memory Management"}),": Efficiently manage memory for continuous operation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Scalability"}),": Design to handle multiple concurrent requests"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(e.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Low Recognition Accuracy"}),": Improve training data or add more pattern templates"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Context Mismatch"}),": Ensure environmental context is properly updated"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Entity Resolution Failures"}),": Expand entity dictionaries or improve resolution logic"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Performance Degradation"}),": Optimize algorithms or implement caching"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"diagnostic-tools",children:"Diagnostic Tools"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"def diagnose_intent_parsing(text: str, expected_intent: str = None):\n    \"\"\"\n    Diagnose intent parsing performance\n    \"\"\"\n    parser = ContextAwareIntentParser()\n    result = parser.parse_intent_with_context(text)\n\n    diagnostics = {\n        'input_text': text,\n        'parsed_intent': result.intent_type.value if result else None,\n        'expected_intent': expected_intent,\n        'confidence': result.confidence if result else 0.0,\n        'entities_extracted': parser.entity_extractor.extract_entities(text) if result else {}\n    }\n\n    return diagnostics\n"})}),"\n",(0,i.jsx)(e.h2,{id:"future-enhancements",children:"Future Enhancements"}),"\n",(0,i.jsx)(e.h3,{id:"advanced-nlu-features",children:"Advanced NLU Features"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Dialogue Management"}),": Handle multi-turn conversations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Coreference Resolution"}),": Better handle pronouns and references"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Emotion Detection"}),": Detect emotional context in commands"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Adaptive Learning"}),": Personalize for individual users"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"integration-improvements",children:"Integration Improvements"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multi-Modal Understanding"}),": Combine speech with visual and sensor data"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Context Prediction"}),": Predict likely intents based on context"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Proactive Assistance"}),": Suggest actions based on observed patterns"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(e.p,{children:"Intent parsing is the crucial bridge between natural language input and actionable robot commands in the VLA system. By accurately extracting user intent and relevant parameters, the system enables intuitive human-robot interaction while maintaining robustness through context integration and error handling."}),"\n",(0,i.jsxs)(e.p,{children:["For implementation details, refer to the complete ",(0,i.jsx)(e.a,{href:"/docs/voice-to-action/",children:"Voice Command Processing"})," overview and the ",(0,i.jsx)(e.a,{href:"/docs/voice-to-action/command-translation",children:"Command Translation"})," documentation for the next step in the pipeline."]})]})}function p(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}}}]);