"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[4487],{2159(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var r=i(2540),t=i(3023);const o={title:"GPU Optimization Techniques",description:"Performance optimization techniques for Isaac ROS using GPU acceleration for humanoid robotics applications",sidebar_position:5,tags:["gpu","optimization","performance","isaac-ros","hardware-acceleration"]},s="GPU Optimization Techniques",a={id:"isaac-ros/gpu-optimization-techniques",title:"GPU Optimization Techniques",description:"Performance optimization techniques for Isaac ROS using GPU acceleration for humanoid robotics applications",source:"@site/docs/isaac-ros/gpu-optimization-techniques.md",sourceDirName:"isaac-ros",slug:"/isaac-ros/gpu-optimization-techniques",permalink:"/docs/isaac-ros/gpu-optimization-techniques",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/isaac-ros/gpu-optimization-techniques.md",tags:[{label:"gpu",permalink:"/docs/tags/gpu"},{label:"optimization",permalink:"/docs/tags/optimization"},{label:"performance",permalink:"/docs/tags/performance"},{label:"isaac-ros",permalink:"/docs/tags/isaac-ros"},{label:"hardware-acceleration",permalink:"/docs/tags/hardware-acceleration"}],version:"current",sidebarPosition:5,frontMatter:{title:"GPU Optimization Techniques",description:"Performance optimization techniques for Isaac ROS using GPU acceleration for humanoid robotics applications",sidebar_position:5,tags:["gpu","optimization","performance","isaac-ros","hardware-acceleration"]},sidebar:"tutorialSidebar",previous:{title:"Hardware-Accelerated Perception",permalink:"/docs/isaac-ros/hardware-accelerated-perception"},next:{title:"VSLAM Pipelines",permalink:"/docs/isaac-ros/vslam-pipelines"}},l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"GPU Architecture Overview",id:"gpu-architecture-overview",level:2},{value:"NVIDIA GPU Architecture for Robotics",id:"nvidia-gpu-architecture-for-robotics",level:3},{value:"Isaac ROS Hardware Requirements",id:"isaac-ros-hardware-requirements",level:3},{value:"Isaac ROS Optimization Strategies",id:"isaac-ros-optimization-strategies",level:2},{value:"Memory Management",id:"memory-management",level:3},{value:"GPU Memory Optimization",id:"gpu-memory-optimization",level:4},{value:"Memory Pool Management",id:"memory-pool-management",level:4},{value:"Kernel Optimization",id:"kernel-optimization",level:3},{value:"Optimized CUDA Kernels",id:"optimized-cuda-kernels",level:4},{value:"Pipeline Parallelization",id:"pipeline-parallelization",level:3},{value:"Stream-Based Processing",id:"stream-based-processing",level:4},{value:"Isaac ROS Package Optimization",id:"isaac-ros-package-optimization",level:2},{value:"Image Pipeline Optimization",id:"image-pipeline-optimization",level:3},{value:"Neural Network Optimization",id:"neural-network-optimization",level:3},{value:"Performance Monitoring and Profiling",id:"performance-monitoring-and-profiling",level:2},{value:"GPU Performance Monitoring",id:"gpu-performance-monitoring",level:3},{value:"Isaac ROS Performance Profiling",id:"isaac-ros-performance-profiling",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Memory Management Best Practices",id:"memory-management-best-practices",level:3},{value:"Kernel Optimization Best Practices",id:"kernel-optimization-best-practices",level:3},{value:"Pipeline Optimization Best Practices",id:"pipeline-optimization-best-practices",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Performance Issues",id:"common-performance-issues",level:3},{value:"Performance Monitoring Commands",id:"performance-monitoring-commands",level:3},{value:"References",id:"references",level:2}];function m(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"gpu-optimization-techniques",children:"GPU Optimization Techniques"}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"GPU optimization is crucial for Isaac ROS applications in humanoid robotics, where real-time perception and processing are essential. This document covers techniques for maximizing GPU performance and optimizing Isaac ROS pipelines for hardware acceleration."}),"\n",(0,r.jsx)(n.h2,{id:"gpu-architecture-overview",children:"GPU Architecture Overview"}),"\n",(0,r.jsx)(n.h3,{id:"nvidia-gpu-architecture-for-robotics",children:"NVIDIA GPU Architecture for Robotics"}),"\n",(0,r.jsx)(n.p,{children:"Understanding GPU architecture is key to optimization:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"CUDA cores"}),": Parallel processing units for general computation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tensor cores"}),": Specialized units for AI inference operations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RT cores"}),": Ray tracing cores for rendering applications"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory hierarchy"}),": Global, shared, and register memory organization"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"isaac-ros-hardware-requirements",children:"Isaac ROS Hardware Requirements"}),"\n",(0,r.jsx)(n.p,{children:"Isaac ROS is optimized for NVIDIA hardware:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Jetson platforms"}),": Optimized for edge robotics applications"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RTX GPUs"}),": High-performance desktop and workstation GPUs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"TensorRT integration"}),": Optimized neural network inference"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"CUDA compatibility"}),": Direct integration with CUDA libraries"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"isaac-ros-optimization-strategies",children:"Isaac ROS Optimization Strategies"}),"\n",(0,r.jsx)(n.h3,{id:"memory-management",children:"Memory Management"}),"\n",(0,r.jsx)(n.h4,{id:"gpu-memory-optimization",children:"GPU Memory Optimization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Example GPU memory optimization for Isaac ROS\nimport rclpy\nfrom rclpy.node import Node\nimport cuda\nimport numpy as np\n\nclass GPUOptimizedNode(Node):\n    def __init__(self):\n        super().__init__(\'gpu_optimized_node\')\n\n        # Pre-allocate GPU memory buffers\n        self.allocate_gpu_buffers()\n\n        # Configure memory pool\n        self.memory_pool = GPUMemoryPool(\n            initial_size=1024*1024*512,  # 512MB initial\n            max_size=1024*1024*2048     # 2GB max\n        )\n\n    def allocate_gpu_buffers(self):\n        """\n        Pre-allocate GPU memory buffers to avoid allocation overhead\n        """\n        # Allocate input buffer\n        self.input_buffer = cuda_malloc(1920 * 1080 * 3)  # 1080p RGB\n\n        # Allocate output buffer\n        self.output_buffer = cuda_malloc(1920 * 1080 * 1)  # Processed result\n\n        # Allocate temporary processing buffers\n        self.temp_buffers = []\n        for i in range(4):  # Multiple temp buffers\n            temp_buf = cuda_malloc(1920 * 1080 * 3)\n            self.temp_buffers.append(temp_buf)\n\n    def process_frame_optimized(self, input_data):\n        """\n        Process frame using pre-allocated GPU buffers\n        """\n        # Copy input to pre-allocated buffer\n        cuda_memcpy(self.input_buffer, input_data, input_data.size)\n\n        # Process using optimized kernel\n        result = self.run_optimized_kernel(\n            self.input_buffer,\n            self.output_buffer,\n            input_data.shape\n        )\n\n        return result\n'})}),"\n",(0,r.jsx)(n.h4,{id:"memory-pool-management",children:"Memory Pool Management"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# GPU memory pool for Isaac ROS applications\nclass GPUMemoryPool:\n    def __init__(self, initial_size, max_size):\n        self.initial_size = initial_size\n        self.max_size = max_size\n        self.allocated_size = 0\n        self.free_blocks = []\n        self.used_blocks = {}\n        self.lock = threading.Lock()\n\n        # Pre-allocate initial memory\n        self.initial_buffer = cuda_malloc(initial_size)\n\n    def allocate(self, size):\n        """\n        Allocate GPU memory from the pool\n        """\n        with self.lock:\n            # Check if we have a free block of appropriate size\n            for i, block in enumerate(self.free_blocks):\n                if block[\'size\'] >= size:\n                    # Use this block\n                    allocated_block = block\n                    del self.free_blocks[i]\n                    self.used_blocks[id(allocated_block[\'ptr\'])] = allocated_block\n                    return allocated_block[\'ptr\']\n\n            # No suitable free block, allocate new one if possible\n            if self.allocated_size + size <= self.max_size:\n                new_ptr = cuda_malloc(size)\n                new_block = {\'ptr\': new_ptr, \'size\': size}\n                self.used_blocks[id(new_ptr)] = new_block\n                self.allocated_size += size\n                return new_ptr\n            else:\n                raise MemoryError("GPU memory pool exhausted")\n\n    def free(self, ptr):\n        """\n        Return GPU memory to the pool\n        """\n        with self.lock:\n            if id(ptr) in self.used_blocks:\n                block = self.used_blocks.pop(id(ptr))\n                self.free_blocks.append(block)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"kernel-optimization",children:"Kernel Optimization"}),"\n",(0,r.jsx)(n.h4,{id:"optimized-cuda-kernels",children:"Optimized CUDA Kernels"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Example optimized CUDA kernel for Isaac ROS\nimport cupy as cp\nimport numpy as np\n\nclass OptimizedImageProcessor:\n    def __init__(self):\n        # Define optimized CUDA kernel\n        self.kernel_code = '''\n        extern \"C\" __global__\n        void optimized_image_filter(\n            const float* input,\n            float* output,\n            int width,\n            int height,\n            float* filter_kernel,\n            int kernel_size\n        ) {\n            int x = blockIdx.x * blockDim.x + threadIdx.x;\n            int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n            if (x < width && y < height) {\n                float sum = 0.0f;\n                int half_kernel = kernel_size / 2;\n\n                for (int ky = 0; ky < kernel_size; ky++) {\n                    for (int kx = 0; kx < kernel_size; kx++) {\n                        int px = x + kx - half_kernel;\n                        int py = y + ky - half_kernel;\n\n                        // Boundary handling\n                        px = max(0, min(width - 1, px));\n                        py = max(0, min(height - 1, py));\n\n                        int input_idx = py * width + px;\n                        int kernel_idx = ky * kernel_size + kx;\n\n                        sum += input[input_idx] * filter_kernel[kernel_idx];\n                    }\n                }\n\n                int output_idx = y * width + x;\n                output[output_idx] = sum;\n            }\n        }\n        '''\n\n        # Compile the kernel\n        self.module = cp.RawModule(code=self.kernel_code)\n        self.filter_kernel = self.module.get_function('optimized_image_filter')\n\n    def apply_filter(self, input_image, filter_kernel):\n        \"\"\"\n        Apply optimized filter to image using CUDA kernel\n        \"\"\"\n        # Transfer data to GPU\n        gpu_input = cp.asarray(input_image, dtype=cp.float32)\n        gpu_kernel = cp.asarray(filter_kernel, dtype=cp.float32)\n        gpu_output = cp.zeros_like(gpu_input)\n\n        # Define block and grid dimensions\n        block_size = (16, 16, 1)\n        grid_size = (\n            (input_image.shape[1] + block_size[0] - 1) // block_size[0],\n            (input_image.shape[0] + block_size[1] - 1) // block_size[1],\n            1\n        )\n\n        # Launch kernel\n        self.filter_kernel(\n            grid_size,\n            block_size,\n            (gpu_input, gpu_output, input_image.shape[1], input_image.shape[0],\n             gpu_kernel, filter_kernel.shape[0])\n        )\n\n        # Return result\n        return cp.asnumpy(gpu_output)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"pipeline-parallelization",children:"Pipeline Parallelization"}),"\n",(0,r.jsx)(n.h4,{id:"stream-based-processing",children:"Stream-Based Processing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Stream-based parallel processing for Isaac ROS\nimport cuda\nfrom concurrent.futures import ThreadPoolExecutor\nimport queue\n\nclass StreamProcessor:\n    def __init__(self, num_streams=4):\n        self.num_streams = num_streams\n        self.streams = []\n        self.event_pool = []\n\n        # Create CUDA streams\n        for i in range(num_streams):\n            stream = cuda.Stream()\n            event = cuda.Event()\n            self.streams.append(stream)\n            self.event_pool.append(event)\n\n        # Processing queue\n        self.process_queue = queue.Queue()\n        self.result_queue = queue.Queue()\n\n    def process_frame_async(self, frame_data, stream_id=None):\n        """\n        Process frame asynchronously using CUDA streams\n        """\n        if stream_id is None:\n            # Round-robin assignment\n            stream_id = len(self.process_queue) % self.num_streams\n\n        stream = self.streams[stream_id]\n\n        # Copy data to GPU asynchronously\n        gpu_frame = cuda.mem_alloc(frame_data.nbytes)\n        cuda.memcpy_htod_async(gpu_frame, frame_data, stream)\n\n        # Process frame (simplified)\n        result = self.process_on_gpu(gpu_frame, stream)\n\n        # Copy result back asynchronously\n        result_host = np.empty_like(frame_data)\n        cuda.memcpy_dtoh_async(result_host, result, stream)\n\n        # Record completion event\n        event = self.event_pool[stream_id]\n        event.record(stream)\n\n        return result_host, event\n\n    def process_on_gpu(self, gpu_data, stream):\n        """\n        Process data on GPU using the specified stream\n        """\n        # Placeholder for actual GPU processing\n        # This would call Isaac ROS GPU-accelerated functions\n        return gpu_data\n'})}),"\n",(0,r.jsx)(n.h2,{id:"isaac-ros-package-optimization",children:"Isaac ROS Package Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"image-pipeline-optimization",children:"Image Pipeline Optimization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Optimized Isaac ROS image pipeline\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cupy as cp\n\nclass OptimizedImagePipeline(Node):\n    def __init__(self):\n        super().__init__(\'optimized_image_pipeline\')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to image topic\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            1  # Minimal queue to reduce latency\n        )\n\n        # Pre-allocate GPU memory\n        self.gpu_buffer = None\n        self.max_width = 1920\n        self.max_height = 1080\n        self.allocate_gpu_memory()\n\n        # Processing pipeline\n        self.processing_pipeline = [\n            self.rectify_image,\n            self.apply_filters,\n            self.extract_features\n        ]\n\n    def allocate_gpu_memory(self):\n        """\n        Allocate GPU memory for image processing\n        """\n        # Allocate buffer for maximum expected image size\n        max_size = self.max_width * self.max_height * 3  # RGB\n        self.gpu_buffer = cp.zeros((self.max_height, self.max_width, 3), dtype=cp.uint8)\n\n    def image_callback(self, msg):\n        """\n        Process incoming image with GPU acceleration\n        """\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n            # Transfer to GPU\n            gpu_image = cp.asarray(cv_image)\n\n            # Process pipeline\n            for process_func in self.processing_pipeline:\n                gpu_image = process_func(gpu_image)\n\n            # Convert back to CPU if needed for further processing\n            result = cp.asnumpy(gpu_image)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in image processing: {e}\')\n\n    def rectify_image(self, gpu_image):\n        """\n        GPU-accelerated image rectification\n        """\n        # Placeholder for actual rectification using Isaac ROS GPU functions\n        # This would use Isaac ROS image rectification with GPU acceleration\n        return gpu_image\n\n    def apply_filters(self, gpu_image):\n        """\n        Apply GPU-accelerated filters\n        """\n        # Example: Gaussian blur using CuPy\n        from cupyx.scipy import ndimage\n        import cupyx.scipy.ndimage as ndi\n\n        # Apply Gaussian filter\n        filtered = ndi.gaussian_filter(gpu_image.astype(cp.float32), sigma=1.0)\n        return filtered.astype(gpu_image.dtype)\n\n    def extract_features(self, gpu_image):\n        """\n        GPU-accelerated feature extraction\n        """\n        # Placeholder for actual feature extraction using Isaac ROS\n        # This would use Isaac ROS feature extraction with GPU acceleration\n        return gpu_image\n'})}),"\n",(0,r.jsx)(n.h3,{id:"neural-network-optimization",children:"Neural Network Optimization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# TensorRT optimization for Isaac ROS\nimport tensorrt as trt\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nimport numpy as np\n\nclass TensorRTOptimizer:\n    def __init__(self):\n        self.logger = trt.Logger(trt.Logger.WARNING)\n        self.runtime = trt.Runtime(self.logger)\n\n    def optimize_model(self, onnx_model_path, precision=\'fp16\'):\n        """\n        Optimize ONNX model using TensorRT\n        """\n        # Create builder\n        builder = trt.Builder(self.logger)\n        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n        parser = trt.OnnxParser(network, self.logger)\n\n        # Parse ONNX model\n        with open(onnx_model_path, \'rb\') as model:\n            parser.parse(model.read())\n\n        # Configure builder\n        config = builder.create_builder_config()\n\n        # Set precision\n        if precision == \'fp16\':\n            if builder.platform_has_fast_fp16:\n                config.set_flag(trt.BuilderFlag.FP16)\n            else:\n                print("FP16 not supported on this platform")\n\n        # Optimize for specific batch size\n        profile = builder.create_optimization_profile()\n        # Set input shape (example: 3x224x224)\n        profile.set_shape("input", (1, 3, 224, 224), (4, 3, 224, 224), (8, 3, 224, 224))\n        config.add_optimization_profile(profile)\n\n        # Build engine\n        serialized_engine = builder.build_serialized_network(network, config)\n        engine = self.runtime.deserialize_cuda_engine(serialized_engine)\n\n        return engine\n\n    def run_inference(self, engine, input_data):\n        """\n        Run inference using optimized TensorRT engine\n        """\n        # Create execution context\n        context = engine.create_execution_context()\n\n        # Allocate I/O buffers\n        inputs, outputs, bindings, stream = self.allocate_buffers(engine)\n\n        # Copy input to GPU\n        cuda.memcpy_htod(inputs[0].device_input, input_data)\n\n        # Run inference\n        context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n\n        # Copy output from GPU\n        cuda.memcpy_dtoh(outputs[0].host_output, outputs[0].device_output)\n\n        return outputs[0].host_output\n\n    def allocate_buffers(self, engine):\n        """\n        Allocate input/output buffers for TensorRT inference\n        """\n        inputs = []\n        outputs = []\n        bindings = []\n        stream = cuda.Stream()\n\n        for binding in engine:\n            size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n            dtype = trt.nptype(engine.get_binding_dtype(binding))\n            host_mem = cuda.pagelocked_empty(size, dtype)\n            device_mem = cuda.mem_alloc(host_mem.nbytes)\n\n            bindings.append(int(device_mem))\n            if engine.binding_is_input(binding):\n                inputs.append(HostDeviceMem(host_mem, device_mem))\n            else:\n                outputs.append(HostDeviceMem(host_mem, device_mem))\n\n        return inputs, outputs, bindings, stream\n\nclass HostDeviceMem:\n    def __init__(self, host_mem, device_mem):\n        self.host = host_mem\n        self.device = device_mem\n\n    def __str__(self):\n        return f"Host:\\n{self.host}\\nDevice:\\n{self.device}"\n\n    def __repr__(self):\n        return self.__str__()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"performance-monitoring-and-profiling",children:"Performance Monitoring and Profiling"}),"\n",(0,r.jsx)(n.h3,{id:"gpu-performance-monitoring",children:"GPU Performance Monitoring"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# GPU performance monitoring for Isaac ROS\nimport pynvml\nimport time\nimport threading\n\nclass GPUPerformanceMonitor:\n    def __init__(self):\n        pynvml.nvmlInit()\n        self.device_count = pynvml.nvmlDeviceGetCount()\n        self.devices = []\n\n        for i in range(self.device_count):\n            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n            self.devices.append(handle)\n\n        self.monitoring = False\n        self.metrics = {}\n\n    def start_monitoring(self):\n        """\n        Start GPU performance monitoring\n        """\n        self.monitoring = True\n        self.monitoring_thread = threading.Thread(target=self._monitor_loop)\n        self.monitoring_thread.start()\n\n    def stop_monitoring(self):\n        """\n        Stop GPU performance monitoring\n        """\n        self.monitoring = False\n        if hasattr(self, \'monitoring_thread\'):\n            self.monitoring_thread.join()\n\n    def _monitor_loop(self):\n        """\n        Monitoring loop that periodically collects GPU metrics\n        """\n        while self.monitoring:\n            for i, device in enumerate(self.devices):\n                # Get GPU utilization\n                utilization = pynvml.nvmlDeviceGetUtilizationRates(device)\n\n                # Get memory info\n                memory_info = pynvml.nvmlDeviceGetMemoryInfo(device)\n\n                # Get temperature\n                temperature = pynvml.nvmlDeviceGetTemperature(device, pynvml.NVML_TEMPERATURE_GPU)\n\n                # Store metrics\n                self.metrics[f\'gpu_{i}\'] = {\n                    \'utilization\': utilization.gpu,\n                    \'memory_used\': memory_info.used,\n                    \'memory_total\': memory_info.total,\n                    \'memory_utilization\': (memory_info.used / memory_info.total) * 100,\n                    \'temperature\': temperature\n                }\n\n            time.sleep(1)  # Update every second\n\n    def get_current_metrics(self):\n        """\n        Get current GPU performance metrics\n        """\n        return self.metrics.copy()\n\n    def log_performance(self, node_name):\n        """\n        Log performance metrics for a specific node\n        """\n        metrics = self.get_current_metrics()\n\n        for gpu_id, gpu_metrics in metrics.items():\n            self.get_logger().info(\n                f"Node {node_name} - {gpu_id}: "\n                f"Util: {gpu_metrics[\'utilization\']}%, "\n                f"Mem: {gpu_metrics[\'memory_utilization\']:.1f}%, "\n                f"Temp: {gpu_metrics[\'temperature\']}\xb0C"\n            )\n'})}),"\n",(0,r.jsx)(n.h3,{id:"isaac-ros-performance-profiling",children:"Isaac ROS Performance Profiling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Isaac ROS performance profiler\nimport time\nfrom functools import wraps\nimport psutil\n\nclass IsaacROSProfiler:\n    def __init__(self):\n        self.profiles = {}\n        self.enabled = True\n\n    def profile_function(self, name=None):\n        \"\"\"\n        Decorator to profile a function\n        \"\"\"\n        def decorator(func):\n            @wraps(func)\n            def wrapper(*args, **kwargs):\n                if not self.enabled:\n                    return func(*args, **kwargs)\n\n                start_time = time.perf_counter()\n                start_cpu = psutil.cpu_percent()\n                start_memory = psutil.virtual_memory().used\n\n                try:\n                    result = func(*args, **kwargs)\n                except Exception as e:\n                    # Record error but still capture metrics\n                    result = None\n                    raise e\n                finally:\n                    end_time = time.perf_counter()\n                    end_cpu = psutil.cpu_percent()\n                    end_memory = psutil.virtual_memory().used\n\n                    func_name = name or func.__name__\n                    duration = end_time - start_time\n                    cpu_used = end_cpu - start_cpu\n                    memory_used = end_memory - start_memory\n\n                    # Store profiling data\n                    if func_name not in self.profiles:\n                        self.profiles[func_name] = {\n                            'call_count': 0,\n                            'total_time': 0,\n                            'avg_time': 0,\n                            'min_time': float('inf'),\n                            'max_time': 0,\n                            'total_memory': 0,\n                            'avg_memory': 0\n                        }\n\n                    profile = self.profiles[func_name]\n                    profile['call_count'] += 1\n                    profile['total_time'] += duration\n                    profile['avg_time'] = profile['total_time'] / profile['call_count']\n                    profile['min_time'] = min(profile['min_time'], duration)\n                    profile['max_time'] = max(profile['max_time'], duration)\n                    profile['total_memory'] += memory_used\n                    profile['avg_memory'] = profile['total_memory'] / profile['call_count']\n\n                return result\n            return wrapper\n        return decorator\n\n    def get_profile_report(self):\n        \"\"\"\n        Generate profiling report\n        \"\"\"\n        report = []\n        report.append(\"Isaac ROS Performance Profile Report\")\n        report.append(\"=\" * 50)\n\n        for func_name, metrics in self.profiles.items():\n            report.append(f\"\\nFunction: {func_name}\")\n            report.append(f\"  Calls: {metrics['call_count']}\")\n            report.append(f\"  Total Time: {metrics['total_time']:.4f}s\")\n            report.append(f\"  Avg Time: {metrics['avg_time']:.4f}s\")\n            report.append(f\"  Min Time: {metrics['min_time']:.4f}s\")\n            report.append(f\"  Max Time: {metrics['max_time']:.4f}s\")\n            report.append(f\"  Avg Memory: {metrics['avg_memory'] / (1024**2):.2f} MB\")\n\n        return \"\\n\".join(report)\n\n    def print_profile_report(self):\n        \"\"\"\n        Print profiling report to console\n        \"\"\"\n        print(self.get_profile_report())\n"})}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsx)(n.h3,{id:"memory-management-best-practices",children:"Memory Management Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pre-allocation"}),": Pre-allocate GPU memory to avoid allocation overhead"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory pools"}),": Use memory pools for frequently allocated/deallocated memory"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unified memory"}),": Use CUDA unified memory for CPU-GPU data sharing when appropriate"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory coalescing"}),": Ensure memory accesses are coalesced for optimal bandwidth"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"kernel-optimization-best-practices",children:"Kernel Optimization Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Thread block size"}),": Use appropriate thread block sizes (typically 128, 256, or 512 threads)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Shared memory"}),": Use shared memory to reduce global memory accesses"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Occupancy"}),": Maximize occupancy by ensuring sufficient threads per multiprocessor"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory access patterns"}),": Use coalesced memory access patterns"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"pipeline-optimization-best-practices",children:"Pipeline Optimization Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stream parallelism"}),": Use CUDA streams to overlap computation and memory transfers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Async operations"}),": Use asynchronous operations where possible"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Batch processing"}),": Process data in batches to maximize GPU utilization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pipeline stages"}),": Overlap different stages of processing pipeline"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsx)(n.h3,{id:"common-performance-issues",children:"Common Performance Issues"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory bottlenecks"}),": Monitor memory bandwidth and optimize access patterns"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kernel launch overhead"}),": Batch kernel launches when possible"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GPU underutilization"}),": Ensure sufficient parallelism in kernels"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory fragmentation"}),": Use memory pools to avoid fragmentation"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"performance-monitoring-commands",children:"Performance Monitoring Commands"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Monitor GPU utilization\nnvidia-smi -l 1\n\n# Monitor detailed GPU metrics\nnvidia-ml-py3 # Python bindings for GPU monitoring\n\n# Profile Isaac ROS nodes\nros2 run isaac_ros_test performance_monitor\n\n# Monitor system resources\nhtop\niotop\n"})}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.p,{children:["For more detailed information about GPU optimization techniques for Isaac ROS, refer to the ",(0,r.jsx)(n.a,{href:"https://isaac-ros.github.io/",children:"official Isaac ROS documentation"}),", the ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/cuda/",children:"NVIDIA CUDA documentation"}),", and the ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/deeplearning/tensorrt/",children:"TensorRT documentation"}),"."]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}},3023(e,n,i){i.d(n,{R:()=>s,x:()=>a});var r=i(3696);const t={},o=r.createContext(t);function s(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);