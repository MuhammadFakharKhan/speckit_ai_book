"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[1468],{1335(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>s,metadata:()=>r,toc:()=>a});var o=i(2540),t=i(3023);const s={title:"Complete Example Workflow",sidebar_label:"Complete Workflow",description:"Complete example workflow demonstrating the full VLA pipeline from voice command to robotic action execution"},l="Complete Example Workflow",r={id:"capstone-system/complete-workflow",title:"Complete Example Workflow",description:"Complete example workflow demonstrating the full VLA pipeline from voice command to robotic action execution",source:"@site/docs/capstone-system/complete-workflow.md",sourceDirName:"capstone-system",slug:"/capstone-system/complete-workflow",permalink:"/docs/capstone-system/complete-workflow",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/capstone-system/complete-workflow.md",tags:[],version:"current",frontMatter:{title:"Complete Example Workflow",sidebar_label:"Complete Workflow",description:"Complete example workflow demonstrating the full VLA pipeline from voice command to robotic action execution"},sidebar:"tutorialSidebar",previous:{title:"Simulation Setup",permalink:"/docs/capstone-system/simulation-setup"}},c={},a=[{value:"Overview",id:"overview",level:2},{value:"Complete Workflow Example",id:"complete-workflow-example",level:2},{value:"Scenario: Fetch Object Task",id:"scenario-fetch-object-task",level:3},{value:"Step 1: Voice Recognition and Processing",id:"step-1-voice-recognition-and-processing",level:4},{value:"Step 2: Cognitive Planning",id:"step-2-cognitive-planning",level:4},{value:"Step 3: ROS 2 Action Execution",id:"step-3-ros-2-action-execution",level:4},{value:"Step 4: Execution Monitoring",id:"step-4-execution-monitoring",level:4},{value:"Complex Multi-Step Example",id:"complex-multi-step-example",level:2},{value:"Scenario: Inspection Task",id:"scenario-inspection-task",level:3},{value:"Complete Pipeline Execution:",id:"complete-pipeline-execution",level:4},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:2},{value:"Voice Recognition Errors",id:"voice-recognition-errors",level:3},{value:"Planning Errors",id:"planning-errors",level:3},{value:"Execution Errors",id:"execution-errors",level:3},{value:"Validation and Testing",id:"validation-and-testing",level:2},{value:"Simulation Testing",id:"simulation-testing",level:3},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Integration with Isaac Ecosystem",id:"integration-with-isaac-ecosystem",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"complete-example-workflow",children:"Complete Example Workflow"}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"This document provides a comprehensive example of the complete Vision-Language-Action (VLA) pipeline, demonstrating how voice commands are processed through cognitive planning to execute robotic actions in simulated environments."}),"\n",(0,o.jsx)(n.h2,{id:"complete-workflow-example",children:"Complete Workflow Example"}),"\n",(0,o.jsx)(n.h3,{id:"scenario-fetch-object-task",children:"Scenario: Fetch Object Task"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command"}),': "Please go to the living room and bring me the blue book from the shelf."']}),"\n",(0,o.jsx)(n.h4,{id:"step-1-voice-recognition-and-processing",children:"Step 1: Voice Recognition and Processing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Input: "Please go to the living room and bring me the blue book from the shelf."\n\u2192 Whisper Processing\n\u2192 Intent: Navigation + Object Detection + Manipulation\n\u2192 Entities: Location=living room, Object=blue book, Action=fetch\n\u2192 Confidence Score: 0.85\n'})}),"\n",(0,o.jsx)(n.h4,{id:"step-2-cognitive-planning",children:"Step 2: Cognitive Planning"}),"\n",(0,o.jsx)(n.p,{children:"The LLM processes the command and generates an action sequence:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"1. Navigate to living room\n   - Identify living room location in map\n   - Plan path to destination\n   - Execute navigation\n\n2. Locate blue book\n   - Identify shelf location\n   - Use perception system to detect blue objects\n   - Verify book characteristics\n\n3. Grasp blue book\n   - Position manipulator appropriately\n   - Execute grasping motion\n   - Verify successful grasp\n\n4. Return to origin\n   - Plan return path\n   - Execute navigation with object\n   - Deliver object to user\n"})}),"\n",(0,o.jsx)(n.h4,{id:"step-3-ros-2-action-execution",children:"Step 3: ROS 2 Action Execution"}),"\n",(0,o.jsx)(n.p,{children:"Each action is converted to ROS 2 messages and executed:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Navigation action\nnavigation_goal = NavigationGoal()\nnavigation_goal.target_pose = living_room_location\nnavigation_client.send_goal(navigation_goal)\n\n# Object detection action\ndetection_request = DetectionRequest()\ndetection_request.object_class = "book"\ndetection_request.color = "blue"\ndetection_client.send_request(detection_request)\n\n# Manipulation action\nmanipulation_goal = ManipulationGoal()\nmanipulation_goal.action = "grasp"\nmanipulation_goal.object_pose = detected_book_pose\nmanipulation_client.send_goal(manipulation_goal)\n'})}),"\n",(0,o.jsx)(n.h4,{id:"step-4-execution-monitoring",children:"Step 4: Execution Monitoring"}),"\n",(0,o.jsx)(n.p,{children:"The system monitors execution and handles potential issues:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation success"}),": Proceed to object detection"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object not found"}),": Search alternative locations"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Grasp failure"}),": Adjust grasp strategy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Path blocked"}),": Recalculate route"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"complex-multi-step-example",children:"Complex Multi-Step Example"}),"\n",(0,o.jsx)(n.h3,{id:"scenario-inspection-task",children:"Scenario: Inspection Task"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command"}),': "Check if the window in the bedroom is closed, and if it\'s open, please close it and report back."']}),"\n",(0,o.jsx)(n.h4,{id:"complete-pipeline-execution",children:"Complete Pipeline Execution:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Voice Processing"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Intent: Inspection + Conditional Action + Reporting\nEntities: Location=bedroom, Object=window, Action=close, Condition=is open\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Cognitive Planning"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Navigate to bedroom"}),"\n",(0,o.jsx)(n.li,{children:"Inspect window state"}),"\n",(0,o.jsx)(n.li,{children:"Conditionally execute closure"}),"\n",(0,o.jsx)(n.li,{children:"Report results"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Execution Sequence"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'- action: navigate\n  target: bedroom\n- action: inspect\n  target: window\n  property: open_state\n- action: conditional\n  condition: window_open == true\n  then:\n    - action: manipulate\n      target: window\n      operation: close\n- action: report\n  message: "Window in bedroom was {{window_state}} and is now closed"\n'})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,o.jsx)(n.h3,{id:"voice-recognition-errors",children:"Voice Recognition Errors"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Low confidence"}),": Request repetition or clarification"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Ambiguous command"}),": Ask for clarification"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Background noise"}),": Retry with noise reduction"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"planning-errors",children:"Planning Errors"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Infeasible task"}),": Break down into simpler steps"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Missing context"}),": Request additional information"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Planning timeout"}),": Execute partial plan with safety measures"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"execution-errors",children:"Execution Errors"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation failure"}),": Recalculate path or abort"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object detection failure"}),": Expand search area"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation failure"}),": Try alternative approach"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"validation-and-testing",children:"Validation and Testing"}),"\n",(0,o.jsx)(n.h3,{id:"simulation-testing",children:"Simulation Testing"}),"\n",(0,o.jsx)(n.p,{children:"All workflows are validated in simulation before real-world deployment:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Run complete workflow in simulation\nros2 launch vla_pipeline complete_demo.launch.py\n\n# Monitor execution\nros2 bag record /vla_pipeline/status /vla_pipeline/debug\n\n# Validate results\npython3 validate_workflow.py --scenario fetch_object\n"})}),"\n",(0,o.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Success rate"}),": Percentage of tasks completed successfully"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Execution time"}),": Total time from command to completion"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Accuracy"}),": Correctness of action execution"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness"}),": Handling of edge cases and errors"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-isaac-ecosystem",children:"Integration with Isaac Ecosystem"}),"\n",(0,o.jsx)(n.p,{children:"The complete workflow integrates seamlessly with the Isaac ecosystem:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Isaac Sim"}),": Provides simulation environment for testing"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Isaac ROS"}),": Handles perception and sensor processing"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Nav2"}),": Manages navigation and path planning for humanoid robots"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This complete example demonstrates the end-to-end capability of the VLA pipeline, from natural language understanding to robotic action execution."})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},3023(e,n,i){i.d(n,{R:()=>l,x:()=>r});var o=i(3696);const t={},s=o.createContext(t);function l(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);