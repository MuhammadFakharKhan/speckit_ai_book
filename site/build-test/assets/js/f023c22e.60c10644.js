"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[1591],{2801(n,e,i){i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>t,default:()=>m,frontMatter:()=>s,metadata:()=>l,toc:()=>c});var o=i(2540),a=i(3023);const s={title:"Localization for Humanoid Robots",description:"Localization techniques adapted for bipedal robots using Nav2, considering sensor fusion and Z-axis movement",sidebar_position:3,tags:["localization","navigation","nav2","humanoid","sensor-fusion"]},t="Localization for Humanoid Robots",l={id:"nav2-humanoid/localization",title:"Localization for Humanoid Robots",description:"Localization techniques adapted for bipedal robots using Nav2, considering sensor fusion and Z-axis movement",source:"@site/docs/nav2-humanoid/localization.md",sourceDirName:"nav2-humanoid",slug:"/nav2-humanoid/localization",permalink:"/docs/nav2-humanoid/localization",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/nav2-humanoid/localization.md",tags:[{label:"localization",permalink:"/docs/tags/localization"},{label:"navigation",permalink:"/docs/tags/navigation"},{label:"nav2",permalink:"/docs/tags/nav-2"},{label:"humanoid",permalink:"/docs/tags/humanoid"},{label:"sensor-fusion",permalink:"/docs/tags/sensor-fusion"}],version:"current",sidebarPosition:3,frontMatter:{title:"Localization for Humanoid Robots",description:"Localization techniques adapted for bipedal robots using Nav2, considering sensor fusion and Z-axis movement",sidebar_position:3,tags:["localization","navigation","nav2","humanoid","sensor-fusion"]},sidebar:"tutorialSidebar",previous:{title:"Path Planning for Bipedal Robots",permalink:"/docs/nav2-humanoid/path-planning"},next:{title:"Bipedal Navigation Concepts",permalink:"/docs/nav2-humanoid/bipedal-navigation"}},r={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Humanoid-Specific Localization Challenges",id:"humanoid-specific-localization-challenges",level:2},{value:"Sensor Placement and Movement",id:"sensor-placement-and-movement",level:3},{value:"3D Localization Requirements",id:"3d-localization-requirements",level:3},{value:"Motion Model Complexity",id:"motion-model-complexity",level:3},{value:"AMCL Adaptations for Humanoid Robots",id:"amcl-adaptations-for-humanoid-robots",level:2},{value:"AMCL Configuration for Humanoid Robots",id:"amcl-configuration-for-humanoid-robots",level:3},{value:"Custom Motion Model",id:"custom-motion-model",level:3},{value:"Sensor Fusion for Humanoid Localization",id:"sensor-fusion-for-humanoid-localization",level:2},{value:"Multi-Sensor Integration",id:"multi-sensor-integration",level:3},{value:"3D Sensor Processing",id:"3d-sensor-processing",level:3},{value:"Head-Mounted Sensor Compensation",id:"head-mounted-sensor-compensation",level:3},{value:"Integration with VSLAM",id:"integration-with-vslam",level:2},{value:"Combining AMCL and VSLAM",id:"combining-amcl-and-vslam",level:3},{value:"Localization in Dynamic Environments",id:"localization-in-dynamic-environments",level:2},{value:"Handling Dynamic Elements",id:"handling-dynamic-elements",level:3},{value:"Multi-Hypothesis Tracking",id:"multi-hypothesis-tracking",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Computational Efficiency",id:"computational-efficiency",level:3},{value:"Memory Management",id:"memory-management",level:3},{value:"Troubleshooting and Debugging",id:"troubleshooting-and-debugging",level:2},{value:"Common Localization Issues",id:"common-localization-issues",level:3},{value:"Debugging Strategies",id:"debugging-strategies",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Configuration Guidelines",id:"configuration-guidelines",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Integration with Navigation",id:"integration-with-navigation",level:2},{value:"Localization-Navigation Interface",id:"localization-navigation-interface",level:3},{value:"References",id:"references",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h1,{id:"localization-for-humanoid-robots",children:"Localization for Humanoid Robots"}),"\n",(0,o.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(e.p,{children:"Localization for humanoid robots presents unique challenges compared to traditional wheeled robots. Bipedal locomotion, head-mounted sensors, and three-dimensional movement patterns require specialized approaches to accurately estimate the robot's pose in the environment. This module covers how to adapt Nav2's localization capabilities for humanoid robots."}),"\n",(0,o.jsx)(e.h2,{id:"humanoid-specific-localization-challenges",children:"Humanoid-Specific Localization Challenges"}),"\n",(0,o.jsx)(e.h3,{id:"sensor-placement-and-movement",children:"Sensor Placement and Movement"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots have unique sensor characteristics:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Head-mounted sensors"}),": Cameras and IMUs mounted on moving head"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Dynamic sensor orientation"}),": Head movement affects sensor readings"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Variable height"}),": Robot height changes during locomotion"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Body occlusion"}),": Robot body may occlude sensors during movement"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"3d-localization-requirements",children:"3D Localization Requirements"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots require 3D localization:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Z-axis tracking"}),": Track changes in elevation and height"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Roll and pitch"}),": Account for balance-related tilting"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Stair navigation"}),": Handle discrete elevation changes"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Ramp navigation"}),": Navigate and localize on inclined surfaces"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"motion-model-complexity",children:"Motion Model Complexity"}),"\n",(0,o.jsx)(e.p,{children:"Bipedal motion models are more complex:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Non-holonomic constraints"}),": Limited motion compared to wheeled robots"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Dynamic balance"}),": Motion affects and is affected by balance"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Discrete step motion"}),": Movement occurs in discrete steps"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Periodic motion patterns"}),": Walking creates periodic motion signatures"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"amcl-adaptations-for-humanoid-robots",children:"AMCL Adaptations for Humanoid Robots"}),"\n",(0,o.jsx)(e.h3,{id:"amcl-configuration-for-humanoid-robots",children:"AMCL Configuration for Humanoid Robots"}),"\n",(0,o.jsx)(e.p,{children:"Adapt AMCL (Adaptive Monte Carlo Localization) for humanoid requirements:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-yaml",children:'# Example AMCL configuration for humanoid robot\namcl:\n  ros__parameters:\n    use_sim_time: False\n    alpha1: 0.2\n    alpha2: 0.2\n    alpha3: 0.2\n    alpha4: 0.2\n    alpha5: 0.2\n    base_frame_id: "base_link"\n    beam_skip_distance: 0.5\n    beam_skip_error_threshold: 0.9\n    beam_skip_threshold: 0.3\n    do_beamskip: false\n    global_frame_id: "map"\n    lambda_short: 0.1\n    laser_likelihood_max_dist: 2.0\n    laser_max_range: 100.0\n    laser_min_range: -1.0\n    laser_model_type: "likelihood_field"\n    max_beams: 60\n    max_particles: 2000\n    min_particles: 500\n    odom_frame_id: "odom"\n    pf_err: 0.05\n    pf_z: 0.99\n    recovery_alpha_fast: 0.0\n    recovery_alpha_slow: 0.0\n    resample_interval: 1\n    robot_model_type: "nav2_amcl::HumanoidMotionModel"  # Custom motion model\n    save_pose_rate: 0.5\n    sigma_hit: 0.2\n    tf_broadcast: true\n    transform_tolerance: 1.0\n    update_min_a: 0.2\n    update_min_d: 0.1\n    z_hit: 0.5\n    z_max: 0.05\n    z_rand: 0.5\n    z_short: 0.05\n\n    # Humanoid-specific parameters\n    max_step_height: 0.2\n    head_movement_compensation: true\n    z_axis_localization: true\n    balance_uncertainty_increase: 0.1\n'})}),"\n",(0,o.jsx)(e.h3,{id:"custom-motion-model",children:"Custom Motion Model"}),"\n",(0,o.jsx)(e.p,{children:"Create a custom motion model for humanoid robots:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Example custom motion model for humanoid robots\nimport numpy as np\nfrom nav2_amcl.motion_model import MotionModel\nfrom geometry_msgs.msg import Pose\n\nclass HumanoidMotionModel(MotionModel):\n    def __init__(self, motion_params):\n        super().__init__()\n        self.max_step_size = motion_params.get(\'max_step_size\', 0.4)\n        self.balance_noise_factor = motion_params.get(\'balance_noise_factor\', 0.1)\n        self.step_timing_uncertainty = motion_params.get(\'step_timing_uncertainty\', 0.05)\n        self.head_movement_factor = motion_params.get(\'head_movement_factor\', 0.05)\n\n    def odometry_update(self, pose, delta):\n        """\n        Update particle poses based on odometry for humanoid robot\n        """\n        # Extract humanoid-specific motion parameters\n        step_distance = np.sqrt(delta.dx**2 + delta.dy**2)\n\n        # Check if step is within humanoid capabilities\n        if step_distance > self.max_step_size:\n            # Likely incorrect odometry, increase uncertainty\n            return self.penalize_particles()\n\n        # Apply motion with humanoid-specific noise model\n        for particle in self.particles:\n            # Add process noise based on humanoid motion characteristics\n            noise_x = np.random.normal(0, abs(delta.dx) * self.balance_noise_factor)\n            noise_y = np.random.normal(0, abs(delta.dy) * self.balance_noise_factor)\n            noise_yaw = np.random.normal(0, abs(delta.dr) * self.balance_noise_factor)\n\n            # Add step timing uncertainty\n            timing_noise = np.random.normal(0, self.step_timing_uncertainty)\n\n            # Update particle pose\n            particle.pose.x += delta.dx + noise_x + timing_noise\n            particle.pose.y += delta.dy + noise_y + timing_noise\n            particle.pose.yaw += delta.dr + noise_yaw\n\n    def penalize_particles(self):\n        """\n        Penalize particles when motion seems inconsistent with humanoid capabilities\n        """\n        for particle in self.particles:\n            # Increase uncertainty for particles that don\'t match humanoid motion model\n            particle.weight *= 0.1  # Reduce weight significantly\n\n    def get_prediction(self, control_input, dt):\n        """\n        Predict pose based on control input and humanoid motion constraints\n        """\n        # Implement humanoid-specific motion prediction\n        # This would include step-based motion and balance constraints\n        pass\n'})}),"\n",(0,o.jsx)(e.h2,{id:"sensor-fusion-for-humanoid-localization",children:"Sensor Fusion for Humanoid Localization"}),"\n",(0,o.jsx)(e.h3,{id:"multi-sensor-integration",children:"Multi-Sensor Integration"}),"\n",(0,o.jsx)(e.p,{children:"Combine multiple sensors for robust humanoid localization:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"LiDAR"}),": For 2D/3D environment mapping"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Cameras"}),": For visual features and landmarks"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"IMU"}),": For orientation and motion detection"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Encoders"}),": For step counting and odometry"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Force/Torque sensors"}),": For contact detection"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"3d-sensor-processing",children:"3D Sensor Processing"}),"\n",(0,o.jsx)(e.p,{children:"Handle 3D sensors for humanoid localization:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Example 3D sensor processing for humanoid localization\nimport numpy as np\nfrom sensor_msgs.msg import PointCloud2\nfrom geometry_msgs.msg import Pose\nfrom tf2_ros import TransformListener\n\nclass Humanoid3DSensorProcessor:\n    def __init__(self):\n        self.tf_buffer = tf2_ros.Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer)\n        self.elevation_threshold = 0.1  # meters\n        self.ground_plane_estimator = GroundPlaneEstimator()\n\n    def process_pointcloud(self, pointcloud_msg):\n        # Extract ground plane from point cloud\n        ground_points = self.extract_ground_points(pointcloud_msg)\n\n        # Estimate current elevation\n        current_elevation = self.estimate_elevation(ground_points)\n\n        # Update localization with elevation information\n        self.update_localization_elevation(current_elevation)\n\n        # Process elevated obstacles\n        elevated_objects = self.extract_elevated_objects(pointcloud_msg)\n        self.update_localization_with_obstacles(elevated_objects)\n\n    def extract_ground_points(self, pointcloud_msg):\n        # Extract ground plane points from point cloud\n        # This is crucial for humanoid navigation as it helps determine step height\n        points = self.pointcloud_to_array(pointcloud_msg)\n\n        # Estimate ground plane\n        ground_plane = self.ground_plane_estimator.estimate(points)\n\n        # Extract points near ground plane\n        ground_points = points[np.abs(self.distance_to_plane(points, ground_plane)) < self.elevation_threshold]\n\n        return ground_points\n\n    def estimate_elevation(self, ground_points):\n        # Estimate robot elevation based on ground plane\n        if len(ground_points) > 0:\n            # Average Z value of ground points gives elevation\n            return np.mean(ground_points[:, 2])\n        return 0.0\n"})}),"\n",(0,o.jsx)(e.h3,{id:"head-mounted-sensor-compensation",children:"Head-Mounted Sensor Compensation"}),"\n",(0,o.jsx)(e.p,{children:"Compensate for head-mounted sensor movement:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Example head-mounted sensor compensation\nclass HeadSensorCompensator:\n    def __init__(self):\n        self.head_pose_sub = rospy.Subscriber('/head_pose', Pose, self.head_pose_callback)\n        self.compensated_sensor_data = None\n        self.head_offset = np.array([0.0, 0.0, 1.5])  # Typical head offset from base\n\n    def head_pose_callback(self, head_pose):\n        # Store current head pose for compensation\n        self.current_head_pose = head_pose\n\n    def compensate_sensor_data(self, raw_sensor_data, sensor_frame):\n        # Transform sensor data from head frame to base frame\n        try:\n            # Get transform from head to base\n            transform = self.tf_buffer.lookup_transform(\n                'base_link', sensor_frame, rospy.Time(0)\n            )\n\n            # Apply transform to sensor data\n            compensated_data = self.apply_transform(raw_sensor_data, transform)\n            return compensated_data\n        except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):\n            # If transform not available, return raw data with warning\n            rospy.logwarn(\"Could not transform sensor data, using raw values\")\n            return raw_sensor_data\n"})}),"\n",(0,o.jsx)(e.h2,{id:"integration-with-vslam",children:"Integration with VSLAM"}),"\n",(0,o.jsx)(e.h3,{id:"combining-amcl-and-vslam",children:"Combining AMCL and VSLAM"}),"\n",(0,o.jsx)(e.p,{children:"Combine map-based localization with visual SLAM:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Example integration of AMCL and VSLAM for humanoid robot\nclass HybridLocalizationManager:\n    def __init__(self):\n        # Initialize AMCL for map-based localization\n        self.amcl_localizer = AMCLLocalizer()\n\n        # Initialize VSLAM for visual localization\n        self.vslam_localizer = VSLAMLocalizer()\n\n        # Initialize sensor fusion\n        self.fusion_filter = ExtendedKalmanFilter()\n\n        # Confidence thresholds\n        self.amcl_confidence_threshold = 0.7\n        self.vslam_confidence_threshold = 0.6\n\n    def get_localization_estimate(self):\n        # Get estimates from both localizers\n        amcl_estimate = self.amcl_localizer.get_pose_estimate()\n        vslam_estimate = self.vslam_localizer.get_pose_estimate()\n\n        # Get confidence scores\n        amcl_confidence = self.amcl_localizer.get_confidence()\n        vslam_confidence = self.vslam_localizer.get_confidence()\n\n        # Fuse estimates based on confidence\n        if amcl_confidence > self.amcl_confidence_threshold and vslam_confidence > self.vslam_confidence_threshold:\n            # Both are confident, use sensor fusion\n            fused_estimate = self.fusion_filter.fuse(amcl_estimate, vslam_estimate)\n        elif amcl_confidence > self.amcl_confidence_threshold:\n            # Only AMCL is confident\n            fused_estimate = amcl_estimate\n        elif vslam_confidence > self.vslam_confidence_threshold:\n            # Only VSLAM is confident\n            fused_estimate = vslam_estimate\n        else:\n            # Neither is confident, use last known good estimate with increased uncertainty\n            fused_estimate = self.get_last_good_estimate()\n            self.increase_uncertainty(fused_estimate)\n\n        return fused_estimate\n"})}),"\n",(0,o.jsx)(e.h2,{id:"localization-in-dynamic-environments",children:"Localization in Dynamic Environments"}),"\n",(0,o.jsx)(e.h3,{id:"handling-dynamic-elements",children:"Handling Dynamic Elements"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots often operate in human-populated environments:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Moving humans"}),": Handle dynamic obstacles that move unpredictably"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Changing layouts"}),": Adapt to environment changes"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Occlusions"}),": Handle temporary sensor occlusions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Clutter changes"}),": Adapt to moved objects"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"multi-hypothesis-tracking",children:"Multi-Hypothesis Tracking"}),"\n",(0,o.jsx)(e.p,{children:"Maintain multiple localization hypotheses:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Example multi-hypothesis localization for humanoid robot\nclass MultiHypothesisLocalizer:\n    def __init__(self):\n        self.hypotheses = []\n        self.max_hypotheses = 5\n        self.hypothesis_merge_threshold = 0.5\n\n    def update_hypotheses(self, sensor_data):\n        # Update all active hypotheses\n        for hypothesis in self.hypotheses:\n            self.update_single_hypothesis(hypothesis, sensor_data)\n\n        # Remove low-probability hypotheses\n        self.hypotheses = [h for h in self.hypotheses if h.probability > 0.01]\n\n        # Merge similar hypotheses\n        self.merge_similar_hypotheses()\n\n        # Normalize probabilities\n        self.normalize_probabilities()\n\n    def update_single_hypothesis(self, hypothesis, sensor_data):\n        # Update single hypothesis with sensor data\n        likelihood = self.calculate_likelihood(hypothesis.pose, sensor_data)\n        hypothesis.probability *= likelihood\n\n        # Update with motion model\n        hypothesis.pose = self.motion_model.predict(hypothesis.pose, hypothesis.control_input)\n\n    def merge_similar_hypotheses(self):\n        # Merge hypotheses that are spatially close\n        merged_hypotheses = []\n\n        for i, hyp1 in enumerate(self.hypotheses):\n            should_merge = False\n            for j, hyp2 in enumerate(merged_hypotheses):\n                if self.poses_close(hyp1.pose, hyp2.pose):\n                    # Merge hypotheses\n                    merged_hyp = self.merge_hypotheses(hyp1, hyp2)\n                    merged_hypotheses[j] = merged_hyp\n                    should_merge = True\n                    break\n\n            if not should_merge:\n                merged_hypotheses.append(hyp1)\n\n        self.hypotheses = merged_hypotheses\n"})}),"\n",(0,o.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(e.h3,{id:"computational-efficiency",children:"Computational Efficiency"}),"\n",(0,o.jsx)(e.p,{children:"Optimize localization for real-time humanoid operation:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Selective resampling"}),": Only resample when uncertainty is high"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Efficient likelihood computation"}),": Use optimized algorithms"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Parallel processing"}),": Process sensor data in parallel"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Adaptive particle count"}),": Adjust particle count based on uncertainty"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"memory-management",children:"Memory Management"}),"\n",(0,o.jsx)(e.p,{children:"Manage memory for continuous localization:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Example memory management for localization\nclass LocalizationMemoryManager:\n    def __init__(self):\n        self.max_particles = 2000\n        self.min_particles = 500\n        self.particle_buffer = []\n        self.map_cache = {}\n        self.max_cache_size = 100\n\n    def manage_particle_memory(self):\n        # Manage particle memory based on requirements\n        current_particles = len(self.particles)\n\n        if current_particles > self.max_particles:\n            # Reduce particles through resampling\n            self.resample_particles()\n        elif current_particles < self.min_particles:\n            # Increase particles for better accuracy\n            self.add_particles()\n\n    def manage_map_cache(self, map_key, map_data):\n        # Manage map cache to prevent memory overflow\n        if len(self.map_cache) >= self.max_cache_size:\n            # Remove oldest cached map\n            oldest_key = min(self.map_cache.keys(), key=lambda k: self.map_cache[k]['timestamp'])\n            del self.map_cache[oldest_key]\n\n        # Add new map to cache\n        self.map_cache[map_key] = {\n            'data': map_data,\n            'timestamp': rospy.Time.now()\n        }\n"})}),"\n",(0,o.jsx)(e.h2,{id:"troubleshooting-and-debugging",children:"Troubleshooting and Debugging"}),"\n",(0,o.jsx)(e.h3,{id:"common-localization-issues",children:"Common Localization Issues"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Particle deprivation"}),": All particles converge to wrong location"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Drift"}),": Gradual deviation from true position"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Head movement effects"}),": Head movement causing localization errors"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sensor noise"}),": High noise affecting localization accuracy"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"debugging-strategies",children:"Debugging Strategies"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Debug localization with ROS 2 tools\n# Monitor localization performance\nros2 run nav2_util lifecycle_bringup amcl\n\n# Visualize localization in RViz\nros2 run rviz2 rviz2\n\n# Check TF tree for transformations\nros2 run tf2_tools view_frames\n\n# Monitor localization topics\nros2 topic echo /amcl_pose\nros2 topic echo /particle_cloud\n"})}),"\n",(0,o.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,o.jsx)(e.h3,{id:"configuration-guidelines",children:"Configuration Guidelines"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Parameter tuning"}),": Carefully tune parameters for your specific humanoid robot"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sensor calibration"}),": Ensure all sensors are properly calibrated"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Map quality"}),": Use high-quality maps for AMCL-based localization"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Testing"}),": Test localization in various environments and conditions"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Localization confidence"}),": Only navigate when localization confidence is high"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Fallback strategies"}),": Implement fallback localization when primary fails"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Safety stops"}),": Stop navigation if localization uncertainty becomes too high"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Validation"}),": Continuously validate localization against other sensors"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"integration-with-navigation",children:"Integration with Navigation"}),"\n",(0,o.jsx)(e.h3,{id:"localization-navigation-interface",children:"Localization-Navigation Interface"}),"\n",(0,o.jsx)(e.p,{children:"Connect localization with navigation systems:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Example localization-navigation interface\nclass LocalizationNavigationInterface:\n    def __init__(self):\n        self.localizer = HybridLocalizationManager()\n        self.navigator = Nav2Navigator()\n\n        # Initialize safety checks\n        self.min_localization_confidence = 0.7\n        self.max_position_uncertainty = 0.5  # meters\n\n    def can_navigate(self):\n        # Check if localization is confident enough for navigation\n        pose_estimate = self.localizer.get_localization_estimate()\n        confidence = self.localizer.get_confidence()\n        uncertainty = self.calculate_uncertainty(pose_estimate)\n\n        return (confidence > self.min_localization_confidence and\n                uncertainty < self.max_position_uncertainty)\n\n    def update_navigation_with_localization(self):\n        # Update navigation system with localization information\n        if self.can_navigate():\n            current_pose = self.localizer.get_localization_estimate()\n            self.navigator.update_current_pose(current_pose)\n            self.navigator.enable_navigation()\n        else:\n            # Disable navigation until localization improves\n            self.navigator.disable_navigation()\n            self.request_relocalization()\n"})}),"\n",(0,o.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,o.jsxs)(e.p,{children:["For more detailed information about Nav2 localization and its customization for humanoid robots, refer to the ",(0,o.jsx)(e.a,{href:"https://navigation.ros.org/",children:"official Nav2 documentation"}),", the ",(0,o.jsx)(e.a,{href:"https://wiki.ros.org/amcl",children:"AMCL documentation"}),", and the ",(0,o.jsx)(e.a,{href:"https://navigation.ros.org/tutorials/",children:"ROS 2 Navigation tutorials"}),"."]})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},3023(n,e,i){i.d(e,{R:()=>t,x:()=>l});var o=i(3696);const a={},s=o.createContext(a);function t(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:t(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);