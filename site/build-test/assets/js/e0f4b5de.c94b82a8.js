"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[4580],{557(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var t=i(2540),o=i(3023);const a={title:"Simulation Environment Setup",sidebar_label:"Simulation Setup",description:"Guide to setting up simulation environment for end-to-end VLA pipeline testing in Isaac Sim"},r="Simulation Environment Setup",s={id:"capstone-system/simulation-setup",title:"Simulation Environment Setup",description:"Guide to setting up simulation environment for end-to-end VLA pipeline testing in Isaac Sim",source:"@site/docs/capstone-system/simulation-setup.md",sourceDirName:"capstone-system",slug:"/capstone-system/simulation-setup",permalink:"/docs/capstone-system/simulation-setup",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/capstone-system/simulation-setup.md",tags:[],version:"current",frontMatter:{title:"Simulation Environment Setup",sidebar_label:"Simulation Setup",description:"Guide to setting up simulation environment for end-to-end VLA pipeline testing in Isaac Sim"},sidebar:"tutorialSidebar",previous:{title:"Pipeline Integration",permalink:"/docs/capstone-system/pipeline-integration"},next:{title:"Complete Workflow",permalink:"/docs/capstone-system/complete-workflow"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Environment Configuration",id:"environment-configuration",level:2},{value:"1. Isaac Sim Setup",id:"1-isaac-sim-setup",level:3},{value:"2. ROS 2 Workspace Configuration",id:"2-ros-2-workspace-configuration",level:3},{value:"3. Simulation Scene Setup",id:"3-simulation-scene-setup",level:3},{value:"Voice Processing Configuration",id:"voice-processing-configuration",level:2},{value:"OpenAI Whisper Integration",id:"openai-whisper-integration",level:3},{value:"Audio Input Setup",id:"audio-input-setup",level:3},{value:"Cognitive Planning Configuration",id:"cognitive-planning-configuration",level:2},{value:"LLM Integration",id:"llm-integration",level:3},{value:"Planning Parameters",id:"planning-parameters",level:3}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"simulation-environment-setup",children:"Simulation Environment Setup"}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"This guide provides instructions for setting up the simulation environment to test the complete Vision-Language-Action (VLA) pipeline. The environment integrates Isaac Sim for realistic simulation, Isaac ROS for perception processing, and Nav2 for navigation in humanoid scenarios."}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before setting up the simulation environment, ensure you have:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"NVIDIA Isaac Sim installed and configured"}),"\n",(0,t.jsx)(n.li,{children:"ROS 2 Humble Hawksbill or later"}),"\n",(0,t.jsx)(n.li,{children:"Isaac ROS packages installed"}),"\n",(0,t.jsx)(n.li,{children:"Nav2 packages for humanoid navigation"}),"\n",(0,t.jsx)(n.li,{children:"OpenAI Whisper API access (for voice processing)"}),"\n",(0,t.jsx)(n.li,{children:"Large Language Model (LLM) API access (for cognitive planning)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"environment-configuration",children:"Environment Configuration"}),"\n",(0,t.jsx)(n.h3,{id:"1-isaac-sim-setup",children:"1. Isaac Sim Setup"}),"\n",(0,t.jsx)(n.p,{children:"Create a humanoid-friendly simulation environment:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Launch Isaac Sim with humanoid support\nisaac-sim --enable-ros2-bridge\n\n# Configure simulation parameters for humanoid robotics\nexport ISAAC_SIM_HUMANOID_SUPPORT=true\nexport ISAAC_SIM_NAVIGATION_SUPPORT=true\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-ros-2-workspace-configuration",children:"2. ROS 2 Workspace Configuration"}),"\n",(0,t.jsx)(n.p,{children:"Set up your ROS 2 workspace for the VLA pipeline:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Create workspace\nmkdir -p ~/vla_ws/src\ncd ~/vla_ws\n\n# Build workspace with Isaac packages\ncolcon build --symlink-install --packages-select \\\n  isaac_ros_benchmark isaac_ros_image_pipeline isaac_ros_visual_slam \\\n  nav2_bringup nav2_simple_commander nav2_behavior_tree_tools\n"})}),"\n",(0,t.jsx)(n.h3,{id:"3-simulation-scene-setup",children:"3. Simulation Scene Setup"}),"\n",(0,t.jsx)(n.p,{children:"Create simulation scenes appropriate for humanoid VLA testing:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example Python script for scene setup\nimport omni\nfrom pxr import Gf, UsdGeom, Sdf\n\n# Create humanoid-friendly environment\nstage = omni.usd.get_context().get_stage()\ndefault_prim = UsdGeom.Xform.Define(stage, "/World")\ndefault_prim.AddTranslateOp().Set(Gf.Vec3d(0.0, 0.0, 0.0))\n\n# Add humanoid robot\nrobot_path = "/World/Robot"\n# Import humanoid robot model\nomni.kit.commands.execute("CreatePrimWithDefaultXform",\n    prim_type="Xform",\n    prim_path=robot_path)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"voice-processing-configuration",children:"Voice Processing Configuration"}),"\n",(0,t.jsx)(n.h3,{id:"openai-whisper-integration",children:"OpenAI Whisper Integration"}),"\n",(0,t.jsx)(n.p,{children:"Configure Whisper for real-time voice processing:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Set up Whisper API configuration\nexport WHISPER_API_KEY="your-api-key"\nexport WHISPER_MODEL="whisper-1"\nexport WHISPER_LANGUAGE="en"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"audio-input-setup",children:"Audio Input Setup"}),"\n",(0,t.jsx)(n.p,{children:"Configure audio input for the simulation environment:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# audio_config.yaml\naudio:\n  input_device: "default"\n  sample_rate: 16000\n  channels: 1\n  buffer_size: 1024\n  format: "float32"\n\nvoice_processing:\n  model: "whisper-1"\n  language: "en"\n  confidence_threshold: 0.7\n  timeout: 5.0\n'})}),"\n",(0,t.jsx)(n.h2,{id:"cognitive-planning-configuration",children:"Cognitive Planning Configuration"}),"\n",(0,t.jsx)(n.h3,{id:"llm-integration",children:"LLM Integration"}),"\n",(0,t.jsx)(n.p,{children:"Configure LLM access for cognitive planning:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Set up LLM API configuration\nexport LLM_PROVIDER="openai"  # or "anthropic", "ollama", etc.\nexport LLM_API_KEY="your-api-key"\nexport LLM_MODEL="gpt-4-turbo"  # or appropriate model\n'})}),"\n",(0,t.jsx)(n.h3,{id:"planning-parameters",children:"Planning Parameters"}),"\n",(0,t.jsx)(n.p,{children:"Configure cognitive planning parameters:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# planning_config.yaml\ncognitive_planning:\n  max_tokens: 2048\n  temperature: 0.3\n  top_p: 0.9\n  timeout: 10.0\n  max_retries: 3\n\ntask_decomposition:\n  max_depth: 5\n  context_window: 4096\n  validation_enabled: true\n"})})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},3023(e,n,i){i.d(n,{R:()=>r,x:()=>s});var t=i(3696);const o={},a=t.createContext(o);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);