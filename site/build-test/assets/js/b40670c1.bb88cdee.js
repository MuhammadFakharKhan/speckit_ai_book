"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[7482],{3023(e,n,i){i.d(n,{R:()=>o,x:()=>l});var s=i(3696);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}},4977(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>l,toc:()=>c});var s=i(2540),r=i(3023);const t={title:"Speech Recognition with OpenAI Whisper",description:"Detailed documentation on implementing speech recognition using OpenAI Whisper for VLA systems",sidebar_position:2,tags:["vla","speech-recognition","whisper","audio-processing","nlp"]},o="Speech Recognition with OpenAI Whisper",l={id:"voice-to-action/speech-recognition-whisper",title:"Speech Recognition with OpenAI Whisper",description:"Detailed documentation on implementing speech recognition using OpenAI Whisper for VLA systems",source:"@site/docs/voice-to-action/speech-recognition-whisper.md",sourceDirName:"voice-to-action",slug:"/voice-to-action/speech-recognition-whisper",permalink:"/docs/voice-to-action/speech-recognition-whisper",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/voice-to-action/speech-recognition-whisper.md",tags:[{label:"vla",permalink:"/docs/tags/vla"},{label:"speech-recognition",permalink:"/docs/tags/speech-recognition"},{label:"whisper",permalink:"/docs/tags/whisper"},{label:"audio-processing",permalink:"/docs/tags/audio-processing"},{label:"nlp",permalink:"/docs/tags/nlp"}],version:"current",sidebarPosition:2,frontMatter:{title:"Speech Recognition with OpenAI Whisper",description:"Detailed documentation on implementing speech recognition using OpenAI Whisper for VLA systems",sidebar_position:2,tags:["vla","speech-recognition","whisper","audio-processing","nlp"]},sidebar:"tutorialSidebar",previous:{title:"Voice Command Processing Overview",permalink:"/docs/voice-to-action/"},next:{title:"Intent Parsing and Natural Language Understanding",permalink:"/docs/voice-to-action/intent-parsing"}},a={},c=[{value:"Overview",id:"overview",level:2},{value:"Whisper Architecture",id:"whisper-architecture",level:2},{value:"Model Capabilities",id:"model-capabilities",level:3},{value:"Technical Specifications",id:"technical-specifications",level:3},{value:"Integration with VLA System",id:"integration-with-vla-system",level:2},{value:"Audio Input Pipeline",id:"audio-input-pipeline",level:3},{value:"Audio Preprocessing",id:"audio-preprocessing",level:3},{value:"Whisper Processing",id:"whisper-processing",level:3},{value:"Configuration Options",id:"configuration-options",level:2},{value:"Model Selection",id:"model-selection",level:3},{value:"Language Settings",id:"language-settings",level:3},{value:"Response Format Options",id:"response-format-options",level:3},{value:"Confidence Scoring and Quality Assessment",id:"confidence-scoring-and-quality-assessment",level:2},{value:"Confidence Metrics",id:"confidence-metrics",level:3},{value:"Quality Thresholds",id:"quality-thresholds",level:3},{value:"Real-Time Processing",id:"real-time-processing",level:2},{value:"Streaming Audio",id:"streaming-audio",level:3},{value:"Error Handling and Fallbacks",id:"error-handling-and-fallbacks",level:2},{value:"Common Recognition Issues",id:"common-recognition-issues",level:3},{value:"Fallback Strategies",id:"fallback-strategies",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Local vs Cloud Processing",id:"local-vs-cloud-processing",level:3},{value:"Resource Management",id:"resource-management",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Message Structure",id:"message-structure",level:3},{value:"Publisher Example",id:"publisher-example",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Audio Quality",id:"audio-quality",level:3},{value:"Processing Optimization",id:"processing-optimization",level:3},{value:"Security and Privacy",id:"security-and-privacy",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Diagnostic Tools",id:"diagnostic-tools",level:3},{value:"Future Enhancements",id:"future-enhancements",level:2},{value:"Advanced Features",id:"advanced-features",level:3},{value:"Performance Improvements",id:"performance-improvements",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"speech-recognition-with-openai-whisper",children:"Speech Recognition with OpenAI Whisper"}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"OpenAI Whisper serves as the core speech recognition component in the Vision-Language-Action (VLA) system, providing state-of-the-art automatic speech recognition (ASR) capabilities. Whisper's robust performance across multiple languages and accents makes it ideal for natural human-robot interaction in humanoid robotics applications."}),"\n",(0,s.jsx)(n.h2,{id:"whisper-architecture",children:"Whisper Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"model-capabilities",children:"Model Capabilities"}),"\n",(0,s.jsx)(n.p,{children:"Whisper is a general-purpose speech recognition model that offers:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multilingual Support"}),": Supports recognition in multiple languages"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robust Performance"}),": Handles various acoustic conditions and accents"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Timestamp Alignment"}),": Provides precise timing information for spoken words"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language Identification"}),": Automatically detects the language being spoken"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Punctuation and Capitalization"}),": Outputs properly formatted text"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"technical-specifications",children:"Technical Specifications"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Architecture"}),": Transformer-based sequence-to-sequence model"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Training Data"}),": Large-scale multilingual and multitask training"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Output Format"}),": Text with confidence scores and timing information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Processing Modes"}),": Both real-time and batch processing capabilities"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-vla-system",children:"Integration with VLA System"}),"\n",(0,s.jsx)(n.h3,{id:"audio-input-pipeline",children:"Audio Input Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"The VLA system processes audio input through the following pipeline:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Microphone Input \u2192 Audio Preprocessing \u2192 Whisper Processing \u2192 Text Output \u2192 Intent Parsing\n"})}),"\n",(0,s.jsx)(n.h3,{id:"audio-preprocessing",children:"Audio Preprocessing"}),"\n",(0,s.jsx)(n.p,{children:"Before Whisper processing, audio undergoes preprocessing:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport librosa\n\ndef preprocess_audio(audio_data, sample_rate=16000):\n    """\n    Preprocess audio data for Whisper processing\n    """\n    # Resample to Whisper\'s expected sample rate\n    if sample_rate != 16000:\n        audio_data = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=16000)\n\n    # Normalize audio levels\n    audio_data = audio_data / np.max(np.abs(audio_data))\n\n    # Apply noise reduction if needed\n    # (implementation details would depend on specific requirements)\n\n    return audio_data\n'})}),"\n",(0,s.jsx)(n.h3,{id:"whisper-processing",children:"Whisper Processing"}),"\n",(0,s.jsx)(n.p,{children:"The core Whisper processing in the VLA system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nfrom openai import OpenAI\n\nclass WhisperSpeechProcessor:\n    def __init__(self, api_key=None, model="whisper-1"):\n        """\n        Initialize Whisper speech processor\n        """\n        if api_key:\n            self.client = OpenAI(api_key=api_key)\n        else:\n            # For local models, initialize accordingly\n            pass\n        self.model = model\n\n    def transcribe_audio(self, audio_file_path):\n        """\n        Transcribe audio file using Whisper\n        """\n        with open(audio_file_path, "rb") as audio_file:\n            transcript = self.client.audio.transcriptions.create(\n                model=self.model,\n                file=audio_file,\n                response_format="verbose_json",\n                timestamp_granularities=["word"]\n            )\n\n        return {\n            \'text\': transcript.text,\n            \'confidence\': self.calculate_confidence(transcript),\n            \'language\': transcript.language,\n            \'words\': transcript.words if hasattr(transcript, \'words\') else []\n        }\n\n    def calculate_confidence(self, transcript):\n        """\n        Calculate overall confidence score from Whisper output\n        """\n        # Implementation would analyze various factors like:\n        # - Word-level confidence scores\n        # - Audio quality metrics\n        # - Model certainty measures\n        return 0.95  # Placeholder - actual implementation would be more complex\n'})}),"\n",(0,s.jsx)(n.h2,{id:"configuration-options",children:"Configuration Options"}),"\n",(0,s.jsx)(n.h3,{id:"model-selection",children:"Model Selection"}),"\n",(0,s.jsx)(n.p,{children:"Whisper offers different model sizes for various performance requirements:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Size"}),(0,s.jsx)(n.th,{children:"Required VRAM"}),(0,s.jsx)(n.th,{children:"Relative Speed"}),(0,s.jsx)(n.th,{children:"Quality"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"tiny"}),(0,s.jsx)(n.td,{children:"75 MB"}),(0,s.jsx)(n.td,{children:"~1 GB"}),(0,s.jsx)(n.td,{children:"~32x"}),(0,s.jsx)(n.td,{children:"Lower"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"base"}),(0,s.jsx)(n.td,{children:"142 MB"}),(0,s.jsx)(n.td,{children:"~1 GB"}),(0,s.jsx)(n.td,{children:"~16x"}),(0,s.jsx)(n.td,{children:"Lower"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"small"}),(0,s.jsx)(n.td,{children:"465 MB"}),(0,s.jsx)(n.td,{children:"~2 GB"}),(0,s.jsx)(n.td,{children:"~6x"}),(0,s.jsx)(n.td,{children:"Medium"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"medium"}),(0,s.jsx)(n.td,{children:"1.5 GB"}),(0,s.jsx)(n.td,{children:"~5 GB"}),(0,s.jsx)(n.td,{children:"~2x"}),(0,s.jsx)(n.td,{children:"High"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"large"}),(0,s.jsx)(n.td,{children:"3.0 GB"}),(0,s.jsx)(n.td,{children:"~10 GB"}),(0,s.jsx)(n.td,{children:"1x"}),(0,s.jsx)(n.td,{children:"Highest"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"language-settings",children:"Language Settings"}),"\n",(0,s.jsx)(n.p,{children:"Whisper can be configured for specific languages or left to auto-detect:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Auto-detect language\ntranscript = client.audio.transcriptions.create(\n    model="whisper-1",\n    file=audio_file,\n    language=None  # Auto-detect\n)\n\n# Specify language explicitly\ntranscript = client.audio.transcriptions.create(\n    model="whisper-1",\n    file=audio_file,\n    language="en"  # English\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"response-format-options",children:"Response Format Options"}),"\n",(0,s.jsx)(n.p,{children:"Whisper supports multiple output formats:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Text"}),": Simple text output"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"JSON"}),": Structured output with additional metadata"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verbose JSON"}),": Detailed output with confidence scores and timestamps"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SRT/VTT"}),": Subtitle formats for timing information"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"confidence-scoring-and-quality-assessment",children:"Confidence Scoring and Quality Assessment"}),"\n",(0,s.jsx)(n.h3,{id:"confidence-metrics",children:"Confidence Metrics"}),"\n",(0,s.jsx)(n.p,{children:"The VLA system uses multiple metrics to assess recognition quality:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Overall Confidence"}),": General measure of transcription reliability"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Word-Level Confidence"}),": Individual word reliability scores"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Quality Metrics"}),": Measures of input audio quality"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language Model Confidence"}),": Measure of linguistic plausibility"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"quality-thresholds",children:"Quality Thresholds"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class ConfidenceEvaluator:\n    def __init__(self):\n        self.high_threshold = 0.90\n        self.medium_threshold = 0.70\n        self.low_threshold = 0.50\n\n    def evaluate_quality(self, transcript_result):\n        \"\"\"\n        Evaluate transcription quality and return appropriate action\n        \"\"\"\n        confidence = transcript_result['confidence']\n\n        if confidence >= self.high_threshold:\n            return 'accept'\n        elif confidence >= self.medium_threshold:\n            return 'confirm'\n        else:\n            return 'request_repeat'\n"})}),"\n",(0,s.jsx)(n.h2,{id:"real-time-processing",children:"Real-Time Processing"}),"\n",(0,s.jsx)(n.h3,{id:"streaming-audio",children:"Streaming Audio"}),"\n",(0,s.jsx)(n.p,{children:"For real-time applications, the VLA system can process audio streams:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import pyaudio\nimport wave\nimport threading\n\nclass RealTimeWhisperProcessor:\n    def __init__(self, chunk_size=1024, format=pyaudio.paInt16):\n        self.chunk_size = chunk_size\n        self.format = format\n        self.channels = 1\n        self.rate = 16000\n        self.audio = pyaudio.PyAudio()\n\n        # Buffer for accumulating audio chunks\n        self.audio_buffer = []\n\n    def start_listening(self):\n        """\n        Start real-time audio capture\n        """\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk_size\n        )\n\n        def capture_audio():\n            while True:\n                data = stream.read(self.chunk_size)\n                self.audio_buffer.append(data)\n\n                # Process accumulated audio when buffer is full\n                if len(self.audio_buffer) > 10:  # Process every 10 chunks\n                    self.process_buffer()\n\n        capture_thread = threading.Thread(target=capture_audio)\n        capture_thread.daemon = True\n        capture_thread.start()\n\n    def process_buffer(self):\n        """\n        Process accumulated audio buffer\n        """\n        # Combine buffer chunks into single audio data\n        audio_data = b\'\'.join(self.audio_buffer)\n        self.audio_buffer = []  # Clear buffer\n\n        # Save to temporary file for Whisper processing\n        temp_file = "temp_audio.wav"\n        wf = wave.open(temp_file, \'wb\')\n        wf.setnchannels(self.channels)\n        wf.setsampwidth(self.audio.get_sample_size(self.format))\n        wf.setframerate(self.rate)\n        wf.writeframes(audio_data)\n        wf.close()\n\n        # Process with Whisper\n        processor = WhisperSpeechProcessor()\n        result = processor.transcribe_audio(temp_file)\n\n        # Clean up temporary file\n        import os\n        os.remove(temp_file)\n\n        return result\n'})}),"\n",(0,s.jsx)(n.h2,{id:"error-handling-and-fallbacks",children:"Error Handling and Fallbacks"}),"\n",(0,s.jsx)(n.h3,{id:"common-recognition-issues",children:"Common Recognition Issues"}),"\n",(0,s.jsx)(n.p,{children:"The system handles various recognition challenges:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Background Noise"}),": Implement noise suppression and request repetition"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multiple Speakers"}),": Use speaker diarization to identify primary speaker"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Quality Issues"}),": Detect poor audio quality and suggest improvements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ambiguous Commands"}),": Request clarification when recognition is uncertain"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"fallback-strategies",children:"Fallback Strategies"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class WhisperFallbackHandler:\n    def __init__(self):\n        self.alternative_processors = []\n\n    def handle_recognition_failure(self, audio_data):\n        \"\"\"\n        Handle cases where primary Whisper processing fails\n        \"\"\"\n        # Try with different model settings\n        try:\n            result = self.process_with_alternative_settings(audio_data)\n            if self.is_confident_enough(result):\n                return result\n        except:\n            pass\n\n        # Try with different language settings\n        try:\n            result = self.process_with_language_detection(audio_data)\n            if self.is_confident_enough(result):\n                return result\n        except:\n            pass\n\n        # Return failure indication\n        return {\n            'success': False,\n            'error': 'Unable to recognize speech with sufficient confidence',\n            'suggestions': ['Speak more clearly', 'Reduce background noise', 'Repeat command']\n        }\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"local-vs-cloud-processing",children:"Local vs Cloud Processing"}),"\n",(0,s.jsx)(n.p,{children:"The VLA system supports both local and cloud-based Whisper processing:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Local Processing:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Advantages: Lower latency, privacy, offline capability"}),"\n",(0,s.jsx)(n.li,{children:"Disadvantages: Higher computational requirements, larger model size"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Cloud Processing:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Advantages: No local computational requirements, maintained by OpenAI"}),"\n",(0,s.jsx)(n.li,{children:"Disadvantages: Network dependency, potential privacy concerns"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"resource-management",children:"Resource Management"}),"\n",(0,s.jsx)(n.p,{children:"For humanoid robot deployment, consider:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Size"}),": Balance accuracy with computational requirements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Processing Frequency"}),": Optimize for real-time performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory Management"}),": Efficiently manage model loading and unloading"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Power Consumption"}),": Consider impact on robot battery life"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,s.jsx)(n.h3,{id:"message-structure",children:"Message Structure"}),"\n",(0,s.jsx)(n.p,{children:"Whisper results are integrated into ROS 2 as custom messages:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Custom message: VoiceRecognitionResult.msg\nstring text\nfloat32 confidence\nstring language\ntime timestamp\nstring[] word_timestamps\n"})}),"\n",(0,s.jsx)(n.h3,{id:"publisher-example",children:"Publisher Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom vla_msgs.msg import VoiceRecognitionResult\n\nclass WhisperROS2Bridge(Node):\n    def __init__(self):\n        super().__init__('whisper_ros2_bridge')\n        self.publisher = self.create_publisher(\n            VoiceRecognitionResult,\n            'voice_recognition/result',\n            10\n        )\n\n    def publish_recognition_result(self, result):\n        \"\"\"\n        Publish Whisper result to ROS 2\n        \"\"\"\n        msg = VoiceRecognitionResult()\n        msg.text = result['text']\n        msg.confidence = result['confidence']\n        msg.language = result['language']\n        msg.timestamp = self.get_clock().now().to_msg()\n\n        self.publisher.publish(msg)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"audio-quality",children:"Audio Quality"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Microphone Placement"}),": Position microphones for optimal speech capture"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Noise Suppression"}),": Implement hardware and software noise reduction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Format"}),": Use appropriate sample rates and bit depths"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Calibration"}),": Regularly calibrate audio input for consistent quality"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"processing-optimization",children:"Processing Optimization"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch Processing"}),": When possible, process longer audio segments for better accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Selection"}),": Choose appropriate model size for performance requirements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Caching"}),": Cache results for repeated commands to improve response time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fallback Handling"}),": Implement graceful degradation for low-confidence results"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"security-and-privacy",children:"Security and Privacy"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Encryption"}),": Encrypt audio data during transmission"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Local Processing"}),": When possible, use local Whisper models to preserve privacy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Access Control"}),": Implement proper authentication for API access"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Retention"}),": Follow appropriate data retention policies"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Low Recognition Accuracy"}),": Check audio quality, microphone placement, and background noise"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High Latency"}),": Optimize model selection and processing pipeline"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"API Errors"}),": Verify API keys and rate limits"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory Issues"}),": Monitor memory usage and optimize model loading"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"diagnostic-tools",children:"Diagnostic Tools"}),"\n",(0,s.jsx)(n.p,{children:"The VLA system includes diagnostic tools for Whisper performance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def diagnose_audio_quality(audio_data):\n    \"\"\"\n    Analyze audio quality metrics\n    \"\"\"\n    metrics = {\n        'signal_to_noise_ratio': calculate_snr(audio_data),\n        'peak_amplitude': np.max(np.abs(audio_data)),\n        'frequency_spectrum': analyze_frequency_spectrum(audio_data)\n    }\n    return metrics\n"})}),"\n",(0,s.jsx)(n.h2,{id:"future-enhancements",children:"Future Enhancements"}),"\n",(0,s.jsx)(n.h3,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speaker Identification"}),": Distinguish between different users"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Emotion Recognition"}),": Detect emotional context in speech"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-Modal Fusion"}),": Combine audio with visual context"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Adaptive Learning"}),": Improve recognition based on user patterns"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-improvements",children:"Performance Improvements"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Edge Optimization"}),": Further optimize models for robot deployment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Latency Reduction"}),": Minimize processing delays for real-time interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Energy Efficiency"}),": Reduce power consumption for battery-powered robots"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"OpenAI Whisper provides robust speech recognition capabilities that form the foundation of natural language interaction in the VLA system. Proper configuration and integration ensure reliable voice command processing for humanoid robotics applications."}),"\n",(0,s.jsxs)(n.p,{children:["For implementation details, refer to the complete ",(0,s.jsx)(n.a,{href:"/docs/voice-to-action/",children:"Voice Command Processing"})," overview and the ",(0,s.jsx)(n.a,{href:"/docs/voice-to-action/intent-parsing",children:"Intent Parsing"})," documentation for the next step in the pipeline."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);