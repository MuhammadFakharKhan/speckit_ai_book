"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[7663],{3023(e,n,t){t.d(n,{R:()=>a,x:()=>r});var o=t(3696);const s={},i=o.createContext(s);function a(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),o.createElement(i.Provider,{value:n},e.children)}},6085(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var o=t(2540),s=t(3023);const i={title:"Simulated Voice Command Examples with ROS 2 Outputs",description:"Comprehensive examples of voice commands and their expected ROS 2 outputs in VLA systems",sidebar_position:8,tags:["vla","voice-examples","ros2-outputs","simulation","testing"]},a="Simulated Voice Command Examples with ROS 2 Outputs",r={id:"voice-to-action/simulated-voice-examples",title:"Simulated Voice Command Examples with ROS 2 Outputs",description:"Comprehensive examples of voice commands and their expected ROS 2 outputs in VLA systems",source:"@site/docs/voice-to-action/simulated-voice-examples.md",sourceDirName:"voice-to-action",slug:"/voice-to-action/simulated-voice-examples",permalink:"/docs/voice-to-action/simulated-voice-examples",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/voice-to-action/simulated-voice-examples.md",tags:[{label:"vla",permalink:"/docs/tags/vla"},{label:"voice-examples",permalink:"/docs/tags/voice-examples"},{label:"ros2-outputs",permalink:"/docs/tags/ros-2-outputs"},{label:"simulation",permalink:"/docs/tags/simulation"},{label:"testing",permalink:"/docs/tags/testing"}],version:"current",sidebarPosition:8,frontMatter:{title:"Simulated Voice Command Examples with ROS 2 Outputs",description:"Comprehensive examples of voice commands and their expected ROS 2 outputs in VLA systems",sidebar_position:8,tags:["vla","voice-examples","ros2-outputs","simulation","testing"]},sidebar:"tutorialSidebar",previous:{title:"Speech-to-Text Processing Workflows and Validation",permalink:"/docs/voice-to-action/stt-processing-workflows"},next:{title:"Cognitive Planning Overview",permalink:"/docs/cognitive-planning/"}},c={},l=[{value:"Overview",id:"overview",level:2},{value:"Example Categories",id:"example-categories",level:2},{value:"Navigation Commands",id:"navigation-commands",level:3},{value:"Example 1: Simple Navigation",id:"example-1-simple-navigation",level:4},{value:"Example 2: Directional Movement",id:"example-2-directional-movement",level:4},{value:"Example 3: Complex Navigation",id:"example-3-complex-navigation",level:4},{value:"Manipulation Commands",id:"manipulation-commands",level:3},{value:"Example 4: Object Grasping",id:"example-4-object-grasping",level:4},{value:"Example 5: Object Placement",id:"example-5-object-placement",level:4},{value:"Perception Commands",id:"perception-commands",level:3},{value:"Example 6: Object Detection",id:"example-6-object-detection",level:4},{value:"Example 7: Specific Object Search",id:"example-7-specific-object-search",level:4},{value:"Complex Multi-Step Commands",id:"complex-multi-step-commands",level:3},{value:"Example 8: Fetch Task",id:"example-8-fetch-task",level:4},{value:"Example 9: Complex Navigation",id:"example-9-complex-navigation",level:4},{value:"Simulation Environment Setup",id:"simulation-environment-setup",level:2},{value:"Example Simulation Configuration",id:"example-simulation-configuration",level:3},{value:"Example ROS 2 Launch File",id:"example-ros-2-launch-file",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Unit Test Examples",id:"unit-test-examples",level:3},{value:"Integration Test Examples",id:"integration-test-examples",level:3},{value:"Performance Metrics",id:"performance-metrics",level:2},{value:"Quality Metrics for Voice Command Processing",id:"quality-metrics-for-voice-command-processing",level:3},{value:"Error Handling Examples",id:"error-handling-examples",level:2},{value:"Common Error Scenarios",id:"common-error-scenarios",level:3},{value:"Advanced Examples",id:"advanced-examples",level:2},{value:"Context-Aware Commands",id:"context-aware-commands",level:3},{value:"Multi-Modal Integration",id:"multi-modal-integration",level:3},{value:"Best Practices for Simulation",id:"best-practices-for-simulation",level:2},{value:"1. Realistic Audio Simulation",id:"1-realistic-audio-simulation",level:3},{value:"2. Comprehensive Testing",id:"2-comprehensive-testing",level:3},{value:"3. Performance Monitoring",id:"3-performance-monitoring",level:3},{value:"4. Iterative Improvement",id:"4-iterative-improvement",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Issue 1: Low Recognition Accuracy",id:"issue-1-low-recognition-accuracy",level:3},{value:"Issue 2: Incorrect ROS 2 Message Generation",id:"issue-2-incorrect-ros-2-message-generation",level:3},{value:"Issue 3: Context Confusion",id:"issue-3-context-confusion",level:3},{value:"Conclusion",id:"conclusion",level:2}];function m(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"simulated-voice-command-examples-with-ros-2-outputs",children:"Simulated Voice Command Examples with ROS 2 Outputs"}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"This document provides comprehensive examples of voice commands and their expected ROS 2 outputs in the Vision-Language-Action (VLA) system. These simulated examples demonstrate how natural language voice commands are processed through the entire pipeline to generate specific ROS 2 messages for robot control."}),"\n",(0,o.jsx)(n.h2,{id:"example-categories",children:"Example Categories"}),"\n",(0,o.jsx)(n.h3,{id:"navigation-commands",children:"Navigation Commands"}),"\n",(0,o.jsx)(n.p,{children:"Navigation commands instruct the robot to move to specific locations or in specific directions."}),"\n",(0,o.jsx)(n.h4,{id:"example-1-simple-navigation",children:"Example 1: Simple Navigation"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command"}),': "Go to the kitchen"\n',(0,o.jsx)(n.strong,{children:"Expected Processing Flow"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Audio Input \u2192 Speech Recognition \u2192 "Go to the kitchen" (confidence: 0.92)\nIntent Parsing \u2192 Intent: MoveTo, Parameters: {destination: "kitchen"}\nCommand Translation \u2192 Generate navigation goal for kitchen location\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected ROS 2 Output"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Message: geometry_msgs/PoseStamped\n{\n    "header": {\n        "stamp": {"sec": 1234567890, "nanosec": 123456789},\n        "frame_id": "map"\n    },\n    "pose": {\n        "position": {"x": 5.0, "y": 3.0, "z": 0.0},\n        "orientation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0}\n    }\n}\n\n# Alternative: nav2_msgs/MoveToPose Action Goal\n{\n    "pose": {\n        "position": {"x": 5.0, "y": 3.0, "z": 0.0},\n        "orientation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0}\n    },\n    "behavior_tree_id": "default"\n}\n'})}),"\n",(0,o.jsx)(n.h4,{id:"example-2-directional-movement",children:"Example 2: Directional Movement"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command"}),': "Move forward 2 meters"\n',(0,o.jsx)(n.strong,{children:"Expected Processing Flow"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Audio Input \u2192 Speech Recognition \u2192 "Move forward 2 meters" (confidence: 0.88)\nIntent Parsing \u2192 Intent: MoveDirection, Parameters: {direction: "forward", distance: 2.0}\nCommand Translation \u2192 Generate movement command for 2m forward\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected ROS 2 Output"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Message: geometry_msgs/Twist\n{\n    "linear": {"x": 0.5, "y": 0.0, "z": 0.0},  # Forward movement\n    "angular": {"x": 0.0, "y": 0.0, "z": 0.0}   # No rotation\n}\n\n# Duration for 2 meters at 0.5 m/s: 4 seconds\n# This would typically be sent as a velocity command for a specific duration\n'})}),"\n",(0,o.jsx)(n.h4,{id:"example-3-complex-navigation",children:"Example 3: Complex Navigation"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command"}),': "Turn left and then go to the living room"\n',(0,o.jsx)(n.strong,{children:"Expected Processing Flow"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Audio Input \u2192 Speech Recognition \u2192 "Turn left and then go to the living room" (confidence: 0.85)\nIntent Parsing \u2192 Intent: MultiStep, Parameters: [\n    {action: "turn", direction: "left"},\n    {action: "move_to", destination: "living room"}\n]\nCommand Translation \u2192 Generate sequence of navigation commands\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected ROS 2 Output"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# First: Turn left command\n# Message: geometry_msgs/Twist\n{\n    "linear": {"x": 0.0, "y": 0.0, "z": 0.0},   # No forward movement\n    "angular": {"x": 0.0, "y": 0.0, "z": 0.5}    # Left turn (positive z)\n}\n\n# After turn completion: Move to living room\n# Message: geometry_msgs/PoseStamped\n{\n    "header": {"stamp": {"sec": 1234567894, "nanosec": 123456789}, "frame_id": "map"},\n    "pose": {\n        "position": {"x": -2.0, "y": 1.5, "z": 0.0},\n        "orientation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0}\n    }\n}\n'})}),"\n",(0,o.jsx)(n.h3,{id:"manipulation-commands",children:"Manipulation Commands"}),"\n",(0,o.jsx)(n.p,{children:"Manipulation commands instruct the robot to interact with objects in the environment."}),"\n",(0,o.jsx)(n.h4,{id:"example-4-object-grasping",children:"Example 4: Object Grasping"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command"}),': "Pick up the red cup"\n',(0,o.jsx)(n.strong,{children:"Expected Processing Flow"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Audio Input \u2192 Speech Recognition \u2192 "Pick up the red cup" (confidence: 0.90)\nIntent Parsing \u2192 Intent: PickUp, Parameters: {object_type: "cup", color: "red"}\nCommand Translation \u2192 Generate manipulation sequence for red cup\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected ROS 2 Output"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Message: control_msgs/FollowJointTrajectory\n{\n    "trajectory": {\n        "joint_names": ["joint1", "joint2", "joint3", "joint4", "joint5", "joint6"],\n        "points": [\n            {\n                "positions": [0.1, -0.5, 1.2, 0.0, 0.8, -0.2],\n                "velocities": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n                "time_from_start": {"sec": 2, "nanosec": 0}\n            },\n            {\n                "positions": [0.2, -0.4, 1.1, 0.1, 0.9, -0.1],\n                "velocities": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n                "time_from_start": {"sec": 4, "nanosec": 0}\n            }\n        ]\n    }\n}\n\n# Also: perception request to locate the red cup\n# Message: vision_msgs/Detection2DArray\n{\n    "header": {"stamp": {"sec": 1234567895, "nanosec": 123456789}, "frame_id": "camera_frame"},\n    "detections": [\n        {\n            "results": [\n                {\n                    "id": "cup",\n                    "score": 0.85,\n                    "hypothesis": {"class_id": "cup", "score": 0.85}\n                }\n            ],\n            "bbox": {"center": {"x": 320, "y": 240}, "size_x": 50, "size_y": 60}\n        }\n    ]\n}\n'})}),"\n",(0,o.jsx)(n.h4,{id:"example-5-object-placement",children:"Example 5: Object Placement"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command"}),': "Put the book on the table"\n',(0,o.jsx)(n.strong,{children:"Expected Processing Flow"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Audio Input \u2192 Speech Recognition \u2192 "Put the book on the table" (confidence: 0.87)\nIntent Parsing \u2192 Intent: Place, Parameters: {object: "book", destination: "table"}\nCommand Translation \u2192 Generate placement sequence for book on table\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected ROS 2 Output"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Message: control_msgs/FollowJointTrajectory\n{\n    "trajectory": {\n        "joint_names": ["joint1", "joint2", "joint3", "joint4", "joint5", "joint6"],\n        "points": [\n            {\n                "positions": [0.5, 0.2, 0.8, -0.3, 0.4, 0.1],\n                "velocities": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n                "time_from_start": {"sec": 3, "nanosec": 0}\n            }\n        ]\n    }\n}\n\n# Message: std_msgs/String (for gripper control)\n{\n    "data": "release_object"\n}\n'})}),"\n",(0,o.jsx)(n.h3,{id:"perception-commands",children:"Perception Commands"}),"\n",(0,o.jsx)(n.p,{children:"Perception commands request environmental information from the robot."}),"\n",(0,o.jsx)(n.h4,{id:"example-6-object-detection",children:"Example 6: Object Detection"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command"}),': "What objects do you see?"\n',(0,o.jsx)(n.strong,{children:"Expected Processing Flow"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Audio Input \u2192 Speech Recognition \u2192 "What objects do you see?" (confidence: 0.91)\nIntent Parsing \u2192 Intent: Describe, Parameters: {request_type: "environmental"}\nCommand Translation \u2192 Generate scene description request\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected ROS 2 Output"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Message: vision_msgs/Detection2DArray\n{\n    "header": {"stamp": {"sec": 1234567896, "nanosec": 123456789}, "frame_id": "camera_frame"},\n    "detections": [\n        {\n            "results": [\n                {\n                    "id": "cup",\n                    "score": 0.88,\n                    "hypothesis": {"class_id": "cup", "score": 0.88}\n                }\n            ],\n            "bbox": {"center": {"x": 100, "y": 150}, "size_x": 40, "size_y": 50}\n        },\n        {\n            "results": [\n                {\n                    "id": "book",\n                    "score": 0.75,\n                    "hypothesis": {"class_id": "book", "score": 0.75}\n                }\n            ],\n            "bbox": {"center": {"x": 200, "y": 300}, "size_x": 80, "size_y": 100}\n        }\n    ]\n}\n\n# Message: std_msgs/String (for voice response)\n{\n    "data": "I see a red cup and a book in front of me."\n}\n'})}),"\n",(0,o.jsx)(n.h4,{id:"example-7-specific-object-search",children:"Example 7: Specific Object Search"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command"}),': "Find the blue ball"\n',(0,o.jsx)(n.strong,{children:"Expected Processing Flow"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Audio Input \u2192 Speech Recognition \u2192 "Find the blue ball" (confidence: 0.89)\nIntent Parsing \u2192 Intent: FindObject, Parameters: {object_type: "ball", color: "blue"}\nCommand Translation \u2192 Generate object search sequence\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected ROS 2 Output"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Message: vision_msgs/Detection2DArray\n{\n    "header": {"stamp": {"sec": 1234567897, "nanosec": 123456789}, "frame_id": "camera_frame"},\n    "detections": [\n        {\n            "results": [\n                {\n                    "id": "ball",\n                    "score": 0.92,\n                    "hypothesis": {"class_id": "ball", "score": 0.92}\n                }\n            ],\n            "bbox": {"center": {"x": 400, "y": 200}, "size_x": 60, "size_y": 60}\n        }\n    ]\n}\n\n# Message: geometry_msgs/PoseStamped (if navigation needed to approach object)\n{\n    "header": {"stamp": {"sec": 1234567898, "nanosec": 123456789}, "frame_id": "map"},\n    "pose": {\n        "position": {"x": 1.5, "y": 2.0, "z": 0.0},\n        "orientation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0}\n    }\n}\n'})}),"\n",(0,o.jsx)(n.h3,{id:"complex-multi-step-commands",children:"Complex Multi-Step Commands"}),"\n",(0,o.jsx)(n.p,{children:"Complex commands involve multiple actions that must be coordinated."}),"\n",(0,o.jsx)(n.h4,{id:"example-8-fetch-task",children:"Example 8: Fetch Task"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command"}),': "Go to the kitchen and bring me the coffee"\n',(0,o.jsx)(n.strong,{children:"Expected Processing Flow"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Audio Input \u2192 Speech Recognition \u2192 "Go to the kitchen and bring me the coffee" (confidence: 0.86)\nIntent Parsing \u2192 Intent: Fetch, Parameters: {destination: "kitchen", object: "coffee"}\nCommand Translation \u2192 Generate multi-step sequence: navigate \u2192 locate \u2192 grasp \u2192 return\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected ROS 2 Output Sequence"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Step 1: Navigate to kitchen\n# Message: geometry_msgs/PoseStamped\n{\n    "header": {"stamp": {"sec": 1234567899, "nanosec": 123456789}, "frame_id": "map"},\n    "pose": {\n        "position": {"x": 5.0, "y": 3.0, "z": 0.0},\n        "orientation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0}\n    }\n}\n\n# Step 2: Locate coffee (perception request)\n# Message: vision_msgs/Detection2DArray\n{\n    "header": {"stamp": {"sec": 1234567900, "nanosec": 123456789}, "frame_id": "camera_frame"},\n    "detections": [\n        {\n            "results": [\n                {\n                    "id": "coffee",\n                    "score": 0.84,\n                    "hypothesis": {"class_id": "coffee", "score": 0.84}\n                }\n            ],\n            "bbox": {"center": {"x": 250, "y": 180}, "size_x": 45, "size_y": 55}\n        }\n    ]\n}\n\n# Step 3: Grasp coffee\n# Message: control_msgs/FollowJointTrajectory\n{\n    "trajectory": {\n        "joint_names": ["joint1", "joint2", "joint3", "joint4", "joint5", "joint6"],\n        "points": [\n            {\n                "positions": [0.3, -0.2, 0.9, 0.1, 0.6, 0.0],\n                "velocities": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n                "time_from_start": {"sec": 2, "nanosec": 0}\n            }\n        ]\n    }\n}\n\n# Step 4: Return to user\n# Message: geometry_msgs/PoseStamped\n{\n    "header": {"stamp": {"sec": 1234567902, "nanosec": 123456789}, "frame_id": "map"},\n    "pose": {\n        "position": {"x": 0.0, "y": 0.0, "z": 0.0},\n        "orientation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0}\n    }\n}\n'})}),"\n",(0,o.jsx)(n.h4,{id:"example-9-complex-navigation",children:"Example 9: Complex Navigation"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command"}),': "Move to the table, turn around, and come back"\n',(0,o.jsx)(n.strong,{children:"Expected Processing Flow"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Audio Input \u2192 Speech Recognition \u2192 "Move to the table, turn around, and come back" (confidence: 0.83)\nIntent Parsing \u2192 Intent: MultiStep, Parameters: [\n    {action: "move_to", destination: "table"},\n    {action: "turn", degrees: 180},\n    {action: "move_to", destination: "original_position"}\n]\nCommand Translation \u2192 Generate coordinated navigation sequence\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected ROS 2 Output Sequence"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Step 1: Move to table\n# Message: geometry_msgs/PoseStamped\n{\n    "header": {"stamp": {"sec": 1234567903, "nanosec": 123456789}, "frame_id": "map"},\n    "pose": {\n        "position": {"x": 1.0, "y": 0.5, "z": 0.0},\n        "orientation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0}\n    }\n}\n\n# Step 2: Turn around (180 degrees)\n# Message: geometry_msgs/Twist\n{\n    "linear": {"x": 0.0, "y": 0.0, "z": 0.0},\n    "angular": {"x": 0.0, "y": 0.0, "z": 1.0}  # Rotate at 1.0 rad/s\n}\n\n# Step 3: Return to starting position\n# Message: geometry_msgs/PoseStamped\n{\n    "header": {"stamp": {"sec": 1234567906, "nanosec": 123456789}, "frame_id": "map"},\n    "pose": {\n        "position": {"x": 0.0, "y": 0.0, "z": 0.0},\n        "orientation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0}\n    }\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"simulation-environment-setup",children:"Simulation Environment Setup"}),"\n",(0,o.jsx)(n.h3,{id:"example-simulation-configuration",children:"Example Simulation Configuration"}),"\n",(0,o.jsx)(n.p,{children:"To properly test these voice commands in simulation, the following configuration is typically used:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'# simulation_config.yaml\nsimulation_environment:\n  world: "vla_test_world"\n  robot_model: "humanoid_robot"\n  sensor_configurations:\n    camera:\n      resolution: [640, 480]\n      fov: 60\n      frame_id: "camera_frame"\n    lidar:\n      range: 10.0\n      resolution: 0.01\n      frame_id: "lidar_frame"\n  navigation_map:\n    resolution: 0.05\n    origin: [-10, -10, 0]\n    size: [20, 20]\n  object_locations:\n    kitchen: [5.0, 3.0, 0.0]\n    living_room: [-2.0, 1.5, 0.0]\n    table: [1.0, 0.5, 0.0]\n    red_cup: [1.2, 0.6, 0.8]\n    book: [0.8, 0.4, 0.85]\n    blue_ball: [2.5, 1.2, 0.5]\n'})}),"\n",(0,o.jsx)(n.h3,{id:"example-ros-2-launch-file",children:"Example ROS 2 Launch File"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:"\x3c!-- vla_simulation.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Navigation stack\n        Node(\n            package='nav2_bringup',\n            executable='nav2_launch.py',\n            name='navigation',\n            parameters=[\n                os.path.join(get_package_share_directory('vla_simulation'), 'config', 'nav2_params.yaml')\n            ]\n        ),\n\n        # Voice processing node\n        Node(\n            package='vla_voice_processing',\n            executable='voice_processor',\n            name='voice_processor',\n            parameters=[\n                os.path.join(get_package_share_directory('vla_simulation'), 'config', 'voice_params.yaml')\n            ]\n        ),\n\n        # Perception stack\n        Node(\n            package='vla_perception',\n            executable='object_detector',\n            name='object_detector'\n        ),\n\n        # Manipulation controller\n        Node(\n            package='vla_manipulation',\n            executable='arm_controller',\n            name='arm_controller'\n        )\n    ])\n"})}),"\n",(0,o.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,o.jsx)(n.h3,{id:"unit-test-examples",children:"Unit Test Examples"}),"\n",(0,o.jsx)(n.p,{children:"Here are examples of how to test these voice command simulations:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import unittest\nfrom vla_voice_processing.voice_processor import VoiceProcessor\nfrom vla_command_translator.command_translator import CommandTranslator\nimport json\n\nclass TestVoiceCommandExamples(unittest.TestCase):\n    def setUp(self):\n        self.voice_processor = VoiceProcessor()\n        self.command_translator = CommandTranslator()\n        self.test_context = {\n            'known_locations': {\n                'kitchen': {'x': 5.0, 'y': 3.0, 'z': 0.0},\n                'living_room': {'x': -2.0, 'y': 1.5, 'z': 0.0},\n                'table': {'x': 1.0, 'y': 0.5, 'z': 0.0}\n            },\n            'visible_objects': [\n                {'type': 'cup', 'color': 'red', 'pose': {'x': 1.2, 'y': 0.6, 'z': 0.8}},\n                {'type': 'book', 'color': 'blue', 'pose': {'x': 0.8, 'y': 0.4, 'z': 0.85}}\n            ]\n        }\n\n    def test_simple_navigation(self):\n        \"\"\"Test 'Go to the kitchen' command\"\"\"\n        audio_input = self.create_mock_audio(\"Go to the kitchen\")\n        stt_result = self.voice_processor.transcribe_audio(audio_input)\n\n        self.assertGreater(stt_result['confidence'], 0.8)\n        self.assertIn(\"kitchen\", stt_result['text'].lower())\n\n        intent = self.voice_processor.parse_intent(stt_result['text'])\n        self.assertEqual(intent.intent_type.value, 'move_to')\n        self.assertEqual(intent.parameters.destination, 'kitchen')\n\n        ros2_commands = self.command_translator.translate_to_ros2(intent, self.test_context)\n        self.assertEqual(len(ros2_commands), 1)\n        self.assertEqual(ros2_commands[0]['type'], 'geometry_msgs/PoseStamped')\n        self.assertEqual(ros2_commands[0]['data']['position']['x'], 5.0)\n\n    def test_manipulation_command(self):\n        \"\"\"Test 'Pick up the red cup' command\"\"\"\n        audio_input = self.create_mock_audio(\"Pick up the red cup\")\n        stt_result = self.voice_processor.transcribe_audio(audio_input)\n\n        self.assertGreater(stt_result['confidence'], 0.8)\n        self.assertIn(\"pick up\", stt_result['text'].lower())\n        self.assertIn(\"cup\", stt_result['text'].lower())\n\n        intent = self.voice_processor.parse_intent(stt_result['text'])\n        self.assertEqual(intent.intent_type.value, 'pick_up')\n        self.assertEqual(intent.parameters.object_type, 'cup')\n        self.assertEqual(intent.parameters.object_color, 'red')\n\n        ros2_commands = self.command_translator.translate_to_ros2(intent, self.test_context)\n        # Should generate perception request and manipulation commands\n        self.assertGreater(len(ros2_commands), 1)\n        command_types = [cmd['type'] for cmd in ros2_commands]\n        self.assertIn('vision_msgs/Detection2DArray', command_types)\n\n    def create_mock_audio(self, text):\n        \"\"\"Create mock audio data for testing\"\"\"\n        # In real tests, this would generate audio from text\n        # For simulation, we can use text directly\n        return text.encode('utf-8')\n\nif __name__ == '__main__':\n    unittest.main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"integration-test-examples",children:"Integration Test Examples"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom vision_msgs.msg import Detection2DArray\nimport time\n\nclass IntegrationTestNode(Node):\n    def __init__(self):\n        super().__init__('vla_integration_test')\n\n        # Publishers for sending commands\n        self.voice_command_pub = self.create_publisher(String, '/voice_commands', 10)\n\n        # Subscribers for monitoring outputs\n        self.nav_goal_sub = self.create_subscription(\n            PoseStamped, '/goal_pose', self.nav_goal_callback, 10\n        )\n        self.perception_sub = self.create_subscription(\n            Detection2DArray, '/object_detections', self.perception_callback, 10\n        )\n\n        self.nav_goals_received = []\n        self.perception_received = []\n\n    def nav_goal_callback(self, msg):\n        self.nav_goals_received.append(msg)\n\n    def perception_callback(self, msg):\n        self.perception_received.append(msg)\n\n    def test_navigation_command(self):\n        \"\"\"Test end-to-end navigation command\"\"\"\n        # Clear previous results\n        self.nav_goals_received.clear()\n\n        # Send voice command\n        cmd_msg = String()\n        cmd_msg.data = \"Go to the kitchen\"\n        self.voice_command_pub.publish(cmd_msg)\n\n        # Wait for processing\n        time.sleep(2.0)\n\n        # Verify navigation goal was generated\n        self.assertEqual(len(self.nav_goals_received), 1)\n        goal = self.nav_goals_received[0]\n        self.assertAlmostEqual(goal.pose.position.x, 5.0, places=1)\n        self.assertAlmostEqual(goal.pose.position.y, 3.0, places=1)\n\ndef main():\n    rclpy.init()\n    test_node = IntegrationTestNode()\n\n    # Run tests\n    test_node.test_navigation_command()\n\n    rclpy.spin_once(test_node, timeout_sec=0.1)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,o.jsx)(n.h3,{id:"quality-metrics-for-voice-command-processing",children:"Quality Metrics for Voice Command Processing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class VoiceCommandMetrics:\n    def __init__(self):\n        self.command_success_rate = 0.0\n        self.average_confidence = 0.0\n        self.average_processing_time = 0.0\n        self.command_accuracy = 0.0\n\n    def calculate_metrics(self, test_results):\n        \"\"\"\n        Calculate performance metrics from test results\n        \"\"\"\n        if not test_results:\n            return {}\n\n        successful_commands = sum(1 for result in test_results if result.get('success', False))\n        total_commands = len(test_results)\n\n        if total_commands > 0:\n            self.command_success_rate = successful_commands / total_commands\n            self.average_confidence = sum(\n                result.get('confidence', 0.0) for result in test_results\n            ) / total_commands\n            self.average_processing_time = sum(\n                result.get('processing_time', 0.0) for result in test_results\n            ) / total_commands\n\n        # Calculate accuracy based on expected vs actual ROS 2 outputs\n        correct_commands = sum(1 for result in test_results if result.get('output_correct', False))\n        self.command_accuracy = correct_commands / total_commands if total_commands > 0 else 0.0\n\n        return {\n            'command_success_rate': self.command_success_rate,\n            'average_confidence': self.average_confidence,\n            'average_processing_time': self.average_processing_time,\n            'command_accuracy': self.command_accuracy\n        }\n\n    def expected_metrics(self):\n        \"\"\"\n        Expected performance metrics for the VLA system\n        \"\"\"\n        return {\n            'command_success_rate': 0.90,  # 90% success rate\n            'average_confidence': 0.85,    # 85% average confidence\n            'average_processing_time': 0.5, # 500ms average processing time\n            'command_accuracy': 0.88       # 88% accuracy in ROS 2 output\n        }\n"})}),"\n",(0,o.jsx)(n.h2,{id:"error-handling-examples",children:"Error Handling Examples"}),"\n",(0,o.jsx)(n.h3,{id:"common-error-scenarios",children:"Common Error Scenarios"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class ErrorHandlingExamples:\n    def __init__(self):\n        self.error_scenarios = {\n            'unknown_location': {\n                'command': \"Go to the garage\",\n                'expected_error': \"Unknown destination: garage\",\n                'recovery_action': \"Request clarification or suggest alternatives\"\n            },\n            'object_not_found': {\n                'command': \"Pick up the purple elephant\",\n                'expected_error': \"Could not find object: purple elephant\",\n                'recovery_action': \"Inform user that object is not visible\"\n            },\n            'low_confidence': {\n                'command': \"Go to the kichen\",  # Intentional typo\n                'expected_error': \"Low confidence recognition\",\n                'recovery_action': \"Request repetition or confirmation\"\n            },\n            'robot_busy': {\n                'command': \"Move forward\",\n                'expected_error': \"Robot is currently executing another command\",\n                'recovery_action': \"Queue command or inform user of delay\"\n            }\n        }\n\n    def demonstrate_error_handling(self, scenario_name):\n        \"\"\"\n        Demonstrate how the system handles specific error scenarios\n        \"\"\"\n        scenario = self.error_scenarios.get(scenario_name)\n        if not scenario:\n            return \"Unknown scenario\"\n\n        # Simulate the error scenario\n        print(f\"Simulating scenario: {scenario_name}\")\n        print(f\"Command: {scenario['command']}\")\n        print(f\"Expected error: {scenario['expected_error']}\")\n        print(f\"Recovery action: {scenario['recovery_action']}\")\n\n        return {\n            'scenario': scenario_name,\n            'command': scenario['command'],\n            'error_handled': True,\n            'recovery_suggested': scenario['recovery_action']\n        }\n"})}),"\n",(0,o.jsx)(n.h2,{id:"advanced-examples",children:"Advanced Examples"}),"\n",(0,o.jsx)(n.h3,{id:"context-aware-commands",children:"Context-Aware Commands"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example of context-aware command processing\ncontext_aware_examples = [\n    {\n        'command': \"Do the same thing again\",\n        'context': {\n            'previous_command': \"Go to the kitchen\",\n            'previous_result': \"Successfully navigated to kitchen at [5.0, 3.0, 0.0]\"\n        },\n        'expected_behavior': \"Navigate to kitchen again\",\n        'ros2_output': {\n            'type': 'geometry_msgs/PoseStamped',\n            'data': {\n                'position': {'x': 5.0, 'y': 3.0, 'z': 0.0},\n                'orientation': {'x': 0.0, 'y': 0.0, 'z': 0.0, 'w': 1.0}\n            }\n        }\n    },\n    {\n        'command': \"What about the other one?\",\n        'context': {\n            'previous_command': \"Find the red ball\",\n            'previous_result': \"Found red ball at [2.5, 1.2, 0.5]\",\n            'environment': \"Two red balls visible: [2.5, 1.2, 0.5] and [3.2, 0.8, 0.5]\"\n        },\n        'expected_behavior': \"Locate the second red ball\",\n        'ros2_output': {\n            'type': 'vision_msgs/Detection2DArray',\n            'data': {\n                'detections': [\n                    {\n                        'bbox': {'center': {'x': 350, 'y': 220}, 'size_x': 60, 'size_y': 60},\n                        'results': [{'class_id': 'ball', 'score': 0.89}]\n                    }\n                ]\n            }\n        }\n    }\n]\n"})}),"\n",(0,o.jsx)(n.h3,{id:"multi-modal-integration",children:"Multi-Modal Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example of commands that integrate voice with other modalities\nmulti_modal_examples = [\n    {\n        'command': \"Go to where I'm pointing\",\n        'combined_input': {\n            'voice': \"Go to where I'm pointing\",\n            'vision': \"Person pointing direction vector [0.7, 0.2, 0.0]\"\n        },\n        'expected_behavior': \"Navigate in the direction the person is pointing\",\n        'ros2_output': {\n            'type': 'geometry_msgs/PoseStamped',\n            'data': {\n                'position': {'x': 3.5, 'y': 1.2, 'z': 0.0},  # Calculated from pointing direction\n                'orientation': {'x': 0.0, 'y': 0.0, 'z': 0.0, 'w': 1.0}\n            }\n        }\n    },\n    {\n        'command': \"Pick up what I showed you\",\n        'combined_input': {\n            'voice': \"Pick up what I showed you\",\n            'vision': \"Object highlighted/pointed at: blue cup at [1.8, 0.9, 0.8]\"\n        },\n        'expected_behavior': \"Grasp the blue cup that was highlighted\",\n        'ros2_output': [\n            {\n                'type': 'control_msgs/FollowJointTrajectory',\n                'data': {\n                    'trajectory': {\n                        'joint_names': ['joint1', 'joint2', 'joint3', 'joint4', 'joint5', 'joint6'],\n                        'points': [\n                            {\n                                'positions': [0.4, -0.1, 0.8, 0.2, 0.7, -0.1],\n                                'time_from_start': {'sec': 2, 'nanosec': 0}\n                            }\n                        ]\n                    }\n                }\n            }\n        ]\n    }\n]\n"})}),"\n",(0,o.jsx)(n.h2,{id:"best-practices-for-simulation",children:"Best Practices for Simulation"}),"\n",(0,o.jsx)(n.h3,{id:"1-realistic-audio-simulation",children:"1. Realistic Audio Simulation"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Use audio samples with realistic background noise"}),"\n",(0,o.jsx)(n.li,{children:"Simulate different acoustic environments (quiet, noisy, reverberant)"}),"\n",(0,o.jsx)(n.li,{children:"Include variations in speaker accents and speech patterns"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"2-comprehensive-testing",children:"2. Comprehensive Testing"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Test with various command formulations for the same action"}),"\n",(0,o.jsx)(n.li,{children:"Include edge cases and ambiguous commands"}),"\n",(0,o.jsx)(n.li,{children:"Test error recovery scenarios"}),"\n",(0,o.jsx)(n.li,{children:"Validate ROS 2 message formats and contents"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"3-performance-monitoring",children:"3. Performance Monitoring"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Monitor processing time for each command"}),"\n",(0,o.jsx)(n.li,{children:"Track success rates under different conditions"}),"\n",(0,o.jsx)(n.li,{children:"Measure resource usage during processing"}),"\n",(0,o.jsx)(n.li,{children:"Validate that safety constraints are maintained"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"4-iterative-improvement",children:"4. Iterative Improvement"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Use simulation results to improve recognition models"}),"\n",(0,o.jsx)(n.li,{children:"Refine intent parsing based on common misinterpretations"}),"\n",(0,o.jsx)(n.li,{children:"Optimize ROS 2 command generation for efficiency"}),"\n",(0,o.jsx)(n.li,{children:"Update validation parameters based on real-world performance"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,o.jsx)(n.h3,{id:"issue-1-low-recognition-accuracy",children:"Issue 1: Low Recognition Accuracy"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Symptoms"}),": Commands frequently misrecognized or confidence scores low\n",(0,o.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Check audio preprocessing parameters"}),"\n",(0,o.jsx)(n.li,{children:"Verify Whisper model configuration"}),"\n",(0,o.jsx)(n.li,{children:"Adjust confidence thresholds appropriately"}),"\n",(0,o.jsx)(n.li,{children:"Consider environmental noise factors"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"issue-2-incorrect-ros-2-message-generation",children:"Issue 2: Incorrect ROS 2 Message Generation"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Symptoms"}),": Generated messages don't match expected robot interfaces\n",(0,o.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Verify message type definitions match ROS 2 interfaces"}),"\n",(0,o.jsx)(n.li,{children:"Check coordinate frame conventions"}),"\n",(0,o.jsx)(n.li,{children:"Validate message field requirements"}),"\n",(0,o.jsx)(n.li,{children:"Test with actual robot simulation environment"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"issue-3-context-confusion",children:"Issue 3: Context Confusion"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Symptoms"}),": Commands misinterpreted due to context issues\n",(0,o.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Improve context tracking mechanisms"}),"\n",(0,o.jsx)(n.li,{children:"Add disambiguation prompts"}),"\n",(0,o.jsx)(n.li,{children:"Enhance entity resolution algorithms"}),"\n",(0,o.jsx)(n.li,{children:"Validate context updates between commands"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,o.jsx)(n.p,{children:"These simulated voice command examples provide a comprehensive reference for testing and validating the VLA system's voice processing capabilities. The examples cover various command types, complexity levels, and error scenarios that the system should handle. By implementing and testing these examples in simulation, developers can ensure that the voice-to-ROS 2 translation pipeline functions correctly and reliably."}),"\n",(0,o.jsx)(n.p,{children:"The examples demonstrate the complete flow from natural language input to specific ROS 2 messages, showing how the system processes different types of commands and generates appropriate robot control commands. This comprehensive set of examples serves as both a testing framework and a reference for expected system behavior."}),"\n",(0,o.jsxs)(n.p,{children:["For implementation details, refer to the complete ",(0,o.jsx)(n.a,{href:"/docs/voice-to-action/",children:"Voice Command Processing"})," overview and continue with the ",(0,o.jsx)(n.a,{href:"/docs/voice-to-action/",children:"Voice-to-Action Pipeline"})," documentation."]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}}}]);