"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[7021],{3023(e,n,s){s.d(n,{R:()=>a,x:()=>o});var i=s(3696);const t={},r=i.createContext(t);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(r.Provider,{value:n},e.children)}},3962(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var i=s(2540),t=s(3023);const r={title:"Speech-to-Text Processing Workflows and Validation",description:"Documentation on speech-to-text processing workflows and validation parameters in VLA systems",sidebar_position:7,tags:["vla","stt","speech-to-text","processing-workflow","validation"]},a="Speech-to-Text Processing Workflows and Validation",o={id:"voice-to-action/stt-processing-workflows",title:"Speech-to-Text Processing Workflows and Validation",description:"Documentation on speech-to-text processing workflows and validation parameters in VLA systems",source:"@site/docs/voice-to-action/stt-processing-workflows.md",sourceDirName:"voice-to-action",slug:"/voice-to-action/stt-processing-workflows",permalink:"/docs/voice-to-action/stt-processing-workflows",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/voice-to-action/stt-processing-workflows.md",tags:[{label:"vla",permalink:"/docs/tags/vla"},{label:"stt",permalink:"/docs/tags/stt"},{label:"speech-to-text",permalink:"/docs/tags/speech-to-text"},{label:"processing-workflow",permalink:"/docs/tags/processing-workflow"},{label:"validation",permalink:"/docs/tags/validation"}],version:"current",sidebarPosition:7,frontMatter:{title:"Speech-to-Text Processing Workflows and Validation",description:"Documentation on speech-to-text processing workflows and validation parameters in VLA systems",sidebar_position:7,tags:["vla","stt","speech-to-text","processing-workflow","validation"]},sidebar:"tutorialSidebar",previous:{title:"Voice Command Data Model and Validation",permalink:"/docs/voice-to-action/voice-command-data-model"},next:{title:"Simulated Voice Command Examples with ROS 2 Outputs",permalink:"/docs/voice-to-action/simulated-voice-examples"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Processing Workflow Architecture",id:"processing-workflow-architecture",level:2},{value:"End-to-End Processing Pipeline",id:"end-to-end-processing-pipeline",level:3},{value:"Real-Time vs Batch Processing",id:"real-time-vs-batch-processing",level:3},{value:"Real-Time Processing",id:"real-time-processing",level:4},{value:"Batch Processing",id:"batch-processing",level:4},{value:"Audio Preprocessing Workflow",id:"audio-preprocessing-workflow",level:2},{value:"Signal Conditioning",id:"signal-conditioning",level:3},{value:"Audio Quality Assessment",id:"audio-quality-assessment",level:3},{value:"STT Recognition Workflow",id:"stt-recognition-workflow",level:2},{value:"OpenAI Whisper Integration",id:"openai-whisper-integration",level:3},{value:"Alternative STT Engines",id:"alternative-stt-engines",level:3},{value:"Post-Processing Workflow",id:"post-processing-workflow",level:2},{value:"Text Refinement",id:"text-refinement",level:3},{value:"Validation Parameters",id:"validation-parameters",level:2},{value:"STT Quality Validation",id:"stt-quality-validation",level:3},{value:"Real-Time Validation",id:"real-time-validation",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching Strategies",id:"caching-strategies",level:3},{value:"Resource Management",id:"resource-management",level:3},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:2},{value:"STT Error Classification",id:"stt-error-classification",level:3},{value:"Integration with VLA Pipeline",id:"integration-with-vla-pipeline",level:2},{value:"Pipeline Coordination",id:"pipeline-coordination",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Quality Assurance",id:"quality-assurance",level:3},{value:"Performance Optimization",id:"performance-optimization-1",level:3},{value:"Error Handling",id:"error-handling",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Diagnostic Tools",id:"diagnostic-tools",level:3},{value:"Future Enhancements",id:"future-enhancements",level:2},{value:"Advanced Features",id:"advanced-features",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"speech-to-text-processing-workflows-and-validation",children:"Speech-to-Text Processing Workflows and Validation"}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"Speech-to-text (STT) processing is the critical first step in the Vision-Language-Action (VLA) system's voice command pipeline. This component converts audio input into textual representation that can be further processed by intent parsing and command translation systems. The quality and reliability of STT processing directly impacts the overall performance of the voice command system."}),"\n",(0,i.jsx)(n.h2,{id:"processing-workflow-architecture",children:"Processing Workflow Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"end-to-end-processing-pipeline",children:"End-to-End Processing Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"The STT processing workflow follows a structured pipeline:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Audio Input \u2192 Preprocessing \u2192 Recognition \u2192 Post-processing \u2192 Validation \u2192 Text Output\n"})}),"\n",(0,i.jsx)(n.p,{children:"Each stage in this pipeline transforms the input data and adds metadata that supports downstream processing and validation."}),"\n",(0,i.jsx)(n.h3,{id:"real-time-vs-batch-processing",children:"Real-Time vs Batch Processing"}),"\n",(0,i.jsx)(n.p,{children:"The system supports both real-time and batch processing workflows:"}),"\n",(0,i.jsx)(n.h4,{id:"real-time-processing",children:"Real-Time Processing"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use Case"}),": Interactive voice command systems"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Latency Requirements"}),": ",(0,i.jsx)(n.code,{children:"<500ms"})," response time"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Resource Usage"}),": Optimized for continuous operation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Quality Considerations"}),": May sacrifice some accuracy for speed"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"batch-processing",children:"Batch Processing"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use Case"}),": Processing recorded commands or offline analysis"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Latency Requirements"}),": Variable, optimized for accuracy"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Resource Usage"}),": Can use more computational resources"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Quality Considerations"}),": Optimized for maximum accuracy"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"audio-preprocessing-workflow",children:"Audio Preprocessing Workflow"}),"\n",(0,i.jsx)(n.h3,{id:"signal-conditioning",children:"Signal Conditioning"}),"\n",(0,i.jsx)(n.p,{children:"Before STT processing, audio signals undergo conditioning to optimize recognition quality:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport librosa\nfrom scipy import signal\n\nclass AudioPreprocessor:\n    def __init__(self):\n        self.sample_rate = 16000  # Standard rate for STT systems\n        self.frame_size = 1024    # Frame size for processing\n        self.overlap = 512        # Overlap for smooth processing\n\n    def preprocess_audio(self, audio_data, original_sample_rate=44100):\n        """\n        Preprocess audio data for optimal STT performance\n        """\n        # Step 1: Resample to standard rate\n        if original_sample_rate != self.sample_rate:\n            audio_data = librosa.resample(\n                audio_data,\n                orig_sr=original_sample_rate,\n                target_sr=self.sample_rate\n            )\n\n        # Step 2: Normalize amplitude\n        audio_data = self._normalize_audio(audio_data)\n\n        # Step 3: Apply noise reduction\n        audio_data = self._reduce_noise(audio_data)\n\n        # Step 4: Apply pre-emphasis filter\n        audio_data = self._apply_preemphasis(audio_data)\n\n        return audio_data\n\n    def _normalize_audio(self, audio_data):\n        """\n        Normalize audio to optimal range for STT processing\n        """\n        max_amplitude = np.max(np.abs(audio_data))\n        if max_amplitude > 0:\n            # Normalize to -1 to 1 range\n            normalized = audio_data / max_amplitude\n            # Scale to optimal range for STT (avoid clipping)\n            return normalized * 0.8\n        return audio_data\n\n    def _reduce_noise(self, audio_data):\n        """\n        Apply noise reduction using spectral subtraction\n        """\n        # Convert to frequency domain\n        stft = librosa.stft(audio_data, n_fft=self.frame_size)\n        magnitude = np.abs(stft)\n        phase = np.angle(stft)\n\n        # Estimate noise floor (using minimum statistics)\n        noise_floor = np.min(magnitude, axis=1, keepdims=True)\n\n        # Subtract noise (with flooring to avoid over-subtraction)\n        enhanced_magnitude = np.maximum(magnitude - 0.5 * noise_floor, 0.3 * magnitude)\n\n        # Convert back to time domain\n        enhanced_stft = enhanced_magnitude * np.exp(1j * phase)\n        enhanced_audio = librosa.istft(enhanced_stft)\n\n        return enhanced_audio.astype(audio_data.dtype)\n\n    def _apply_preemphasis(self, audio_data, preemphasis_coeff=0.97):\n        """\n        Apply pre-emphasis filter to boost high frequencies\n        """\n        return np.append(\n            audio_data[0],\n            audio_data[1:] - preemphasis_coeff * audio_data[:-1]\n        )\n'})}),"\n",(0,i.jsx)(n.h3,{id:"audio-quality-assessment",children:"Audio Quality Assessment"}),"\n",(0,i.jsx)(n.p,{children:"The system assesses audio quality before STT processing:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class AudioQualityAssessor:\n    def __init__(self):\n        self.min_snr_db = 10\n        self.min_amplitude = 0.01\n        self.max_background_noise = 0.1\n\n    def assess_audio_quality(self, audio_data):\n        """\n        Assess audio quality and provide recommendations\n        """\n        metrics = {\n            \'snr_db\': self._calculate_snr(audio_data),\n            \'amplitude\': self._calculate_amplitude(audio_data),\n            \'background_noise\': self._estimate_background_noise(audio_data),\n            \'clipping\': self._detect_clipping(audio_data),\n            \'frequency_balance\': self._analyze_frequency_balance(audio_data)\n        }\n\n        quality_score = self._calculate_quality_score(metrics)\n\n        recommendations = self._generate_recommendations(metrics)\n\n        return {\n            \'quality_score\': quality_score,\n            \'metrics\': metrics,\n            \'recommendations\': recommendations,\n            \'is_suitable_for_stt\': quality_score > 0.5\n        }\n\n    def _calculate_snr(self, audio_data):\n        """\n        Calculate signal-to-noise ratio\n        """\n        signal_power = np.mean(audio_data ** 2)\n        noise_power = self._estimate_noise_floor(audio_data)\n        return 10 * np.log10(signal_power / max(noise_power, 1e-10))\n\n    def _estimate_noise_floor(self, audio_data):\n        """\n        Estimate noise floor using minimum statistics\n        """\n        # Divide into windows and find minimum power windows\n        window_size = 1024\n        windows = []\n        for i in range(0, len(audio_data) - window_size, window_size):\n            window = audio_data[i:i + window_size]\n            windows.append(np.mean(window ** 2))\n\n        # Take minimum 10% as noise estimate\n        windows.sort()\n        noise_windows = windows[:max(1, len(windows) // 10)]\n        return np.mean(noise_windows) if noise_windows else 1e-10\n\n    def _calculate_amplitude(self, audio_data):\n        """\n        Calculate average amplitude\n        """\n        return np.mean(np.abs(audio_data))\n\n    def _estimate_background_noise(self, audio_data):\n        """\n        Estimate background noise level\n        """\n        # Calculate amplitude of lowest 10% of samples\n        sorted_amplitudes = np.sort(np.abs(audio_data))\n        noise_samples = sorted_amplitudes[:max(1, len(sorted_amplitudes) // 10)]\n        return np.mean(noise_samples)\n\n    def _detect_clipping(self, audio_data):\n        """\n        Detect audio clipping\n        """\n        max_amplitude = np.max(np.abs(audio_data))\n        return max_amplitude >= 0.95  # Assuming normalized audio\n\n    def _analyze_frequency_balance(self, audio_data):\n        """\n        Analyze frequency content balance\n        """\n        # Compute FFT\n        fft = np.fft.fft(audio_data)\n        magnitude = np.abs(fft[:len(fft)//2])\n\n        # Focus on speech frequency range (300-3400 Hz for 16kHz sample rate)\n        speech_start = int(300 * len(magnitude) / (self.sample_rate / 2))\n        speech_end = int(3400 * len(magnitude) / (self.sample_rate / 2))\n        speech_energy = np.sum(magnitude[speech_start:speech_end])\n        total_energy = np.sum(magnitude)\n\n        return speech_energy / total_energy if total_energy > 0 else 0.0\n\n    def _calculate_quality_score(self, metrics):\n        """\n        Calculate overall quality score from individual metrics\n        """\n        # Normalize metrics to 0-1 scale\n        snr_score = min(metrics[\'snr_db\'] / 30.0, 1.0) if metrics[\'snr_db\'] != float(\'inf\') else 1.0\n        amplitude_score = min(metrics[\'amplitude\'] / 0.1, 1.0)\n        noise_score = max(1.0 - metrics[\'background_noise\'] / 0.1, 0.0)\n        clipping_score = 0.0 if metrics[\'clipping\'] else 1.0\n        frequency_score = metrics[\'frequency_balance\']\n\n        # Weighted average\n        weights = {\n            \'snr\': 0.3,\n            \'amplitude\': 0.1,\n            \'noise\': 0.3,\n            \'clipping\': 0.2,\n            \'frequency\': 0.1\n        }\n\n        return (\n            snr_score * weights[\'snr\'] +\n            amplitude_score * weights[\'amplitude\'] +\n            noise_score * weights[\'noise\'] +\n            clipping_score * weights[\'clipping\'] +\n            frequency_score * weights[\'frequency\']\n        )\n\n    def _generate_recommendations(self, metrics):\n        """\n        Generate recommendations based on audio quality metrics\n        """\n        recommendations = []\n\n        if metrics[\'snr_db\'] < self.min_snr_db:\n            recommendations.append("Low signal-to-noise ratio detected. Try moving closer to microphone or reducing background noise.")\n\n        if metrics[\'background_noise\'] > self.max_background_noise:\n            recommendations.append("High background noise detected. Consider using noise suppression or moving to a quieter location.")\n\n        if metrics[\'clipping\']:\n            recommendations.append("Audio clipping detected. Reduce microphone gain or move farther from microphone.")\n\n        if metrics[\'frequency_balance\'] < 0.3:\n            recommendations.append("Poor frequency balance detected. Check microphone placement and ensure clear speech path.")\n\n        return recommendations if recommendations else ["Audio quality is suitable for STT processing."]\n'})}),"\n",(0,i.jsx)(n.h2,{id:"stt-recognition-workflow",children:"STT Recognition Workflow"}),"\n",(0,i.jsx)(n.h3,{id:"openai-whisper-integration",children:"OpenAI Whisper Integration"}),"\n",(0,i.jsx)(n.p,{children:"The primary STT engine uses OpenAI Whisper with configurable parameters:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import openai\nfrom openai import OpenAI\nimport tempfile\nimport os\n\nclass WhisperSTTProcessor:\n    def __init__(self, api_key=None, model=\"whisper-1\"):\n        \"\"\"\n        Initialize Whisper STT processor\n        \"\"\"\n        if api_key:\n            self.client = OpenAI(api_key=api_key)\n        else:\n            # For local models, initialize accordingly\n            pass\n        self.model = model\n        self.default_params = {\n            'response_format': 'verbose_json',\n            'timestamp_granularities': ['word'],\n            'temperature': 0.0,\n            'suppress_tokens': [-1]  # Suppress timestamps\n        }\n\n    def transcribe_audio(self, audio_data, language=None, quality_mode='balanced'):\n        \"\"\"\n        Transcribe audio data using Whisper\n        \"\"\"\n        # Save audio to temporary file\n        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:\n            # Save audio data to file (implementation depends on audio format)\n            temp_filename = temp_file.name\n\n        try:\n            # Configure parameters based on quality mode\n            params = self._configure_parameters(quality_mode)\n            if language:\n                params['language'] = language\n\n            # Transcribe the audio file\n            with open(temp_filename, \"rb\") as audio_file:\n                transcript = self.client.audio.transcriptions.create(\n                    model=self.model,\n                    file=audio_file,\n                    **params\n                )\n\n            # Extract confidence information if available\n            confidence = self._extract_confidence(transcript)\n\n            return {\n                'text': transcript.text,\n                'confidence': confidence,\n                'language': transcript.language,\n                'processing_time': self._calculate_processing_time(),\n                'words': self._extract_word_timestamps(transcript)\n            }\n\n        finally:\n            # Clean up temporary file\n            os.unlink(temp_filename)\n\n    def _configure_parameters(self, quality_mode):\n        \"\"\"\n        Configure Whisper parameters based on quality mode\n        \"\"\"\n        params = self.default_params.copy()\n\n        if quality_mode == 'accuracy':\n            params.update({\n                'temperature': 0.0,\n                'response_format': 'verbose_json'\n            })\n        elif quality_mode == 'speed':\n            params.update({\n                'temperature': 0.2,\n                'response_format': 'text'\n            })\n        elif quality_mode == 'balanced':\n            params.update({\n                'temperature': 0.1,\n                'response_format': 'verbose_json'\n            })\n\n        return params\n\n    def _extract_confidence(self, transcript):\n        \"\"\"\n        Extract confidence information from Whisper result\n        \"\"\"\n        # Whisper doesn't provide direct confidence scores in all formats\n        # This is a simplified approach - real implementation would use more sophisticated methods\n        if hasattr(transcript, 'segments') and transcript.segments:\n            avg_logprob = sum([seg.avg_logprob for seg in transcript.segments if hasattr(seg, 'avg_logprob')]) / len(transcript.segments)\n            # Convert log probability to confidence (0-1 scale)\n            return max(0, min(1, (avg_logprob + 2) / 2))  # Normalize -2 to 0 range to 0-1\n        else:\n            return 0.8  # Default confidence for text-only response\n\n    def _extract_word_timestamps(self, transcript):\n        \"\"\"\n        Extract word-level timestamps from transcript\n        \"\"\"\n        if hasattr(transcript, 'segments') and transcript.segments:\n            words = []\n            for segment in transcript.segments:\n                if hasattr(segment, 'words') and segment.words:\n                    for word in segment.words:\n                        words.append({\n                            'word': word.word if hasattr(word, 'word') else '',\n                            'start': word.start if hasattr(word, 'start') else 0,\n                            'end': word.end if hasattr(word, 'end') else 0,\n                            'probability': word.probability if hasattr(word, 'probability') else 1.0\n                        })\n            return words\n        return []\n"})}),"\n",(0,i.jsx)(n.h3,{id:"alternative-stt-engines",children:"Alternative STT Engines"}),"\n",(0,i.jsx)(n.p,{children:"The system supports alternative STT engines for different use cases:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class AlternativeSTTEngines:\n    def __init__(self):\n        self.engines = {\n            \'google\': self._google_stt,\n            \'azure\': self._azure_stt,\n            \'vosk\': self._vosk_stt\n        }\n\n    def _google_stt(self, audio_data, language=\'en-US\'):\n        """\n        Google Cloud Speech-to-Text implementation\n        """\n        # This would require Google Cloud credentials\n        # Implementation would use google.cloud.speech\n        pass\n\n    def _azure_stt(self, audio_data, language=\'en-US\'):\n        """\n        Azure Cognitive Services Speech-to-Text implementation\n        """\n        # This would require Azure credentials\n        # Implementation would use azure.cognitiveservices.speech\n        pass\n\n    def _vosk_stt(self, audio_data, language=\'en-US\'):\n        """\n        Vosk offline Speech-to-Text implementation\n        """\n        # This would use Vosk library for offline processing\n        # Implementation would use vosk library\n        pass\n'})}),"\n",(0,i.jsx)(n.h2,{id:"post-processing-workflow",children:"Post-Processing Workflow"}),"\n",(0,i.jsx)(n.h3,{id:"text-refinement",children:"Text Refinement"}),"\n",(0,i.jsx)(n.p,{children:"After STT recognition, the text undergoes refinement:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import re\nfrom typing import Dict, List\n\nclass TextPostProcessor:\n    def __init__(self):\n        self.correction_rules = self._load_correction_rules()\n        self.normalization_rules = self._load_normalization_rules()\n\n    def post_process_text(self, raw_text: str, confidence: float) -> Dict[str, any]:\n        \"\"\"\n        Post-process raw STT output to improve quality\n        \"\"\"\n        processed_text = raw_text.strip()\n\n        # Apply normalization rules\n        processed_text = self._apply_normalization(processed_text)\n\n        # Apply correction rules\n        processed_text = self._apply_corrections(processed_text)\n\n        # Clean up formatting\n        processed_text = self._clean_formatting(processed_text)\n\n        # Calculate post-processed confidence\n        post_process_confidence = self._calculate_post_process_confidence(\n            raw_text, processed_text, confidence\n        )\n\n        return {\n            'original_text': raw_text,\n            'processed_text': processed_text,\n            'confidence': post_process_confidence,\n            'processing_applied': True\n        }\n\n    def _load_correction_rules(self):\n        \"\"\"\n        Load correction rules for common STT errors\n        \"\"\"\n        return {\n            # Common misrecognitions\n            r'\\bwer\\b': 'where',\n            r'\\bwan\\b': 'want',\n            r'\\bwat\\b': 'what',\n            r'\\bda\\b': 'the',\n            r'\\bgonna\\b': 'going to',\n            r'\\bwanna\\b': 'want to',\n            r'\\bhafta\\b': 'have to',\n        }\n\n    def _load_normalization_rules(self):\n        \"\"\"\n        Load normalization rules for text standardization\n        \"\"\"\n        return {\n            # Number normalization\n            r'\\bto\\b': '2',\n            r'\\bfor\\b': '4',\n            r'\\btoo\\b': '2',\n            r'\\bfore\\b': '4',\n        }\n\n    def _apply_normalization(self, text: str) -> str:\n        \"\"\"\n        Apply normalization rules to text\n        \"\"\"\n        result = text\n        for pattern, replacement in self.normalization_rules.items():\n            result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n        return result\n\n    def _apply_corrections(self, text: str) -> str:\n        \"\"\"\n        Apply correction rules to text\n        \"\"\"\n        result = text\n        for pattern, replacement in self.correction_rules.items():\n            result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n        return result\n\n    def _clean_formatting(self, text: str) -> str:\n        \"\"\"\n        Clean up text formatting and punctuation\n        \"\"\"\n        # Remove extra whitespace\n        text = re.sub(r'\\s+', ' ', text)\n        # Fix common punctuation issues\n        text = re.sub(r'\\s+([,.!?;:])', r'\\1', text)\n        # Ensure proper capitalization\n        text = text.capitalize()\n        return text.strip()\n\n    def _calculate_post_process_confidence(self, original: str, processed: str, base_confidence: float) -> float:\n        \"\"\"\n        Calculate confidence after post-processing\n        \"\"\"\n        # If post-processing made significant changes, reduce confidence\n        if original.lower() != processed.lower():\n            # Calculate similarity\n            similarity = self._calculate_similarity(original.lower(), processed.lower())\n            # Reduce confidence based on changes made\n            adjustment = (1.0 - similarity) * 0.1  # Max 10% reduction\n            return max(0.0, base_confidence - adjustment)\n        else:\n            return base_confidence\n\n    def _calculate_similarity(self, str1: str, str2: str) -> float:\n        \"\"\"\n        Calculate similarity between two strings\n        \"\"\"\n        if len(str1) == 0 and len(str2) == 0:\n            return 1.0\n\n        # Simple character-based similarity\n        common_chars = sum(1 for a, b in zip(str1, str2) if a == b)\n        max_len = max(len(str1), len(str2))\n        return common_chars / max_len if max_len > 0 else 0.0\n"})}),"\n",(0,i.jsx)(n.h2,{id:"validation-parameters",children:"Validation Parameters"}),"\n",(0,i.jsx)(n.h3,{id:"stt-quality-validation",children:"STT Quality Validation"}),"\n",(0,i.jsx)(n.p,{children:"The system validates STT output quality using multiple parameters:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class STTQualityValidator:\n    def __init__(self):\n        self.min_confidence_threshold = 0.7\n        self.max_unrecognized_ratio = 0.3\n        self.min_word_count = 1\n        self.max_word_count = 100\n\n    def validate_stt_output(self, stt_result: Dict[str, any], original_audio: np.ndarray = None) -> Dict[str, any]:\n        \"\"\"\n        Validate STT output quality\n        \"\"\"\n        validation_result = {\n            'is_valid': True,\n            'confidence_score': stt_result.get('confidence', 0.0),\n            'issues': [],\n            'recommendations': [],\n            'validation_details': {}\n        }\n\n        # Check confidence threshold\n        confidence = stt_result.get('confidence', 0.0)\n        if confidence < self.min_confidence_threshold:\n            validation_result['is_valid'] = False\n            validation_result['issues'].append(f\"Confidence {confidence:.2f} below threshold {self.min_confidence_threshold}\")\n            validation_result['recommendations'].append(\"Request command repetition or use alternative input method\")\n\n        # Check text content\n        text = stt_result.get('processed_text', stt_result.get('text', ''))\n        word_count = len(text.split())\n\n        if word_count < self.min_word_count:\n            validation_result['is_valid'] = False\n            validation_result['issues'].append(f\"Text too short: {word_count} words, minimum {self.min_word_count}\")\n\n        if word_count > self.max_word_count:\n            validation_result['is_valid'] = False\n            validation_result['issues'].append(f\"Text too long: {word_count} words, maximum {self.max_word_count}\")\n\n        # Check for unrecognized placeholders\n        unrecognized_ratio = self._calculate_unrecognized_ratio(text)\n        if unrecognized_ratio > self.max_unrecognized_ratio:\n            validation_result['is_valid'] = False\n            validation_result['issues'].append(f\"High unrecognized ratio: {unrecognized_ratio:.2f}, maximum {self.max_unrecognized_ratio}\")\n\n        # Validate language consistency if language info available\n        if 'language' in stt_result and stt_result['language'] != 'en':\n            validation_result['recommendations'].append(f\"Detected language: {stt_result['language']}. Ensure system supports this language.\")\n\n        validation_result['validation_details'] = {\n            'word_count': word_count,\n            'unrecognized_ratio': unrecognized_ratio,\n            'language': stt_result.get('language', 'unknown'),\n            'processing_time': stt_result.get('processing_time', 0)\n        }\n\n        return validation_result\n\n    def _calculate_unrecognized_ratio(self, text: str) -> float:\n        \"\"\"\n        Calculate ratio of unrecognized elements in text\n        \"\"\"\n        # Count common placeholders that indicate unrecognized speech\n        placeholders = ['you know', 'um', 'uh', 'er', 'ah']\n        placeholder_count = sum(1 for placeholder in placeholders if placeholder in text.lower())\n\n        word_count = len(text.split())\n        return placeholder_count / word_count if word_count > 0 else 0.0\n\n    def validate_for_intent_parsing(self, stt_result: Dict[str, any]) -> Dict[str, any]:\n        \"\"\"\n        Validate STT output specifically for intent parsing\n        \"\"\"\n        text = stt_result.get('processed_text', stt_result.get('text', ''))\n        confidence = stt_result.get('confidence', 0.0)\n\n        validation_result = {\n            'suitable_for_intent_parsing': True,\n            'issues': [],\n            'suggestions': []\n        }\n\n        # Check if text contains actionable commands\n        if not self._contains_actionable_command(text):\n            validation_result['suitable_for_intent_parsing'] = False\n            validation_result['issues'].append(\"Text does not contain actionable commands\")\n            validation_result['suggestions'].append(\"Use more direct command language (e.g., 'Go to kitchen' instead of 'Could you go to the kitchen?')\")\n\n        # Check confidence for intent parsing\n        if confidence < 0.6:\n            validation_result['suitable_for_intent_parsing'] = False\n            validation_result['issues'].append(f\"Low confidence ({confidence:.2f}) may affect intent parsing accuracy\")\n\n        return validation_result\n\n    def _contains_actionable_command(self, text: str) -> bool:\n        \"\"\"\n        Check if text contains actionable commands\n        \"\"\"\n        # Common command indicators\n        command_indicators = [\n            'go', 'move', 'navigate', 'walk', 'run', 'turn', 'rotate',\n            'pick', 'grasp', 'take', 'get', 'bring', 'place', 'put',\n            'find', 'look', 'search', 'see', 'describe', 'show',\n            'stop', 'wait', 'pause', 'continue', 'start', 'begin'\n        ]\n\n        text_lower = text.lower()\n        return any(indicator in text_lower for indicator in command_indicators)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"real-time-validation",children:"Real-Time Validation"}),"\n",(0,i.jsx)(n.p,{children:"For real-time applications, the system implements streaming validation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import asyncio\nfrom collections import deque\n\nclass RealTimeSTTValidator:\n    def __init__(self, window_size=10):\n        self.window_size = window_size\n        self.confidence_history = deque(maxlen=window_size)\n        self.quality_history = deque(maxlen=window_size)\n        self.validator = STTQualityValidator()\n\n    async def validate_streaming_input(self, audio_stream):\n        \"\"\"\n        Validate STT output in real-time from audio stream\n        \"\"\"\n        async for audio_chunk in audio_stream:\n            # Process audio chunk\n            stt_result = await self._process_audio_chunk(audio_chunk)\n\n            # Validate result\n            validation = self.validator.validate_stt_output(stt_result)\n\n            # Update history\n            self.confidence_history.append(validation['confidence_score'])\n            self.quality_history.append(validation['is_valid'])\n\n            # Check for consistent quality issues\n            if self._has_consistent_quality_issues():\n                yield {\n                    'type': 'quality_alert',\n                    'message': 'Consistent quality issues detected',\n                    'suggestions': ['Check microphone placement', 'Reduce background noise']\n                }\n\n            # Yield result\n            yield {\n                'type': 'stt_result',\n                'result': stt_result,\n                'validation': validation\n            }\n\n    async def _process_audio_chunk(self, audio_chunk):\n        \"\"\"\n        Process a single audio chunk\n        \"\"\"\n        # Implementation would process the audio chunk\n        # using the STT pipeline\n        pass\n\n    def _has_consistent_quality_issues(self) -> bool:\n        \"\"\"\n        Check if there are consistent quality issues in the history\n        \"\"\"\n        if len(self.quality_history) < self.window_size:\n            return False\n\n        # If more than 50% of recent results are invalid\n        invalid_count = sum(1 for valid in self.quality_history if not valid)\n        return invalid_count > (self.window_size / 2)\n\n    def get_streaming_quality_metrics(self) -> Dict[str, any]:\n        \"\"\"\n        Get quality metrics for streaming validation\n        \"\"\"\n        if not self.confidence_history:\n            return {\n                'average_confidence': 0.0,\n                'validity_rate': 0.0,\n                'recent_trend': 'unknown'\n            }\n\n        avg_confidence = sum(self.confidence_history) / len(self.confidence_history)\n        validity_rate = sum(self.quality_history) / len(self.quality_history)\n\n        # Determine trend\n        if len(self.confidence_history) >= 3:\n            recent_values = list(self.confidence_history)[-3:]\n            if recent_values[-1] > recent_values[0]:\n                trend = 'improving'\n            elif recent_values[-1] < recent_values[0]:\n                trend = 'declining'\n            else:\n                trend = 'stable'\n        else:\n            trend = 'insufficient_data'\n\n        return {\n            'average_confidence': avg_confidence,\n            'validity_rate': validity_rate,\n            'recent_trend': trend,\n            'window_size': self.window_size,\n            'samples_in_window': len(self.confidence_history)\n        }\n"})}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"caching-strategies",children:"Caching Strategies"}),"\n",(0,i.jsx)(n.p,{children:"The system implements caching for improved performance:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from functools import lru_cache\nimport hashlib\n\nclass STTCache:\n    def __init__(self, max_size=1000):\n        self.max_size = max_size\n        self.cache = {}\n\n    @lru_cache(maxsize=1000)\n    def cached_transcribe(self, audio_hash: str, language: str = \'en\', quality_mode: str = \'balanced\'):\n        """\n        Cached transcription to avoid reprocessing identical audio\n        """\n        # This would call the actual STT service\n        # Implementation would be in the main STT processor\n        pass\n\n    def get_audio_hash(self, audio_data: np.ndarray) -> str:\n        """\n        Generate hash for audio data to use as cache key\n        """\n        # Use first 1000 samples for hash to balance uniqueness and performance\n        sample_data = audio_data[:min(1000, len(audio_data))]\n        return hashlib.md5(sample_data.tobytes()).hexdigest()\n\n    def is_cached(self, audio_hash: str) -> bool:\n        """\n        Check if audio result is already cached\n        """\n        return audio_hash in self.cache\n\n    def cache_result(self, audio_hash: str, result: Dict[str, any]):\n        """\n        Cache STT result\n        """\n        if len(self.cache) >= self.max_size:\n            # Remove oldest entry (FIFO)\n            oldest_key = next(iter(self.cache))\n            del self.cache[oldest_key]\n\n        self.cache[audio_hash] = result\n\n    def get_cached_result(self, audio_hash: str) -> Dict[str, any]:\n        """\n        Retrieve cached STT result\n        """\n        return self.cache.get(audio_hash)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"resource-management",children:"Resource Management"}),"\n",(0,i.jsx)(n.p,{children:"The system manages resources efficiently:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class STTResourceManager:\n    def __init__(self):\n        self.max_concurrent_processes = 5\n        self.current_processes = 0\n        self.process_queue = []\n        self.resource_limits = {\n            \'cpu_usage\': 0.8,  # 80% max CPU usage\n            \'memory_usage\': 0.7,  # 70% max memory usage\n            \'processing_queue\': 10  # Max items in processing queue\n        }\n\n    def acquire_resources(self) -> bool:\n        """\n        Acquire resources for STT processing\n        """\n        if self.current_processes >= self.max_concurrent_processes:\n            # Add to queue if possible\n            if len(self.process_queue) < self.resource_limits[\'processing_queue\']:\n                self.process_queue.append(\'pending\')\n                return False  # Need to wait\n            else:\n                return False  # Queue full, reject\n\n        # Check system resources\n        if self._system_resources_available():\n            self.current_processes += 1\n            return True\n        else:\n            return False\n\n    def release_resources(self):\n        """\n        Release resources after STT processing\n        """\n        if self.current_processes > 0:\n            self.current_processes -= 1\n\n        # Process queued items if resources available\n        if self.process_queue and self._system_resources_available():\n            self.process_queue.pop(0)  # Remove from queue\n            self.current_processes += 1  # Acquire for queued item\n\n    def _system_resources_available(self) -> bool:\n        """\n        Check if system resources are available\n        """\n        # In a real implementation, this would check actual system resources\n        # using psutil or similar library\n        import psutil\n\n        cpu_percent = psutil.cpu_percent(interval=1)\n        memory_percent = psutil.virtual_memory().percent\n\n        return (\n            cpu_percent < (self.resource_limits[\'cpu_usage\'] * 100) and\n            memory_percent < (self.resource_limits[\'memory_usage\'] * 100)\n        )\n'})}),"\n",(0,i.jsx)(n.h2,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,i.jsx)(n.h3,{id:"stt-error-classification",children:"STT Error Classification"}),"\n",(0,i.jsx)(n.p,{children:"The system classifies and handles different types of STT errors:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class STTErrorClassifier:\n    def __init__(self):\n        self.error_patterns = {\n            'audio_quality': [\n                r'no speech detected',\n                r'audio too quiet',\n                r'background noise too high',\n                r'audio clipping detected'\n            ],\n            'service_unavailable': [\n                r'connection timeout',\n                r'service unavailable',\n                r'rate limit exceeded',\n                r'authentication failed'\n            ],\n            'processing_error': [\n                r'invalid audio format',\n                r'file not found',\n                r'corrupted audio',\n                r'unsupported format'\n            ]\n        }\n\n    def classify_error(self, error_message: str) -> str:\n        \"\"\"\n        Classify STT error type\n        \"\"\"\n        error_lower = error_message.lower()\n\n        for error_type, patterns in self.error_patterns.items():\n            for pattern in patterns:\n                if pattern in error_lower:\n                    return error_type\n\n        return 'unknown_error'\n\n    def generate_recovery_strategy(self, error_type: str, context: Dict[str, any] = None) -> Dict[str, any]:\n        \"\"\"\n        Generate recovery strategy based on error type\n        \"\"\"\n        strategies = {\n            'audio_quality': {\n                'immediate_action': 'request_repetition',\n                'improvements': [\n                    'improve_microphone_placement',\n                    'reduce_background_noise',\n                    'increase_speech_volume'\n                ],\n                'retry_allowed': True\n            },\n            'service_unavailable': {\n                'immediate_action': 'use_alternative_service',\n                'improvements': [\n                    'check_internet_connection',\n                    'verify_api_credentials',\n                    'reduce_request_frequency'\n                ],\n                'retry_allowed': True\n            },\n            'processing_error': {\n                'immediate_action': 'format_audio_correctly',\n                'improvements': [\n                    'convert_audio_format',\n                    'validate_audio_file',\n                    'check_file_integrity'\n                ],\n                'retry_allowed': False  # Don't retry same file\n            },\n            'unknown_error': {\n                'immediate_action': 'fallback_to_manual_input',\n                'improvements': [\n                    'contact_support',\n                    'check_system_status',\n                    'retry_with_different_input'\n                ],\n                'retry_allowed': True\n            }\n        }\n\n        return strategies.get(error_type, strategies['unknown_error'])\n"})}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-vla-pipeline",children:"Integration with VLA Pipeline"}),"\n",(0,i.jsx)(n.h3,{id:"pipeline-coordination",children:"Pipeline Coordination"}),"\n",(0,i.jsx)(n.p,{children:"The STT workflow integrates with the broader VLA pipeline:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class STTPipelineCoordinator:\n    def __init__(self):\n        self.preprocessor = AudioPreprocessor()\n        self.stt_processor = WhisperSTTProcessor()\n        self.postprocessor = TextPostProcessor()\n        self.validator = STTQualityValidator()\n        self.error_classifier = STTErrorClassifier()\n\n    async def process_voice_command(self, audio_data, context=None) -> Dict[str, any]:\n        \"\"\"\n        Process a complete voice command through the STT pipeline\n        \"\"\"\n        result = {\n            'success': False,\n            'text': '',\n            'confidence': 0.0,\n            'language': '',\n            'processing_steps': [],\n            'validation': {},\n            'error_info': None\n        }\n\n        try:\n            # Step 1: Audio preprocessing\n            quality_assessment = self.preprocessor.assess_audio_quality(audio_data)\n            result['processing_steps'].append({\n                'step': 'audio_preprocessing',\n                'quality_score': quality_assessment['quality_score'],\n                'recommendations': quality_assessment['recommendations']\n            })\n\n            if not quality_assessment['is_suitable_for_stt']:\n                result['error_info'] = {\n                    'type': 'audio_quality_issue',\n                    'message': 'Audio quality insufficient for STT processing',\n                    'recommendations': quality_assessment['recommendations']\n                }\n                return result\n\n            # Step 2: Preprocess audio\n            processed_audio = self.preprocessor.preprocess_audio(audio_data)\n\n            # Step 3: STT recognition\n            stt_result = self.stt_processor.transcribe_audio(\n                processed_audio,\n                language=context.get('preferred_language') if context else None\n            )\n            result['processing_steps'].append({\n                'step': 'stt_recognition',\n                'raw_text': stt_result['text'],\n                'confidence': stt_result['confidence']\n            })\n\n            # Step 4: Text post-processing\n            post_processed = self.postprocessor.post_process_text(\n                stt_result['text'],\n                stt_result['confidence']\n            )\n            result['processing_steps'].append({\n                'step': 'text_postprocessing',\n                'original_text': post_processed['original_text'],\n                'processed_text': post_processed['processed_text'],\n                'confidence': post_processed['confidence']\n            })\n\n            # Step 5: Validation\n            validation = self.validator.validate_stt_output(post_processed)\n            result['validation'] = validation\n            result['processing_steps'].append({\n                'step': 'validation',\n                'is_valid': validation['is_valid'],\n                'issues': validation['issues']\n            })\n\n            # Step 6: Final result\n            if validation['is_valid']:\n                result['success'] = True\n                result['text'] = post_processed['processed_text']\n                result['confidence'] = post_processed['confidence']\n                result['language'] = stt_result.get('language', 'unknown')\n            else:\n                result['error_info'] = {\n                    'type': 'validation_failure',\n                    'message': 'STT output did not pass validation',\n                    'issues': validation['issues'],\n                    'recommendations': validation['recommendations']\n                }\n\n        except Exception as e:\n            result['error_info'] = {\n                'type': 'processing_error',\n                'message': str(e),\n                'error_class': self.error_classifier.classify_error(str(e))\n            }\n\n        return result\n"})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"quality-assurance",children:"Quality Assurance"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-Stage Validation"}),": Validate at each processing stage to catch issues early"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context-Aware Processing"}),": Consider environmental context in processing decisions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Confidence-Based Handling"}),": Use confidence scores to determine appropriate actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Continuous Monitoring"}),": Monitor STT performance and quality over time"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"performance-optimization-1",children:"Performance Optimization"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Caching"}),": Cache results for repeated audio inputs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Resource Management"}),": Efficiently manage computational resources"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Batch Processing"}),": Process multiple inputs efficiently when possible"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Asynchronous Processing"}),": Use async processing for better responsiveness"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"error-handling",children:"Error Handling"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Graceful Degradation"}),": Provide fallbacks when STT fails"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Clear Feedback"}),": Give users clear information about issues and solutions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recovery Strategies"}),": Implement appropriate recovery for different error types"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitoring"}),": Track error rates and types for system improvement"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Low Recognition Accuracy"}),": Check audio quality, microphone placement, and background noise"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"High Latency"}),": Optimize processing pipeline and resource allocation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Service Unavailability"}),": Verify API credentials and service status"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory Issues"}),": Implement proper resource management and caching"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"diagnostic-tools",children:"Diagnostic Tools"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def diagnose_stt_pipeline(audio_data, expected_text=None):\n    \"\"\"\n    Diagnose issues in the STT pipeline\n    \"\"\"\n    coordinator = STTPipelineCoordinator()\n    result = await coordinator.process_voice_command(audio_data)\n\n    diagnosis = {\n        'pipeline_result': result,\n        'quality_metrics': {},\n        'performance_metrics': {},\n        'recommendations': []\n    }\n\n    # Analyze quality metrics\n    if result['validation']:\n        diagnosis['quality_metrics'] = {\n            'final_confidence': result['confidence'],\n            'validation_passed': result['validation']['is_valid'],\n            'issues_count': len(result['validation'].get('issues', []))\n        }\n\n    # Generate recommendations\n    if result['error_info']:\n        diagnosis['recommendations'].append(result['error_info'].get('recommendations', []))\n\n    # Compare with expected text if provided\n    if expected_text:\n        similarity = calculate_text_similarity(result.get('text', ''), expected_text)\n        diagnosis['quality_metrics']['accuracy'] = similarity\n        if similarity < 0.8:\n            diagnosis['recommendations'].append(\"Consider improving audio quality or using clearer speech.\")\n\n    return diagnosis\n"})}),"\n",(0,i.jsx)(n.h2,{id:"future-enhancements",children:"Future Enhancements"}),"\n",(0,i.jsx)(n.h3,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speaker Adaptation"}),": Adapt STT models to individual speakers"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-Microphone Processing"}),": Use multiple microphones for better audio capture"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context-Aware Recognition"}),": Use context to improve recognition accuracy"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Emotion Detection"}),": Detect emotional context in speech"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"The speech-to-text processing workflow is fundamental to the VLA system's voice command capabilities. By implementing robust preprocessing, recognition, post-processing, and validation steps, the system ensures reliable and accurate conversion of voice commands to text. The comprehensive validation parameters and error handling mechanisms maintain system reliability while providing optimal user experience."}),"\n",(0,i.jsxs)(n.p,{children:["For implementation details, refer to the complete ",(0,i.jsx)(n.a,{href:"/docs/voice-to-action/",children:"Voice Command Processing"})," overview and continue with the ",(0,i.jsx)(n.a,{href:"/docs/voice-to-action/",children:"Voice-to-Action Pipeline"})," documentation."]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);