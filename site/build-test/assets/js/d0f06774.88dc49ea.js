"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[1613],{6430(n){n.exports=JSON.parse('{"label":"vla","permalink":"/docs/tags/vla","allTagsPath":"/docs/tags","count":19,"items":[{"id":"cognitive-planning/action-sequencing","title":"Action Sequencing in Cognitive Planning","description":"Documentation on action sequencing techniques using LLMs for humanoid robot planning in VLA systems","permalink":"/docs/cognitive-planning/action-sequencing"},{"id":"cognitive-planning/data-model","title":"Cognitive Planning Data Model","description":"Documentation on the data model for cognitive planning in VLA systems with validation steps","permalink":"/docs/cognitive-planning/data-model"},{"id":"cognitive-planning/index","title":"Cognitive Planning Overview","description":"Overview of cognitive planning using LLMs for translating natural language tasks into action sequences in VLA systems","permalink":"/docs/cognitive-planning/"},{"id":"voice-to-action/command-translation","title":"Command Translation to ROS 2","description":"Documentation on translating parsed voice commands to ROS 2 actions and messages in VLA systems","permalink":"/docs/voice-to-action/command-translation"},{"id":"voice-to-action/confidence-scoring","title":"Confidence Scoring and Validation in Voice Processing","description":"Documentation on confidence scoring mechanisms and validation approaches for voice command processing in VLA systems","permalink":"/docs/voice-to-action/confidence-scoring"},{"id":"cognitive-planning/context-awareness","title":"Context Awareness and Environmental Integration","description":"Documentation on context awareness and environmental integration for cognitive planning in VLA systems","permalink":"/docs/cognitive-planning/context-awareness"},{"id":"voice-to-action/intent-parsing","title":"Intent Parsing and Natural Language Understanding","description":"Documentation on parsing natural language voice commands to extract actionable intents in VLA systems","permalink":"/docs/voice-to-action/intent-parsing"},{"id":"cognitive-planning/llm-integration","title":"LLM Integration for Cognitive Planning","description":"Documentation on integrating Large Language Models for cognitive planning in VLA systems","permalink":"/docs/cognitive-planning/llm-integration"},{"id":"cognitive-planning/validation","title":"Planning Validation with LLMs and Action Feasibility Checks","description":"Documentation on planning validation using LLMs and action feasibility checks in VLA systems","permalink":"/docs/cognitive-planning/validation"},{"id":"voice-to-action/simulated-voice-examples","title":"Simulated Voice Command Examples with ROS 2 Outputs","description":"Comprehensive examples of voice commands and their expected ROS 2 outputs in VLA systems","permalink":"/docs/voice-to-action/simulated-voice-examples"},{"id":"voice-to-action/speech-recognition-whisper","title":"Speech Recognition with OpenAI Whisper","description":"Detailed documentation on implementing speech recognition using OpenAI Whisper for VLA systems","permalink":"/docs/voice-to-action/speech-recognition-whisper"},{"id":"voice-to-action/stt-processing-workflows","title":"Speech-to-Text Processing Workflows and Validation","description":"Documentation on speech-to-text processing workflows and validation parameters in VLA systems","permalink":"/docs/voice-to-action/stt-processing-workflows"},{"id":"cognitive-planning/task-decomposition","title":"Task Decomposition in Cognitive Planning","description":"Documentation on task decomposition techniques using LLMs for humanoid robot planning in VLA systems","permalink":"/docs/cognitive-planning/task-decomposition"},{"id":"vla-overview/index","title":"Vision-Language-Action (VLA) Systems for Autonomous Humanoids","description":"Comprehensive guide to integrating language, perception, and action for humanoid robot autonomy","permalink":"/docs/vla-overview/"},{"id":"vla-cross-references","title":"VLA Cross-Module References","description":"Cross-references between Vision-Language-Action documentation modules","permalink":"/docs/vla-cross-references"},{"id":"vla-documentation-standards","title":"VLA Documentation Standards","description":"Standards and formatting guidelines for Vision-Language-Action documentation following Docusaurus best practices","permalink":"/docs/vla-documentation-standards"},{"id":"vla-ecosystem-overview","title":"VLA Ecosystem Integration Overview","description":"Overview of the Vision-Language-Action ecosystem and how components integrate for humanoid autonomy","permalink":"/docs/vla-ecosystem-overview"},{"id":"voice-to-action/voice-command-data-model","title":"Voice Command Data Model and Validation","description":"Documentation on the voice command data model and validation processes in VLA systems","permalink":"/docs/voice-to-action/voice-command-data-model"},{"id":"voice-to-action/index","title":"Voice Command Processing Overview","description":"Overview of how voice commands are processed and converted to ROS 2 actions in the VLA system","permalink":"/docs/voice-to-action/"}],"unlisted":false}')}}]);